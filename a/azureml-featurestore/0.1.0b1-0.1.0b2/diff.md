# Comparing `tmp/azureml_featurestore-0.1.0b1-py3-none-any.whl.zip` & `tmp/azureml_featurestore-0.1.0b2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,57 +1,58 @@
-Zip file size: 66566 bytes, number of entries: 55
--rw-rw-rw-  2.0 fat      267 b- defN 23-May-17 19:57 azureml/__init__.py
--rw-rw-rw-  2.0 fat      565 b- defN 23-May-17 19:57 azureml/featurestore/__init__.py
--rw-rw-rw-  2.0 fat    21083 b- defN 23-May-17 19:57 azureml/featurestore/_feature_set.py
--rw-rw-rw-  2.0 fat       21 b- defN 23-May-17 20:05 azureml/featurestore/_version.py
--rw-rw-rw-  2.0 fat     1108 b- defN 23-May-17 19:57 azureml/featurestore/abstract_feature_store.py
--rw-rw-rw-  2.0 fat    25597 b- defN 23-May-17 19:57 azureml/featurestore/feature_set_spec.py
--rw-rw-rw-  2.0 fat    27132 b- defN 23-May-17 19:57 azureml/featurestore/feature_store_client.py
--rw-rw-rw-  2.0 fat      410 b- defN 23-May-17 19:57 azureml/featurestore/_identity/__init__.py
--rw-rw-rw-  2.0 fat     6332 b- defN 23-May-17 19:57 azureml/featurestore/_identity/aml_hobospark_on_behalf_of.py
--rw-rw-rw-  2.0 fat      455 b- defN 23-May-17 19:57 azureml/featurestore/_offline_query/__init__.py
--rw-rw-rw-  2.0 fat      401 b- defN 23-May-17 19:57 azureml/featurestore/_offline_query/offline_retrieval_job.py
--rw-rw-rw-  2.0 fat    10924 b- defN 23-May-17 19:57 azureml/featurestore/_offline_query/point_at_time.py
--rw-rw-rw-  2.0 fat      267 b- defN 23-May-17 19:57 azureml/featurestore/_utils/__init__.py
--rw-rw-rw-  2.0 fat     1549 b- defN 23-May-17 19:57 azureml/featurestore/_utils/_constants.py
--rw-rw-rw-  2.0 fat      398 b- defN 23-May-17 19:57 azureml/featurestore/_utils/_preview_method.py
--rw-rw-rw-  2.0 fat     7359 b- defN 23-May-17 19:57 azureml/featurestore/_utils/arm_id_utils.py
--rw-rw-rw-  2.0 fat     5210 b- defN 23-May-17 19:57 azureml/featurestore/_utils/materialize.py
--rw-rw-rw-  2.0 fat     9810 b- defN 23-May-17 19:57 azureml/featurestore/_utils/spark_utils.py
--rw-rw-rw-  2.0 fat     2370 b- defN 23-May-17 19:57 azureml/featurestore/_utils/type_map.py
--rw-rw-rw-  2.0 fat    15837 b- defN 23-May-17 19:57 azureml/featurestore/_utils/utils.py
--rw-rw-rw-  2.0 fat      681 b- defN 23-May-17 19:57 azureml/featurestore/contracts/__init__.py
--rw-rw-rw-  2.0 fat     1243 b- defN 23-May-17 19:57 azureml/featurestore/contracts/column.py
--rw-rw-rw-  2.0 fat     1058 b- defN 23-May-17 19:57 azureml/featurestore/contracts/datetimeoffset.py
--rw-rw-rw-  2.0 fat     3593 b- defN 23-May-17 19:57 azureml/featurestore/contracts/feature.py
--rw-rw-rw-  2.0 fat     9275 b- defN 23-May-17 19:57 azureml/featurestore/contracts/feature_retrieval_spec.py
--rw-rw-rw-  2.0 fat     2846 b- defN 23-May-17 19:57 azureml/featurestore/contracts/feature_source.py
--rw-rw-rw-  2.0 fat     2019 b- defN 23-May-17 19:57 azureml/featurestore/contracts/offline_store.py
--rw-rw-rw-  2.0 fat      646 b- defN 23-May-17 19:57 azureml/featurestore/contracts/online_store.py
--rw-rw-rw-  2.0 fat      694 b- defN 23-May-17 19:57 azureml/featurestore/contracts/partition.py
--rw-rw-rw-  2.0 fat     1230 b- defN 23-May-17 19:57 azureml/featurestore/contracts/store_connection.py
--rw-rw-rw-  2.0 fat      753 b- defN 23-May-17 19:57 azureml/featurestore/contracts/timestamp_column.py
--rw-rw-rw-  2.0 fat     2578 b- defN 23-May-17 19:57 azureml/featurestore/contracts/transformation_code.py
--rw-rw-rw-  2.0 fat      294 b- defN 23-May-17 19:57 azureml/featurestore/grpc/__init__.py
--rw-rw-rw-  2.0 fat     1364 b- defN 23-May-17 19:57 azureml/featurestore/grpc/_flight_feature_retrieval_client.py
--rw-rw-rw-  2.0 fat     3276 b- defN 23-May-17 19:57 azureml/featurestore/grpc/_flight_feature_retrieval_server.py
--rw-rw-rw-  2.0 fat     5923 b- defN 23-May-17 19:57 azureml/featurestore/grpc/_flight_helper.py
--rw-rw-rw-  2.0 fat      934 b- defN 23-May-17 19:57 azureml/featurestore/grpc/_inline_feature_retrieval_client.py
--rw-rw-rw-  2.0 fat      391 b- defN 23-May-17 19:57 azureml/featurestore/offline_store/__init__.py
--rw-rw-rw-  2.0 fat     9020 b- defN 23-May-17 19:57 azureml/featurestore/offline_store/azure_data_lake_offline_store.py
--rw-rw-rw-  2.0 fat      367 b- defN 23-May-17 19:57 azureml/featurestore/offline_store/partition/__init__.py
--rw-rw-rw-  2.0 fat     2381 b- defN 23-May-17 19:57 azureml/featurestore/offline_store/partition/timestamp_partition.py
--rw-rw-rw-  2.0 fat      242 b- defN 23-May-17 19:57 azureml/featurestore/online/__init__.py
--rw-rw-rw-  2.0 fat     5515 b- defN 23-May-17 19:57 azureml/featurestore/online/_online_feature_getter.py
--rw-rw-rw-  2.0 fat     4492 b- defN 23-May-17 19:57 azureml/featurestore/online/_online_feature_materialization.py
--rw-rw-rw-  2.0 fat     1669 b- defN 23-May-17 19:57 azureml/featurestore/online/_redis_client_pool.py
--rw-rw-rw-  2.0 fat      643 b- defN 23-May-17 19:57 azureml/featurestore/online/_utils.py
--rw-rw-rw-  2.0 fat      267 b- defN 23-May-17 19:57 azureml/featurestore/schema/__init__.py
--rw-rw-rw-  2.0 fat     1655 b- defN 23-May-17 19:57 azureml/featurestore/schema/feature_retrieval_spec_schema.py
--rw-rw-rw-  2.0 fat     5944 b- defN 23-May-17 19:57 azureml/featurestore/schema/feature_set_schema.py
--rw-rw-rw-  2.0 fat      823 b- defN 23-May-17 20:08 azureml_featurestore-0.1.0b1.dist-info/DESCRIPTION.rst
--rw-rw-rw-  2.0 fat     1142 b- defN 23-May-17 20:08 azureml_featurestore-0.1.0b1.dist-info/metadata.json
--rw-rw-rw-  2.0 fat        8 b- defN 23-May-17 20:08 azureml_featurestore-0.1.0b1.dist-info/top_level.txt
--rw-rw-rw-  2.0 fat       97 b- defN 23-May-17 20:08 azureml_featurestore-0.1.0b1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat     1921 b- defN 23-May-17 20:08 azureml_featurestore-0.1.0b1.dist-info/METADATA
--rw-rw-rw-  2.0 fat     5698 b- defN 23-May-17 20:08 azureml_featurestore-0.1.0b1.dist-info/RECORD
-55 files, 217777 bytes uncompressed, 57198 bytes compressed:  73.7%
+Zip file size: 68490 bytes, number of entries: 56
+-rw-rw-rw-  2.0 fat      267 b- defN 23-Jun-14 19:40 azureml/__init__.py
+-rw-rw-rw-  2.0 fat      917 b- defN 23-Jun-14 19:40 azureml/featurestore/__init__.py
+-rw-rw-rw-  2.0 fat    21202 b- defN 23-Jun-14 19:40 azureml/featurestore/_feature_set.py
+-rw-rw-rw-  2.0 fat       21 b- defN 23-Jun-14 19:50 azureml/featurestore/_version.py
+-rw-rw-rw-  2.0 fat     1108 b- defN 23-Jun-14 19:40 azureml/featurestore/abstract_feature_store.py
+-rw-rw-rw-  2.0 fat    25045 b- defN 23-Jun-14 19:40 azureml/featurestore/feature_set_spec.py
+-rw-rw-rw-  2.0 fat    26844 b- defN 23-Jun-14 19:40 azureml/featurestore/feature_store_client.py
+-rw-rw-rw-  2.0 fat      410 b- defN 23-Jun-14 19:40 azureml/featurestore/_identity/__init__.py
+-rw-rw-rw-  2.0 fat     6334 b- defN 23-Jun-14 19:40 azureml/featurestore/_identity/aml_hobospark_on_behalf_of.py
+-rw-rw-rw-  2.0 fat      455 b- defN 23-Jun-14 19:40 azureml/featurestore/_offline_query/__init__.py
+-rw-rw-rw-  2.0 fat      401 b- defN 23-Jun-14 19:40 azureml/featurestore/_offline_query/offline_retrieval_job.py
+-rw-rw-rw-  2.0 fat    10931 b- defN 23-Jun-14 19:40 azureml/featurestore/_offline_query/point_at_time.py
+-rw-rw-rw-  2.0 fat      267 b- defN 23-Jun-14 19:40 azureml/featurestore/_utils/__init__.py
+-rw-rw-rw-  2.0 fat     1694 b- defN 23-Jun-14 19:40 azureml/featurestore/_utils/_constants.py
+-rw-rw-rw-  2.0 fat      398 b- defN 23-Jun-14 19:40 azureml/featurestore/_utils/_preview_method.py
+-rw-rw-rw-  2.0 fat     7359 b- defN 23-Jun-14 19:40 azureml/featurestore/_utils/arm_id_utils.py
+-rw-rw-rw-  2.0 fat     2564 b- defN 23-Jun-14 19:40 azureml/featurestore/_utils/error_constants.py
+-rw-rw-rw-  2.0 fat     5732 b- defN 23-Jun-14 19:40 azureml/featurestore/_utils/materialize.py
+-rw-rw-rw-  2.0 fat     9728 b- defN 23-Jun-14 19:40 azureml/featurestore/_utils/spark_utils.py
+-rw-rw-rw-  2.0 fat     2370 b- defN 23-Jun-14 19:40 azureml/featurestore/_utils/type_map.py
+-rw-rw-rw-  2.0 fat    15684 b- defN 23-Jun-14 19:40 azureml/featurestore/_utils/utils.py
+-rw-rw-rw-  2.0 fat      681 b- defN 23-Jun-14 19:40 azureml/featurestore/contracts/__init__.py
+-rw-rw-rw-  2.0 fat     1243 b- defN 23-Jun-14 19:40 azureml/featurestore/contracts/column.py
+-rw-rw-rw-  2.0 fat     1058 b- defN 23-Jun-14 19:40 azureml/featurestore/contracts/datetimeoffset.py
+-rw-rw-rw-  2.0 fat     3593 b- defN 23-Jun-14 19:40 azureml/featurestore/contracts/feature.py
+-rw-rw-rw-  2.0 fat     9258 b- defN 23-Jun-14 19:40 azureml/featurestore/contracts/feature_retrieval_spec.py
+-rw-rw-rw-  2.0 fat     2846 b- defN 23-Jun-14 19:40 azureml/featurestore/contracts/feature_source.py
+-rw-rw-rw-  2.0 fat     2019 b- defN 23-Jun-14 19:40 azureml/featurestore/contracts/offline_store.py
+-rw-rw-rw-  2.0 fat      646 b- defN 23-Jun-14 19:40 azureml/featurestore/contracts/online_store.py
+-rw-rw-rw-  2.0 fat      694 b- defN 23-Jun-14 19:40 azureml/featurestore/contracts/partition.py
+-rw-rw-rw-  2.0 fat     1230 b- defN 23-Jun-14 19:40 azureml/featurestore/contracts/store_connection.py
+-rw-rw-rw-  2.0 fat      753 b- defN 23-Jun-14 19:40 azureml/featurestore/contracts/timestamp_column.py
+-rw-rw-rw-  2.0 fat     2530 b- defN 23-Jun-14 19:40 azureml/featurestore/contracts/transformation_code.py
+-rw-rw-rw-  2.0 fat      294 b- defN 23-Jun-14 19:40 azureml/featurestore/grpc/__init__.py
+-rw-rw-rw-  2.0 fat     1364 b- defN 23-Jun-14 19:40 azureml/featurestore/grpc/_flight_feature_retrieval_client.py
+-rw-rw-rw-  2.0 fat     3276 b- defN 23-Jun-14 19:40 azureml/featurestore/grpc/_flight_feature_retrieval_server.py
+-rw-rw-rw-  2.0 fat     5923 b- defN 23-Jun-14 19:40 azureml/featurestore/grpc/_flight_helper.py
+-rw-rw-rw-  2.0 fat      934 b- defN 23-Jun-14 19:40 azureml/featurestore/grpc/_inline_feature_retrieval_client.py
+-rw-rw-rw-  2.0 fat      391 b- defN 23-Jun-14 19:40 azureml/featurestore/offline_store/__init__.py
+-rw-rw-rw-  2.0 fat     9020 b- defN 23-Jun-14 19:40 azureml/featurestore/offline_store/azure_data_lake_offline_store.py
+-rw-rw-rw-  2.0 fat      367 b- defN 23-Jun-14 19:40 azureml/featurestore/offline_store/partition/__init__.py
+-rw-rw-rw-  2.0 fat     2381 b- defN 23-Jun-14 19:40 azureml/featurestore/offline_store/partition/timestamp_partition.py
+-rw-rw-rw-  2.0 fat      242 b- defN 23-Jun-14 19:40 azureml/featurestore/online/__init__.py
+-rw-rw-rw-  2.0 fat     5515 b- defN 23-Jun-14 19:40 azureml/featurestore/online/_online_feature_getter.py
+-rw-rw-rw-  2.0 fat     5571 b- defN 23-Jun-14 19:40 azureml/featurestore/online/_online_feature_materialization.py
+-rw-rw-rw-  2.0 fat     1669 b- defN 23-Jun-14 19:40 azureml/featurestore/online/_redis_client_pool.py
+-rw-rw-rw-  2.0 fat     1055 b- defN 23-Jun-14 19:40 azureml/featurestore/online/_utils.py
+-rw-rw-rw-  2.0 fat      267 b- defN 23-Jun-14 19:40 azureml/featurestore/schema/__init__.py
+-rw-rw-rw-  2.0 fat     1655 b- defN 23-Jun-14 19:40 azureml/featurestore/schema/feature_retrieval_spec_schema.py
+-rw-rw-rw-  2.0 fat     5944 b- defN 23-Jun-14 19:40 azureml/featurestore/schema/feature_set_schema.py
+-rw-rw-rw-  2.0 fat     1047 b- defN 23-Jun-14 19:53 azureml_featurestore-0.1.0b2.dist-info/DESCRIPTION.rst
+-rw-rw-rw-  2.0 fat     1142 b- defN 23-Jun-14 19:53 azureml_featurestore-0.1.0b2.dist-info/metadata.json
+-rw-rw-rw-  2.0 fat        8 b- defN 23-Jun-14 19:53 azureml_featurestore-0.1.0b2.dist-info/top_level.txt
+-rw-rw-rw-  2.0 fat       97 b- defN 23-Jun-14 19:53 azureml_featurestore-0.1.0b2.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat     2145 b- defN 23-Jun-14 19:53 azureml_featurestore-0.1.0b2.dist-info/METADATA
+-rw-rw-rw-  2.0 fat     5804 b- defN 23-Jun-14 19:53 azureml_featurestore-0.1.0b2.dist-info/RECORD
+56 files, 222393 bytes uncompressed, 58954 bytes compressed:  73.5%
```

## zipnote {}

```diff
@@ -42,14 +42,17 @@
 
 Filename: azureml/featurestore/_utils/_preview_method.py
 Comment: 
 
 Filename: azureml/featurestore/_utils/arm_id_utils.py
 Comment: 
 
+Filename: azureml/featurestore/_utils/error_constants.py
+Comment: 
+
 Filename: azureml/featurestore/_utils/materialize.py
 Comment: 
 
 Filename: azureml/featurestore/_utils/spark_utils.py
 Comment: 
 
 Filename: azureml/featurestore/_utils/type_map.py
@@ -141,26 +144,26 @@
 
 Filename: azureml/featurestore/schema/feature_retrieval_spec_schema.py
 Comment: 
 
 Filename: azureml/featurestore/schema/feature_set_schema.py
 Comment: 
 
-Filename: azureml_featurestore-0.1.0b1.dist-info/DESCRIPTION.rst
+Filename: azureml_featurestore-0.1.0b2.dist-info/DESCRIPTION.rst
 Comment: 
 
-Filename: azureml_featurestore-0.1.0b1.dist-info/metadata.json
+Filename: azureml_featurestore-0.1.0b2.dist-info/metadata.json
 Comment: 
 
-Filename: azureml_featurestore-0.1.0b1.dist-info/top_level.txt
+Filename: azureml_featurestore-0.1.0b2.dist-info/top_level.txt
 Comment: 
 
-Filename: azureml_featurestore-0.1.0b1.dist-info/WHEEL
+Filename: azureml_featurestore-0.1.0b2.dist-info/WHEEL
 Comment: 
 
-Filename: azureml_featurestore-0.1.0b1.dist-info/METADATA
+Filename: azureml_featurestore-0.1.0b2.dist-info/METADATA
 Comment: 
 
-Filename: azureml_featurestore-0.1.0b1.dist-info/RECORD
+Filename: azureml_featurestore-0.1.0b2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## azureml/featurestore/__init__.py

```diff
@@ -10,7 +10,18 @@
 
 __all__ = [
     "create_feature_set_spec",
     "get_offline_features",
     "FeatureSetSpec",
     "FeatureStoreClient",
 ]
+
+from azureml.featurestore._utils._preview_method import _is_private_preview_enabled
+
+if _is_private_preview_enabled():
+    from .feature_store_client import init_online_lookup, shutdown_online_lookup, get_online_features
+
+    __all__ += [
+        "init_online_lookup",
+        "shutdown_online_lookup",
+        "get_online_features",
+    ]
```

## azureml/featurestore/_feature_set.py

```diff
@@ -8,14 +8,20 @@
 from datetime import datetime
 from pathlib import Path
 from typing import Dict, List, Optional, Union
 
 import requests
 from azureml.featurestore import FeatureSetSpec
 from azureml.featurestore._utils._constants import PACKAGE_NAME, PARTITION_COLUMN
+from azureml.featurestore._utils.error_constants import (
+    FEATURE_NAME_NOT_FOUND_FEATURE_SET,
+    FEATURE_NAME_NOT_STRING_FEATURE_SET,
+    FEATURE_SET_NOT_REGISTERED,
+    MISSING_TIMESTAMP_COLUMN,
+)
 from azureml.featurestore._utils.utils import _build_logger
 from azureml.featurestore.contracts.column import Column, ColumnType
 from marshmallow import EXCLUDE
 
 from azure.ai.ml import MLClient
 from azure.ai.ml._exception_helper import log_and_raise_error
 from azure.ai.ml._restclient.v2023_04_01_preview.models import FeaturesetVersion
@@ -131,32 +137,29 @@
         return "FeatureSet\n{}".format(formatted_info)
 
     def __str__(self):
         return self.__repr__()
 
     def get_feature(self, name: str):
         if not isinstance(name, str):
-            msg = "Name must be the string name of a feature in this feature set. Found: {}"
             raise ValidationException(
-                message=msg.format(type(name)),
-                no_personal_data_message="Name must be the string name of a feature in this feature set.",
+                message=FEATURE_NAME_NOT_STRING_FEATURE_SET.format(type(name)),
+                no_personal_data_message=FEATURE_NAME_NOT_STRING_FEATURE_SET,
                 error_type=ValidationErrorType.INVALID_VALUE,
                 error_category=ErrorCategory.USER_ERROR,
                 target=ErrorTarget.GENERAL,
             )
 
         for feature in self.features:
             if feature.name == name:
-                feature.feature_set_reference = self
                 return feature
 
-        msg = "Feature '{}' not found in this feature set."
         raise ValidationException(
-            message=msg.format(name),
-            no_personal_data_message="Feature is not found in this feature set.",
+            message=FEATURE_NAME_NOT_FOUND_FEATURE_SET.format(name),
+            no_personal_data_message=FEATURE_NAME_NOT_FOUND_FEATURE_SET,
             error_type=ValidationErrorType.INVALID_VALUE,
             error_category=ErrorCategory.USER_ERROR,
             target=ErrorTarget.GENERAL,
         )
 
     def __hash__(self):
         return hash(self.name)
@@ -208,15 +211,19 @@
 
     @property
     def entities(self):
         return self._entities
 
     @property
     def features(self):
-        return self.__feature_set_spec.features
+        def with_feature_set_ref(feature):
+            feature.feature_set_reference = self
+            return feature
+
+        return [with_feature_set_ref(feature) for feature in self.__feature_set_spec.features]
 
     @property
     def timestamp_column(self):
         return self.__feature_set_spec.source.timestamp_column
 
     @property
     def stage(self):
@@ -461,30 +468,28 @@
                     online_store_target=featurestore.online_store.target,
                 )
 
         return featureset
 
     def validate(self):
         if not self.__is_registered:
-            msg = "FeatureSet object must be registered as asset to do this operation."
             raise ValidationException(
-                message=msg,
+                message=FEATURE_SET_NOT_REGISTERED,
                 target=ErrorTarget.GENERAL,
-                no_personal_data_message=msg,
+                no_personal_data_message=FEATURE_SET_NOT_REGISTERED,
                 error_category=ErrorCategory.USER_ERROR,
                 error_type=ValidationErrorType.CANNOT_PARSE,
             )
 
     def get_timestamp_column(self):
         if not self.timestamp_column:
             # TODO: Suppport Non-timeseries data [prp2]
-            msg = "Expected timestamp columns not found in featureset {}."
             raise ValidationException(
-                message=msg.format(self.name),
-                no_personal_data_message=msg,
+                message=MISSING_TIMESTAMP_COLUMN.format(self.name),
+                no_personal_data_message=MISSING_TIMESTAMP_COLUMN,
                 error_type=ValidationErrorType.MISSING_FIELD,
                 error_category=ErrorCategory.USER_ERROR,
                 target=ErrorTarget.GENERAL,
             )
 
         return self.timestamp_column.name, self.timestamp_column.format
```

## azureml/featurestore/_version.py

```diff
@@ -1 +1 @@
-VERSION = "0.1.0b1"
+VERSION = "0.1.0b2"
```

## azureml/featurestore/feature_set_spec.py

```diff
@@ -12,14 +12,28 @@
 
 import yaml
 from azureml.featurestore._utils._constants import (
     FEATURE_SET_SPEC_YAML_FILENAME,
     FEATURE_SET_SPEC_YAML_FILENAME_FALLBACK,
     PACKAGE_NAME,
 )
+from azureml.featurestore._utils.error_constants import (
+    DESTINATION_NOT_EXIST,
+    DESTINATION_NOT_LOCAL_PATH,
+    EMPTY_FEATURE_MESSAGE,
+    FEATURE_NAME_NOT_FOUND,
+    FEATURE_NAME_NOT_STRING,
+    FILE_ALREADY_EXIST,
+    MISSING_FEATURE_SOURCE,
+    MISSING_INDEX_COLUMN,
+    MISSING_TIMESTAMP_COLUMN,
+    PATH_NOT_EXISTING_FOLDER,
+    SCHEMA_ERROR_NO_INDEX_COLUMN,
+    SCHEMA_ERROR_WRONG_DATA_TYPE,
+)
 from azureml.featurestore._utils.utils import (
     PathType,
     _build_logger,
     _parse_path_format,
     _process_path,
     _strip_local_path,
 )
@@ -111,29 +125,27 @@
             self._spec_folder_path = None
 
             # Generated name and version for offline join
             self.__name = hashlib.md5(self.__str__().encode()).hexdigest()
             self.__version = "1"
 
             if not self.source:
-                msg = "Feature source is required for a feature set, please provide a feature source"
                 raise ValidationException(
-                    message=msg,
+                    message=MISSING_FEATURE_SOURCE,
                     target=ErrorTarget.FEATURE_SET,
-                    no_personal_data_message=msg,
+                    no_personal_data_message=MISSING_FEATURE_SOURCE,
                     error_category=ErrorCategory.USER_ERROR,
                     error_type=ValidationErrorType.MISSING_FIELD,
                 )
 
             if len(self.index_columns) == 0:
-                msg = "Index columns is required for a feature set, please provide non empty index columns"
                 raise ValidationException(
-                    message=msg,
+                    message=MISSING_INDEX_COLUMN,
                     target=ErrorTarget.FEATURE_SET,
-                    no_personal_data_message=msg,
+                    no_personal_data_message=MISSING_INDEX_COLUMN,
                     error_category=ErrorCategory.USER_ERROR,
                     error_type=ValidationErrorType.MISSING_FIELD,
                 )
 
     def __repr__(self):
         info = OrderedDict()
         info["source"] = self.source.__repr__()
@@ -147,32 +159,30 @@
         return "FeatureSetSpec\n{}".format(formatted_info)
 
     def __str__(self):
         return self.__repr__()
 
     def get_feature(self, name: str):
         if not isinstance(name, str):
-            msg = "Name must be the string name of a feature in this feature set spec. Found: {}"
             raise ValidationException(
-                message=msg.format(type(name)),
-                no_personal_data_message=msg,
+                message=FEATURE_NAME_NOT_STRING.format(type(name)),
+                no_personal_data_message=FEATURE_NAME_NOT_STRING,
                 error_type=ValidationErrorType.INVALID_VALUE,
                 error_category=ErrorCategory.USER_ERROR,
                 target=ErrorTarget.GENERAL,
             )
 
         for feature in self.features:
             if feature.name == name:
                 feature.feature_set_reference = self
                 return feature
 
-        msg = "Feature '{}' not found in this feature set spec."
         raise ValidationException(
-            message=msg.format(name),
-            no_personal_data_message=msg,
+            message=FEATURE_NAME_NOT_FOUND.format(name),
+            no_personal_data_message=FEATURE_NAME_NOT_FOUND,
             error_type=ValidationErrorType.INVALID_VALUE,
             error_category=ErrorCategory.USER_ERROR,
             target=ErrorTarget.GENERAL,
         )
 
     @property
     def name(self):
@@ -186,28 +196,26 @@
         """Dump the feature set spec into a file in yaml format. Destination mush be a folder path, the spec file name is assumed as FeatureSetSpec.yaml, and an exception is raised if the file exists.
         If there is a transformation code specified, it will be copied to destination folder as a subfolder named 'code', and an exception is raised if code folder exists.
         :param dest: The folder path destination to receive this spec.
         :type dest: Union[PathLike, str]
         """
         path_type, _ = _parse_path_format(dest)
         if path_type != PathType.local:
-            msg = "Destination {} must be local path"
             raise ValidationException(
-                message=msg.format(dest),
+                message=DESTINATION_NOT_LOCAL_PATH.format(dest),
                 target=ErrorTarget.FEATURE_SET,
-                no_personal_data_message="Destination must be a local folder path",
+                no_personal_data_message=DESTINATION_NOT_LOCAL_PATH,
                 error_category=ErrorCategory.USER_ERROR,
                 error_type=ValidationErrorType.INVALID_VALUE,
             )
         if not os.path.isdir(dest):
-            msg = "Destination {} must be an existing folder path"
             raise ValidationException(
-                message=msg.format(dest),
+                message=DESTINATION_NOT_EXIST.format(dest),
                 target=ErrorTarget.FEATURE_SET,
-                no_personal_data_message="Destination must be an existing folder path",
+                no_personal_data_message=DESTINATION_NOT_EXIST,
                 error_category=ErrorCategory.USER_ERROR,
                 error_type=ValidationErrorType.INVALID_VALUE,
             )
 
         if self.feature_transformation_code:
             import shutil
 
@@ -221,19 +229,18 @@
             self.feature_transformation_code.path = origin_code_path
         else:
             yaml_serialized = self._to_dict()
 
         dest = os.path.join(dest, FEATURE_SET_SPEC_YAML_FILENAME)
 
         if os.path.isfile(dest):
-            msg = "Spec file {} already exists"
             raise ValidationException(
-                message=msg.format(dest),
+                message=FILE_ALREADY_EXIST.format(dest),
                 target=ErrorTarget.FEATURE_SET,
-                no_personal_data_message="Spec file already exists",
+                no_personal_data_message=FILE_ALREADY_EXIST,
                 error_category=ErrorCategory.USER_ERROR,
                 error_type=ValidationErrorType.INVALID_VALUE,
             )
 
         dump_yaml_to_file(dest, yaml_serialized, default_flow_style=False, **kwargs)
 
     def _to_dict(self) -> Dict:
@@ -263,34 +270,32 @@
         cls, spec_path: Union[str, PathLike], datastore_operations: DatastoreOperations = None
     ) -> "FeatureSetSpec":
         """Load a feature set spec from yaml config. Spec path must be a folder path, the spec file name is assumed as FeatureSetSpec.yaml
         :param spec_path: The path to fetch this spec.
         :type spec_path: Union[str, PathLike]
         """
         try:
-            local_spec_path = _process_path(path=spec_path, is_folder=True, datastore_operations=datastore_operations)
+            spec_folder_path = _process_path(path=spec_path, datastore_operations=datastore_operations)
 
-            if not os.path.isdir(local_spec_path):
-                msg = "Spec path {} must be an existing folder path"
+            if not os.path.isdir(spec_folder_path):
                 raise ValidationException(
-                    message=msg.format(spec_path),
+                    message=PATH_NOT_EXISTING_FOLDER.format(spec_path),
                     target=ErrorTarget.FEATURE_SET,
-                    no_personal_data_message="Spec path must be an existing folder path",
+                    no_personal_data_message=PATH_NOT_EXISTING_FOLDER,
                     error_category=ErrorCategory.USER_ERROR,
                     error_type=ValidationErrorType.INVALID_VALUE,
                 )
-            spec_folder_path = local_spec_path
-            local_spec_path = os.path.join(local_spec_path, FEATURE_SET_SPEC_YAML_FILENAME)
 
+            local_spec_path = os.path.join(spec_folder_path, FEATURE_SET_SPEC_YAML_FILENAME)
             try:
                 with open(local_spec_path) as f:
                     cfg = yaml.safe_load(f)
             except FileNotFoundError:
                 # Fall back to previous naming format
-                local_spec_path = os.path.join(local_spec_path, FEATURE_SET_SPEC_YAML_FILENAME_FALLBACK)
+                local_spec_path = os.path.join(spec_folder_path, FEATURE_SET_SPEC_YAML_FILENAME_FALLBACK)
                 with open(local_spec_path) as f:
                     cfg = yaml.safe_load(f)
             except yaml.YAMLError as ex:
                 raise ValueError(str(ex)) from ex
 
             spec = FeatureSetSpec._load(cfg, local_spec_path)
             if spec.feature_transformation_code:
@@ -410,19 +415,18 @@
             log_and_raise_error(error=ex, debug=True)
 
     def get_index_columns(self):
         return self.index_columns
 
     def get_timestamp_column(self):
         if not self.source.timestamp_column:
-            # TODO: Suppport Non-timeseries data [prp2]
-            msg = "Expected timestamp columns not found in feature set {}."
+            # TODO: Suppport Non-timeseries data
             raise ValidationException(
-                message=msg.format(self.name),
-                no_personal_data_message=msg,
+                message=MISSING_TIMESTAMP_COLUMN.format(self.name),
+                no_personal_data_message=MISSING_TIMESTAMP_COLUMN,
                 error_type=ValidationErrorType.MISSING_FIELD,
                 error_category=ErrorCategory.USER_ERROR,
                 target=ErrorTarget.GENERAL,
             )
 
         return self.source.timestamp_column.name, self.source.timestamp_column.format
 
@@ -433,36 +437,30 @@
         for feature in self.features:
             if feature.name not in columns_set:
                 raise Exception("Schema check errors, no feature column: {} in output dataframe".format(feature.name))
             data_type = TypeMap.spark_to_column_type(df.schema[feature.name].dataType.typeName())
             expected_data_type = feature.type
             if data_type != expected_data_type:
                 raise ValidationException(
-                    message="Schema check errors, feature column: {} has data type: {}, expected: {}".format(
-                        feature.name, data_type, expected_data_type
-                    ),
-                    no_personal_data_message="Schema check errors, feature column data type mismatch.",
+                    message=SCHEMA_ERROR_WRONG_DATA_TYPE.format(feature.name, data_type, expected_data_type),
+                    no_personal_data_message=SCHEMA_ERROR_WRONG_DATA_TYPE,
                     error_type=ValidationErrorType.INVALID_VALUE,
                     error_category=ErrorCategory.USER_ERROR,
                     target=ErrorTarget.GENERAL,
                 )
 
         for index_column in self.index_columns:
             if index_column.name not in columns_set:
-                raise Exception(
-                    "Schema check errors, no index column: {} in output dataframe".format(index_column.name)
-                )
+                raise Exception(SCHEMA_ERROR_NO_INDEX_COLUMN.format(index_column.name))
             data_type = TypeMap.spark_to_column_type(df.schema[index_column.name].dataType.typeName())
             expected_data_type = index_column.type
             if data_type != expected_data_type:
                 raise ValidationException(
-                    message="Schema check errors, index column: {} has data type: {}, expected: {}".format(
-                        index_column.name, data_type, expected_data_type
-                    ),
-                    no_personal_data_message="Schema check errors, index column data type mismatch.",
+                    message=SCHEMA_ERROR_WRONG_DATA_TYPE.format(index_column.name, data_type, expected_data_type),
+                    no_personal_data_message=SCHEMA_ERROR_WRONG_DATA_TYPE,
                     error_type=ValidationErrorType.INVALID_VALUE,
                     error_category=ErrorCategory.USER_ERROR,
                     target=ErrorTarget.GENERAL,
                 )
 
 
 @monitor_with_activity(_get_logger(), f"{PACKAGE_NAME}->CreateFeatureSetSpec", ActivityType.PUBLICAPI)
@@ -527,23 +525,22 @@
 
             for column_name in df.columns:
                 if column_name != timestamp_column and column_name not in index_columns_set:
                     data_type = TypeMap.spark_to_column_type(df.schema[column_name].dataType.typeName())
                     features.append(Feature(name=column_name, type=data_type))
             feature_set_spec.features = features
 
-            if len(feature_set_spec.features) == 0:
-                msg = "Inferred feature set spec has empty features, please check feature source data."
-                raise ValidationException(
-                    message=msg,
-                    target=ErrorTarget.FEATURE_SET,
-                    no_personal_data_message=msg,
-                    error_category=ErrorCategory.USER_ERROR,
-                    error_type=ValidationErrorType.INVALID_VALUE,
-                )
+        if len(feature_set_spec.features) == 0:
+            raise ValidationException(
+                message=EMPTY_FEATURE_MESSAGE,
+                target=ErrorTarget.FEATURE_SET,
+                no_personal_data_message=EMPTY_FEATURE_MESSAGE,
+                error_category=ErrorCategory.USER_ERROR,
+                error_type=ValidationErrorType.INVALID_VALUE,
+            )
 
         return feature_set_spec
     except Exception as ex:
         if isinstance(ex, MlException):
             _get_logger().error(
                 f"{PACKAGE_NAME}->CreateFeatureSetSpec, {type(ex).__name__}: {ex.no_personal_data_message}"
             )
```

## azureml/featurestore/feature_store_client.py

```diff
@@ -4,14 +4,21 @@
 from os import PathLike
 from typing import IO, AnyStr, Dict, List, Optional, Union
 
 from azureml.featurestore._feature_set import FeatureSet
 from azureml.featurestore._offline_query import PointAtTimeRetrievalJob
 from azureml.featurestore._utils._constants import PACKAGE_NAME, QUERY_MODE_DEFAULT, QUERY_MODE_KEY
 from azureml.featurestore._utils._preview_method import _is_private_preview_enabled
+from azureml.featurestore._utils.error_constants import (
+    EMPTY_FEATURE_MESSAGE,
+    FEATURE_STORE_CLIENT_INCORRECT_SETUP,
+    FEATURE_WRONG_TYPE,
+    NOT_A_FEATURE_STORE,
+    UNSUPPORTED_QUERY_MODE,
+)
 from azureml.featurestore._utils.utils import (
     _build_logger,
     feature_uri_parser_with_rename,
     resolve_features,
     validate_features,
 )
 from azureml.featurestore.abstract_feature_store import AbstractFeatureStore
@@ -91,19 +98,18 @@
                     self._feature_stores = FeatureStoreClient.FeatureStoreDataplaneOperations(ml_client=self._ml_client)
                     self._feature_store_entities = FeatureStoreClient.FeatureStoreEntityDataplaneOperations(
                         ml_client=self._ml_client
                     )
                     self._feature_sets = FeatureStoreClient.FeatureSetDataplaneOperations(ml_client=self._ml_client)
 
                     if not self._feature_stores.get():
-                        msg = "{} is not a Feature Store workspace."
                         raise ValidationException(
-                            message=msg.format(name),
+                            message=NOT_A_FEATURE_STORE.format(name),
                             target=ErrorTarget.WORKSPACE,
-                            no_personal_data_message=msg,
+                            no_personal_data_message=NOT_A_FEATURE_STORE,
                             error_category=ErrorCategory.USER_ERROR,
                             error_type=ValidationErrorType.RESOURCE_NOT_FOUND,
                         )
 
         except Exception as ex:
             if isinstance(ex, MlException):
                 _get_logger().error(
@@ -122,98 +128,24 @@
 
     @monitor_with_activity(_get_logger(), f"{PACKAGE_NAME}->Resolve_FeatureRetrievalSpec", ActivityType.PUBLICAPI)
     def resolve_feature_retrieval_spec(self, path: Union[str, PathLike, IO[AnyStr]]):
         feature_retrieval_spec = FeatureRetrievalSpec.from_config(path)
         features = feature_retrieval_spec.resolve_to_features(self._credential)
         return features
 
-    if _is_private_preview_enabled():
-
-        @staticmethod
-        def init_online_lookup(features: List[Feature], credential=None, force=False):
-            if not credential:
-                from azure.identity import DefaultAzureCredential
-
-                credential = DefaultAzureCredential()
-
-            from azureml.featurestore.grpc import initialize
-
-            initialize(features, credential, force)
-
-        @staticmethod
-        def shutdown_online_lookup():
-            from azureml.featurestore.grpc import shutdown
-
-            shutdown()
-
-    if _is_private_preview_enabled():
-
-        @staticmethod
-        @monitor_with_activity(
-            _get_logger(), f"{PACKAGE_NAME}->FeatureStoreClient.Get_OnlineFeatures", ActivityType.PUBLICAPI
-        )
-        def get_online_features(features: List[Feature], observation_data):
-            """Join online features in a dataframe.
-
-            Enrich an entity dataframe with online feature values for batch scoring.
-            This method joins online feature data from one or more feature sets to an entity dataframe.
-            Each feature set is joined to the entity dataframe using all entities configured for the respective feature set.
-            All configured entities must be available in the entity dataframe. Therefore, the entity dataframe must
-            contain all entities found in all feature sets, but the individual feature sets can have different entities.
-            Args:
-                observation_data: (pandas.DataFrame): An entity dataframe is a collection of rows containing all entity
-                    columns (e.g., customer_id, driver_id) on which features need to be joined. The entity dataframe is a pandas DataFrame.
-                features: The list of features that should be retrieved from the online store. Feature is obtained by feature setspec['feature_name']
-            Returns:
-                pandas DataFrame which can be used to show the results.
-            """
-
-            try:
-                if not all(isinstance(n, Feature) for n in features):
-                    msg = "Features must be of type 'Feature'. Did you run `resolve_feature_uri()`?"
-                    raise ValidationException(
-                        message=msg,
-                        target=ErrorTarget.GENERAL,
-                        no_personal_data_message=msg,
-                        error_category=ErrorCategory.USER_ERROR,
-                        error_type=ValidationErrorType.MISSING_FIELD,
-                    )
-
-                from azureml.featurestore.grpc import get_online_features as get_online_features_impl
-                from azureml.featurestore.grpc import is_initialized
-
-                validate_features(features)
-
-                if not is_initialized():
-                    FeatureStoreClient.init_online_lookup(features)
-
-                return get_online_features_impl(features, observation_data)
-            except Exception as ex:
-                if isinstance(ex, MlException):
-                    _get_logger().error(
-                        f"{PACKAGE_NAME}->FeatureStoreClient.Get_OnlineFeatures, {type(ex).__name__}: {ex.no_personal_data_message}"
-                    )
-                else:
-                    _get_logger().error(
-                        f"{PACKAGE_NAME}->FeatureStoreClient.Get_OnlineFeatures, {type(ex).__name__}: {ex}"
-                    )
-
-                log_and_raise_error(error=ex, debug=True)
-
     @monitor_with_activity(
         _get_logger(), f"{PACKAGE_NAME}->FeatureStoreClient.Resolve_FeatureUri", ActivityType.PUBLICAPI
     )
     def resolve_feature_uri(self, feature_uris: List[str], **kwargs: Dict) -> List[Feature]:
         try:
             if not feature_uris:
-                msg = "Must provide features."
                 raise ValidationException(
-                    message=msg,
+                    message=EMPTY_FEATURE_MESSAGE,
                     target=ErrorTarget.GENERAL,
-                    no_personal_data_message=msg,
+                    no_personal_data_message=EMPTY_FEATURE_MESSAGE,
                     error_category=ErrorCategory.USER_ERROR,
                     error_type=ValidationErrorType.MISSING_FIELD,
                 )
 
             features = []
 
             for uri in feature_uris:
@@ -452,57 +384,54 @@
     def feature_stores(self) -> FeatureStoreDataplaneOperations:
         """A collection of feature store related operations.
 
         :return: Feature store operations
         :rtype: FeatureStoreDataplaneOperations
         """
         if not self._feature_stores:
-            msg = "FeatureStoreClient was not configured with subscription, resource group and workspace information"
             raise ValidationException(
-                message=msg,
+                message=FEATURE_STORE_CLIENT_INCORRECT_SETUP,
                 target=ErrorTarget.GENERAL,
-                no_personal_data_message=msg,
+                no_personal_data_message=FEATURE_STORE_CLIENT_INCORRECT_SETUP,
                 error_category=ErrorCategory.USER_ERROR,
                 error_type=ValidationErrorType.MISSING_FIELD,
             )
 
         return self._feature_stores
 
     @property
     def feature_store_entities(self) -> FeatureStoreEntityDataplaneOperations:
         """A collection of workspace related operations.
 
         :return: Feature store entity operations
         :rtype: FeatureStoreEntityDataplaneOperations
         """
         if not self._feature_store_entities:
-            msg = "FeatureStoreClient was not configured with subscription, resource group and workspace information"
             raise ValidationException(
-                message=msg,
+                message=FEATURE_STORE_CLIENT_INCORRECT_SETUP,
                 target=ErrorTarget.GENERAL,
-                no_personal_data_message=msg,
+                no_personal_data_message=FEATURE_STORE_CLIENT_INCORRECT_SETUP,
                 error_category=ErrorCategory.USER_ERROR,
                 error_type=ValidationErrorType.MISSING_FIELD,
             )
 
         return self._feature_store_entities
 
     @property
     def feature_sets(self) -> FeatureSetDataplaneOperations:
         """A collection of workspace related operations.
 
         :return: Feature set operations
         :rtype: FeatureSetDataplaneOperations
         """
         if not self._feature_sets:
-            msg = "FeatureStoreClient was not configured with subscription, resource group and workspace information"
             raise ValidationException(
-                message=msg,
+                message=FEATURE_STORE_CLIENT_INCORRECT_SETUP,
                 target=ErrorTarget.GENERAL,
-                no_personal_data_message=msg,
+                no_personal_data_message=FEATURE_STORE_CLIENT_INCORRECT_SETUP,
                 error_category=ErrorCategory.USER_ERROR,
                 error_type=ValidationErrorType.MISSING_FIELD,
             )
 
         return self._feature_sets
 
 
@@ -530,19 +459,18 @@
         use_materialized_data: When set to true, sdk will first try to pull feature data from offline store and fallback to run the query through if None is in offline store
     Returns:
         Spark Dataframe which can be used to show the results.
     """
 
     try:
         if not all(isinstance(n, Feature) for n in features):
-            msg = "Features must be of type 'Feature'. Did you run `resolve_feature_uri()`?"
             raise ValidationException(
-                message=msg,
+                message=FEATURE_WRONG_TYPE,
                 target=ErrorTarget.GENERAL,
-                no_personal_data_message=msg,
+                no_personal_data_message=FEATURE_WRONG_TYPE,
                 error_category=ErrorCategory.USER_ERROR,
                 error_type=ValidationErrorType.MISSING_FIELD,
             )
 
         validate_features(features)
 
         feature_references, feature_sets = resolve_features(features)
@@ -552,33 +480,99 @@
         if QUERY_MODE_KEY in kwargs:
             query_mode = kwargs[QUERY_MODE_KEY]
 
         if query_mode == QUERY_MODE_DEFAULT:
             job = PointAtTimeRetrievalJob(
                 feature_sets=feature_sets,
                 feature_references=feature_references,
-                features = features,
+                features=features,
                 observation_data=observation_data,
                 timestamp_column=timestamp_column,
                 use_materialized_data=use_materialized_data,
                 **kwargs,
             )
         else:
-            msg = f"query mode {query_mode} is not supported."
             raise ValidationException(
-                message=msg,
+                message=UNSUPPORTED_QUERY_MODE.format(query_mode),
                 target=ErrorTarget.GENERAL,
-                no_personal_data_message=msg,
+                no_personal_data_message=UNSUPPORTED_QUERY_MODE.format(query_mode),
                 error_category=ErrorCategory.USER_ERROR,
                 error_type=ValidationErrorType.INVALID_VALUE,
             )
 
         return job.to_spark_dataframe()
     except Exception as ex:
         if isinstance(ex, MlException):
             _get_logger().error(
                 f"{PACKAGE_NAME}->Get_OfflineFeatures, {type(ex).__name__}: {ex.no_personal_data_message}"
             )
         else:
             _get_logger().error(f"{PACKAGE_NAME}->Get_OfflineFeatures, {type(ex).__name__}: {ex}")
 
         log_and_raise_error(error=ex, debug=True)
+
+
+if _is_private_preview_enabled():
+
+    @monitor_with_activity(_get_logger(), f"{PACKAGE_NAME}->Init_OnlineLookup", ActivityType.PUBLICAPI)
+    def init_online_lookup(features: List[Feature], credential=None, force=False):
+        if not credential:
+            from azure.identity import DefaultAzureCredential
+
+            credential = DefaultAzureCredential()
+
+        from azureml.featurestore.grpc import initialize
+
+        initialize(features, credential, force)
+
+    @monitor_with_activity(_get_logger(), f"{PACKAGE_NAME}->Shutdown_OnlineLookup", ActivityType.PUBLICAPI)
+    def shutdown_online_lookup():
+        from azureml.featurestore.grpc import shutdown
+
+        shutdown()
+
+    @monitor_with_activity(_get_logger(), f"{PACKAGE_NAME}->Get_OnlineFeatures", ActivityType.PUBLICAPI)
+    def get_online_features(features: List[Feature], observation_data):
+        """Join online features in a dataframe.
+
+        Enrich an entity dataframe with online feature values for batch scoring.
+        This method joins online feature data from one or more feature sets to an entity dataframe.
+        Each feature set is joined to the entity dataframe using all entities configured for the respective feature set.
+        All configured entities must be available in the entity dataframe. Therefore, the entity dataframe must
+        contain all entities found in all feature sets, but the individual feature sets can have different entities.
+        Args:
+            observation_data: (pandas.DataFrame): An entity dataframe is a collection of rows containing all entity
+                columns (e.g., customer_id, driver_id) on which features need to be joined. The entity dataframe is a pandas DataFrame.
+            features: The list of features that should be retrieved from the online store. Feature is obtained by feature setspec['feature_name']
+        Returns:
+            pandas DataFrame which can be used to show the results.
+        """
+
+        try:
+            if not all(isinstance(n, Feature) for n in features):
+                msg = "Features must be of type 'Feature'. Did you run `resolve_feature_uri()`?"
+                raise ValidationException(
+                    message=msg,
+                    target=ErrorTarget.GENERAL,
+                    no_personal_data_message=msg,
+                    error_category=ErrorCategory.USER_ERROR,
+                    error_type=ValidationErrorType.MISSING_FIELD,
+                )
+
+            from azureml.featurestore.grpc import get_online_features as get_online_features_impl
+            from azureml.featurestore.grpc import is_initialized
+
+            validate_features(features)
+
+            if not is_initialized():
+                init_online_lookup(features)
+
+            return get_online_features_impl(features, observation_data)
+        except Exception as ex:
+            if isinstance(ex, MlException):
+                _get_logger().error(
+                    f"{PACKAGE_NAME}->Get_OnlineFeatures, {type(ex).__name__}: {ex.no_personal_data_message}"
+                )
+            else:
+                _get_logger().error(f"{PACKAGE_NAME}->Get_OnlineFeatures, {type(ex).__name__}: {ex}")
+
+            log_and_raise_error(error=ex, debug=True)
```

## azureml/featurestore/_identity/aml_hobospark_on_behalf_of.py

```diff
@@ -94,15 +94,15 @@
         self.tid = os.environ.get("TID")
 
         if not self.obo_access_token:
             return None
 
     def get_token(self, *scopes, **kwargs):  # type: (*str, **Any) -> AccessToken
         resource = _scopes_to_resource(*scopes)
-        request_url = "https://{}/api/v1/proxy/obotoken/v1.0/subscriptions/{}/resourceGroups/{}/providers/Microsoft.MachineLearningServices/workspaces/{}/getuseraccesstokenforrun".format(
+        request_url = "https://{}/api/v1/proxy/obotoken/v1.0/subscriptions/{}/resourceGroups/{}/providers/Microsoft.MachineLearningServices/workspaces/{}/getuseraccesstokenforspark".format(
             self.token_service_endpoint,
             self.subscription_id,
             self.resource_group,
             self.workspace_name,
         )
 
         request_body = {
```

## azureml/featurestore/_offline_query/point_at_time.py

```diff
@@ -1,27 +1,27 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 from typing import List, Set
 
 from azureml.featurestore._feature_set import FeatureSet
-from azureml.featurestore.contracts.feature import Feature
 from azureml.featurestore._offline_query.offline_retrieval_job import OfflineRetrievalJob
 from azureml.featurestore._utils._constants import (
     COL_OBSERVATION_ENTITY_TIMESTAMP,
     COL_OBSERVATION_FEATURE_SET_UNIQUE_ROW_ID,
     CREATE_TIMESTAMP_COLUMN,
     QUERY_APPLY_SOURCE_DELAY_KEY,
     QUERY_APPLY_TEMPORAL_JOIN_LOOKBACK_KEY,
 )
 from azureml.featurestore._utils.utils import (
     get_feature_set_to_features_map,
     infer_event_timestamp_range,
     validate_expected_columns_in_entity_df,
 )
+from azureml.featurestore.contracts.feature import Feature
 
 
 class PointAtTimeRetrievalJob(OfflineRetrievalJob):
     def __init__(
         self,
         feature_sets: Set[FeatureSet],
         feature_references: List[str],
@@ -107,15 +107,15 @@
         ## process each feature set dataframe
         all_fset_df = {}
         for feature_set in self.feature_sets:
             df = self._process_feature_set(
                 observation_df,
                 feature_set,
                 feature_set_to_features_map[feature_set],
-                feature_set_unique_row_id_col_names[feature_set],
+                f"`{feature_set_unique_row_id_col_names[feature_set]}`",
             )
 
             all_fset_df[feature_set] = df
 
         # left join each processed feature set df into the observation df
         # join key is the unique row id corresponding to each feature set
         for feature_set, df in all_fset_df.items():
```

## azureml/featurestore/_utils/_constants.py

```diff
@@ -24,21 +24,25 @@
 STORAGE_URI_PATTERNS = r"adl://|wasbs?://|abfss?://"
 
 DATALAKE_URI_REGEX = r"([a-zA-Z0-9_\-]+)@([a-zA-Z0-9_\-]+).dfs.core.windows.net/([a-zA-Z0-9_/\-]+)"
 
 PACKAGE_NAME = "{}/{}".format("featurestore", VERSION)
 
 # metrics
-NUMBER_OF_MATERIALIZED_ROWS = "numberOfMaterializedRows"
+NUMBER_OF_OFFLINE_MATERIALIZED_ROWS = "numberOfOfflineMaterializedRows"
+NUMBER_OF_ONLINE_MATERIALIZED_ROWS = "numberOfOnlineMaterializedRows"
 NUMBER_OF_SOURCE_ROWS = "numberOfSourceRows"
 
 # offline query
 QUERY_MODE_KEY = "query_mode"
 QUERY_MODE_DEFAULT = "point_at_time"
 QUERY_MODE_FEAST = "point_at_time_feast"
 
 QUERY_APPLY_SOURCE_DELAY_KEY = "apply_source_delay"
 QUERY_APPLY_TEMPORAL_JOIN_LOOKBACK_KEY = "apply_temporal_join_lookback"
 
 COL_OBSERVATION_FEATURE_SET_UNIQUE_ROW_ID = "{fstore_guid}_{fset_name}_{fset_version}_entity_row_unique_id"
 
 COL_OBSERVATION_ENTITY_TIMESTAMP = "entity_timestamp"
+
+# online materialization
+TIME_TO_LIVE = "time_to_live"
```

## azureml/featurestore/_utils/materialize.py

```diff
@@ -4,15 +4,16 @@
 
 import datetime
 from typing import Dict
 
 from azureml.featurestore._feature_set import FeatureSet
 from azureml.featurestore._utils._constants import (
     CREATE_TIMESTAMP_COLUMN,
-    NUMBER_OF_MATERIALIZED_ROWS,
+    NUMBER_OF_OFFLINE_MATERIALIZED_ROWS,
+    NUMBER_OF_ONLINE_MATERIALIZED_ROWS,
     NUMBER_OF_SOURCE_ROWS,
     PACKAGE_NAME,
     SYS_CREATE_TIMESTAMP_COLUMN,
     SYS_UPDATE_TIMESTAMP_COLUMN,
 )
 from azureml.featurestore._utils.spark_utils import _deduplicate_dataframe
 from azureml.featurestore._utils.utils import _build_logger
@@ -39,18 +40,20 @@
     feature_window_start_time: datetime = None,
     feature_window_end_time: datetime = None,
     upsert: bool = True,
     **kwargs,
 ) -> Dict:
     """Materialize a feature set into offline store (if applicable)
 
-    :param featureWindowStartDateTime: feature window start time
-    :type featureWindowStartDateTime: datetime, optional
-    :param featureWindowEndDateTime: feature window end time
-    :type featureWindowEndDateTime: datetime, optional
+    :param feature_set: the feature set to materialize
+    :type feature_set: FeatureSet
+    :param feature_window_start_time: feature window start time
+    :type feature_window_start_time: datetime, optional
+    :param feature_window_end_time: feature window end time
+    :type feature_window_end_time: datetime, optional
     :param upsert: true: perform insert and update, false: perform append, default: true
     :type upsert: bool, optional
 
     Returns:
         Dict: metrics
     """
 
@@ -72,15 +75,15 @@
             dedup=False,
         )
 
         # empty data sanity check
         number_of_source_rows = source_df.count()
         if number_of_source_rows == 0:
             print("[Materialization Job] Input data is empty, please check input data")
-            return number_of_source_rows, 0
+            return {NUMBER_OF_SOURCE_ROWS: 0}
 
         join_keys = [index_col.name for e in feature_set.entities for index_col in e.index_columns]
 
         # deduplicate data
         df, has_dup = _deduplicate_dataframe(
             df=source_df, join_keys=join_keys, timestamp_column=feature_set.source.timestamp_column.name
         )
@@ -93,32 +96,41 @@
         if feature_set.feature_transformation_code or CREATE_TIMESTAMP_COLUMN not in df.columns:
             df = df.withColumn(CREATE_TIMESTAMP_COLUMN, cur_time)
 
         df = df.withColumn(SYS_CREATE_TIMESTAMP_COLUMN, cur_time)
         df = df.withColumn(SYS_UPDATE_TIMESTAMP_COLUMN, cur_time)
 
         # materialize to online store
+        number_of_online_materialized_rows = 0
         if feature_set.materialization_settings.online_enabled and feature_set.online_store:
             print(
                 "[Materialization Job] Materializing feature set: {}, version: {} into online store..".format(
                     feature_set.name, feature_set.version
                 )
             )
-            materialize_online(feature_set=feature_set, dataframe_to_store=df, upsert=upsert)
+            number_of_online_materialized_rows = materialize_online(
+                feature_set=feature_set, dataframe_to_store=df, upsert=upsert
+            )
 
         # materialize to offline store
+        number_of_offline_materialized_rows = 0
         if feature_set.materialization_settings.offline_enabled and feature_set.offline_store:
             print(
                 "[Materialization Job] Materializing feature set: {}, version: {} into offline store..".format(
                     feature_set.name, feature_set.version
                 )
             )
             feature_set.offline_store.write_data(feature_set=feature_set, df=df, upsert=upsert)
+            number_of_offline_materialized_rows = number_of_materialized_rows
 
-        return {NUMBER_OF_SOURCE_ROWS: number_of_source_rows, NUMBER_OF_MATERIALIZED_ROWS: number_of_materialized_rows}
+        return {
+            NUMBER_OF_SOURCE_ROWS: number_of_source_rows,
+            NUMBER_OF_OFFLINE_MATERIALIZED_ROWS: number_of_offline_materialized_rows,
+            NUMBER_OF_ONLINE_MATERIALIZED_ROWS: number_of_online_materialized_rows,
+        }
     except Exception as ex:
         if isinstance(ex, MlException):
             _get_logger().error(f"{PACKAGE_NAME}->Materialize, {type(ex).__name__}: {ex.no_personal_data_message}")
         else:
             _get_logger().error(f"{PACKAGE_NAME}->Materialize, {type(ex).__name__}: {ex}")
 
         log_and_raise_error(error=ex, debug=True)
```

## azureml/featurestore/_utils/spark_utils.py

```diff
@@ -9,14 +9,19 @@
 
 from azureml.featurestore._utils._constants import (
     CREATE_TIMESTAMP_COLUMN,
     PARTITION_COLUMN,
     SYS_CREATE_TIMESTAMP_COLUMN,
     SYS_UPDATE_TIMESTAMP_COLUMN,
 )
+from azureml.featurestore._utils.error_constants import (
+    SCHEMA_ERROR_NO_INDEX_COLUMN,
+    SCHEMA_ERROR_NO_TIMESTAMP_COLUMN,
+    SCHEMA_ERROR_WRONG_DATA_TYPE,
+)
 from azureml.featurestore._utils.type_map import TypeMap
 from azureml.featurestore.contracts.datetimeoffset import DateTimeOffset
 from azureml.featurestore.contracts.feature import Feature
 from azureml.featurestore.contracts.feature_source import SourceType
 from azureml.featurestore.contracts.partition import Partition
 from pyspark.sql import DataFrame, SparkSession
 from pyspark.sql.functions import col, lit, rand, row_number, to_timestamp
@@ -95,15 +100,15 @@
     df: DataFrame,
     featureWindowStartDateTime: datetime,
     featureWindowEndDateTime: datetime,
     index_columns: List[str],
     timestamp_column: str,
     features: List[str],
 ) -> DataFrame:
-    # filter
+    # filter the dataframe to the given feature window and remove intermediate rows from source lookback (if any)
     if featureWindowStartDateTime:
         df = df.filter(col(timestamp_column) >= to_timestamp(lit(featureWindowStartDateTime)))
 
     if featureWindowEndDateTime:
         df = df.filter(col(timestamp_column) < to_timestamp(lit(featureWindowEndDateTime)))
 
     columns = index_columns
@@ -122,34 +127,30 @@
     for feature in features:
         if feature.name not in df.columns:
             raise Exception("Schema check errors, no feature column: {} in output dataframe".format(feature.name))
         data_type = TypeMap.spark_to_column_type(df.schema[feature.name].dataType.typeName())
         expected_data_type = feature.type
         if data_type != expected_data_type:
             raise ValidationException(
-                message="Schema check errors, feature column: {} has data type: {}, expected: {}".format(
-                    feature.name, data_type, expected_data_type
-                ),
-                no_personal_data_message="Schema check errors, no feature column with expected data type",
+                message=SCHEMA_ERROR_WRONG_DATA_TYPE.format(feature.name, data_type, expected_data_type),
+                no_personal_data_message=SCHEMA_ERROR_WRONG_DATA_TYPE,
                 error_type=ValidationErrorType.MISSING_FIELD,
                 error_category=ErrorCategory.USER_ERROR,
                 target=ErrorTarget.GENERAL,
             )
 
     for index_column in index_columns:
         if index_column.name not in df.columns:
-            raise Exception("Schema check errors, no index column: {} in output dataframe".format(index_column.name))
+            raise Exception(SCHEMA_ERROR_NO_INDEX_COLUMN.format(index_column.name))
         data_type = TypeMap.spark_to_column_type(df.schema[index_column.name].dataType.typeName())
         expected_data_type = index_column.type
         if data_type != expected_data_type:
             raise ValidationException(
-                message="Schema check errors, index column: {} has data type: {}, expected: {}".format(
-                    index_column.name, data_type, expected_data_type
-                ),
-                no_personal_data_message="Schema check errors, no index column with expected data type",
+                message=SCHEMA_ERROR_WRONG_DATA_TYPE.format(index_column.name, data_type, expected_data_type),
+                no_personal_data_message=SCHEMA_ERROR_WRONG_DATA_TYPE,
                 error_type=ValidationErrorType.MISSING_FIELD,
                 error_category=ErrorCategory.USER_ERROR,
                 target=ErrorTarget.GENERAL,
             )
 
     return df, list(map(lambda i: i.name, index_columns))
 
@@ -166,16 +167,16 @@
         if df.schema[timestamp_column].dataType != TimestampType():
             if timestamp_column_format:
                 df = df.withColumn(timestamp_column, to_timestamp(timestamp_column, timestamp_column_format))
             else:
                 df = df.withColumn(timestamp_column, to_timestamp(timestamp_column))
     else:
         raise ValidationException(
-            message="Schema check errors, timestamp column: {} is not in output dataframe".format(timestamp_column),
-            no_personal_data_message="Schema check errors, timestamp column specified is not in output dataframe",
+            message=SCHEMA_ERROR_NO_TIMESTAMP_COLUMN.format(timestamp_column),
+            no_personal_data_message=SCHEMA_ERROR_NO_TIMESTAMP_COLUMN,
             error_type=ValidationErrorType.MISSING_FIELD,
             error_category=ErrorCategory.USER_ERROR,
             target=ErrorTarget.GENERAL,
         )
 
     return df, timestamp_column
```

## azureml/featurestore/_utils/utils.py

```diff
@@ -9,14 +9,21 @@
 import shutil
 import uuid
 from collections import Counter, defaultdict
 from dataclasses import asdict, dataclass
 from typing import TYPE_CHECKING, List, Optional
 
 from azureml.featurestore._utils._constants import AZUREML_URI_PATTERNS, CLOUD_URI_PATTERNS, STORAGE_URI_PATTERNS
+from azureml.featurestore._utils.error_constants import (
+    EMPTY_FEATURE_MESSAGE,
+    FEATURE_NAME_COLLISION_MESSAGE,
+    INVALID_FEATURE_URI_MESSAGE,
+    SCHEMA_ERROR_MISSING_COLUMNS,
+    UNSUPORTED_STORAGE_TYPE_MESSAGE,
+)
 
 from azure.ai.ml._telemetry.logging_handler import get_appinsights_log_handler
 from azure.ai.ml._user_agent import USER_AGENT
 from azure.ai.ml._utils._logger_utils import OpsLogger
 from azure.ai.ml.entities._assets._artifacts.artifact import ArtifactStorageInfo
 from azure.ai.ml.exceptions import ErrorCategory, ErrorTarget, ValidationErrorType, ValidationException
 from azure.ai.ml.operations import DatastoreOperations
@@ -29,27 +36,25 @@
 class PathType:
     azureml = 1
     cloud = 2
     local = 3
     storage = 4
 
 
-def _process_path(path: str, is_folder: bool = False, datastore_operations: DatastoreOperations = None):
+def _process_path(path: str, datastore_operations: DatastoreOperations = None):
     path_type, base_path = _parse_path_format(path)
     if path_type == PathType.local:
         base_path = os.path.normpath(base_path)
         local_path = base_path
         if not os.path.isabs(base_path):
             local_path = os.path.join(os.getcwd(), base_path)
         if not os.path.exists(local_path):
             raise ValueError("File '%s' does not exist." % local_path)
     else:
-        local_path = _download_file(
-            path=path, path_type=path_type, is_folder=is_folder, datastore_operations=datastore_operations
-        )
+        local_path = _download_file(path=path, path_type=path_type, datastore_operations=datastore_operations)
 
     return local_path
 
 
 def _strip_local_path(path: str):
     path = path.rstrip("/\\")
     if path.startswith("../") or path.startswith("..\\") or path == ".":
@@ -57,15 +62,14 @@
     else:
         return path.lstrip("./\\")
 
 
 def _download_file(
     path: str,
     path_type: PathType,
-    is_folder: bool = False,
     target_path: str = None,
     datastore_operations: DatastoreOperations = None,
 ):
     from tempfile import mkdtemp
 
     # normalize path to yaml config path
     normalized_path = path.rstrip("/")
@@ -85,14 +89,16 @@
                 raise ValueError(f"{e.args[0]}, uri: {normalized_path}")
             elif "StreamError(NotFound)" in e.args[0]:
                 raise ValueError(f"{e.args[0]}; Not able to find path: {normalized_path}")
             elif "PermissionDenied" in e.args[0]:
                 raise PermissionError(f"{e.args[0]}; No permission to access path: {normalized_path}")
             else:
                 raise SystemError(f"{e.args[0]}, uri: {normalized_path}")
+
+        local_path = os.path.join(local_path, re.split(normalized_path, r"/\\")[-1])
     else:
         from azure.ai.ml._artifacts._artifact_utilities import (
             download_artifact_from_aml_uri,
             download_artifact_from_storage_url,
         )
 
         if path_type == PathType.cloud:
@@ -107,20 +113,14 @@
                 uri=normalized_path, destination=local_path, datastore_operation=datastore_operations
             )
         elif path_type == PathType.local:
             return local_path
         else:
             raise Exception(f"Can't download from path: {normalized_path}")
 
-    if not is_folder:
-        file_list = os.listdir(local_path)
-        if len(file_list) > 1:
-            raise Exception("Found multiple items from uri")
-        local_path = os.path.join(local_path, file_list[0])
-
     return local_path
 
 
 def _list_files(parent_uri: str, file: str):
     from azureml.dataprep.api._rslex_executor import ensure_rslex_environment
     from azureml.dataprep.rslex import Copier, PyIfDestinationExists, PyLocationInfo
 
@@ -186,15 +186,15 @@
     return shutil.make_archive(zip_path, "zip", root_dir=zip_path)
 
 
 def resolve_features(features: List[Feature]):
     from azureml.featurestore._feature_set import FeatureSet
 
     if not features:
-        msg = "Must provide features."
+        msg = EMPTY_FEATURE_MESSAGE
         raise ValidationException(
             message=msg,
             target=ErrorTarget.GENERAL,
             no_personal_data_message=msg,
             error_category=ErrorCategory.USER_ERROR,
             error_type=ValidationErrorType.MISSING_FIELD,
         )
@@ -213,34 +213,32 @@
 
 
 def validate_features(features: List[Feature]):
     feature_names = [f.name for f in features]
     collided_names = [f for f, occurrences in Counter(feature_names).items() if occurrences > 1]
 
     if len(collided_names) > 0:
-        msg = "There are feature name collisions. Duplicate features: {}"
         raise ValidationException(
-            message=msg.format(collided_names),
+            message=FEATURE_NAME_COLLISION_MESSAGE.format(collided_names),
             target=ErrorTarget.GENERAL,
-            no_personal_data_message=msg,
+            no_personal_data_message=FEATURE_NAME_COLLISION_MESSAGE,
             error_category=ErrorCategory.USER_ERROR,
         )
 
 
 def feature_uri_parser_with_rename(uri):
     URI_REGEX_PATTERN = "^([^/]+):([^/]+):([^/]+)"
 
     match = re.match(URI_REGEX_PATTERN, uri)
     if match:
         return match.group(1), match.group(2), match.group(3)
     else:
-        msg = 'Invalid feature reference {}. Feature reference must be in the form "<feature_set>:<version>:<feature_name>"'
         raise ValidationException(
-            message=msg.format(uri),
-            no_personal_data_message=msg.format("[uri]"),
+            message=INVALID_FEATURE_URI_MESSAGE.format(uri),
+            no_personal_data_message=INVALID_FEATURE_URI_MESSAGE.format("[uri]"),
             error_type=ValidationErrorType.INVALID_VALUE,
             error_category=ErrorCategory.USER_ERROR,
             target=ErrorTarget.ARM_RESOURCE,
         )
 
 
 def _resolve_hdfs_path_from_storage_info(asset_artifact: ArtifactStorageInfo) -> str:
@@ -262,18 +260,17 @@
         return path
     if blob_match:
         path = BLOB_HDFS_FORMAT.format(
             asset_artifact.container_name, blob_match.group(1), blob_match.group(2), asset_artifact.relative_path
         )
         return path
 
-    msg = "Unsupported Storage account type. Storage url: {}"
     raise ValidationException(
-        message=msg.format(asset_artifact.storage_account_url),
-        no_personal_data_message=msg,
+        message=UNSUPORTED_STORAGE_TYPE_MESSAGE.format(asset_artifact.storage_account_url),
+        no_personal_data_message=UNSUPORTED_STORAGE_TYPE_MESSAGE,
         error_type=ValidationErrorType.INVALID_VALUE,
         error_category=ErrorCategory.USER_ERROR,
         target=ErrorTarget.ARM_RESOURCE,
     )
 
 
 def infer_event_timestamp_range(observation_data, timestamp_column):
@@ -318,18 +315,17 @@
             join_keys.add(column.name)
 
     entity_columns = set(entity_schema.keys())
     expected_columns = join_keys | {timestamp_column}
     missing_columns = expected_columns - entity_columns
 
     if len(missing_columns) > 0:
-        msg = "Expected entity and timestamp columns not found. Expected: {}. Missing: {}"
         raise ValidationException(
-            message=msg.format(expected_columns, missing_columns),
-            no_personal_data_message=msg,
+            message=SCHEMA_ERROR_MISSING_COLUMNS.format(expected_columns, missing_columns),
+            no_personal_data_message=SCHEMA_ERROR_MISSING_COLUMNS,
             error_type=ValidationErrorType.INVALID_VALUE,
             error_category=ErrorCategory.USER_ERROR,
             target=ErrorTarget.ARM_RESOURCE,
         )
 
 
 @dataclass(frozen=True)
```

## azureml/featurestore/contracts/feature_retrieval_spec.py

```diff
@@ -77,15 +77,15 @@
     @monitor_with_activity(_get_logger(), f"{PACKAGE_NAME}->FeatureRetrievalSpec.FromConfig", ActivityType.PUBLICAPI)
     def from_config(spec_path: Union[str, PathLike]):
         """Load a feature retrieval spec from yaml config. Spec path must be a folder path, the spec file name is assumed as feature_retrieval_spec.yaml
         :param spec_path: The path to fetch this spec.
         :type spec_path: Union[str, PathLike]
         """
         try:
-            local_spec_path = _process_path(path=spec_path, is_folder=False)
+            local_spec_path = _process_path(path=spec_path)
 
             if not os.path.isdir(local_spec_path):
                 msg = "Spec path {} must be an existing folder path"
                 raise ValidationException(
                     message=msg.format(spec_path),
                     target=ErrorTarget.FEATURE_SET,
                     no_personal_data_message="Spec path must be an existing folder path",
```

## azureml/featurestore/contracts/transformation_code.py

```diff
@@ -54,14 +54,12 @@
 
             code_path = os.path.join(spec_folder_path, code_path)
         else:
             code_path = self.path
 
         path_type, code_path = _parse_path_format(code_path)
         if path_type != PathType.local:
-            code_path = _download_file(
-                path=code_path, path_type=path_type, is_folder=True, datastore_operations=datastore_operations
-            )
+            code_path = _download_file(path=code_path, path_type=path_type, datastore_operations=datastore_operations)
 
         # Put code in a uuid() folder and zip so that sc can import script without collision
         code_zip = copy_rename_and_zip(code_path)
         return code_zip
```

## azureml/featurestore/online/_online_feature_materialization.py

```diff
@@ -1,15 +1,16 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 
 from azureml.featurestore._feature_set import FeatureSet
+from azureml.featurestore._utils._constants import TIME_TO_LIVE
 from azureml.featurestore.contracts.store_connection import OnlineStoreType
-
-from ._utils import _get_lookup_key
+from azureml.featurestore.online._redis_client_pool import _get_redis_connection_string
+from ._utils import _get_lookup_key, _get_lookup_key_udf, _get_lookup_key_pattern
 
 
 def materialize_online(feature_set, dataframe_to_store, upsert):
     # Do the 3 steps
     from pyspark.sql import SparkSession
 
     spark = SparkSession.builder.getOrCreate()
@@ -18,77 +19,95 @@
     key_index_column = feature_set.get_index_columns()
     # Extract only the name from the column object
     key_index_column = [x.name for x in key_index_column]
     # Rest of the header will be feature values to store
     value_columns = [x for x in dataframe_to_store.schema.names if x not in key_index_column]
 
     from pyspark.sql import Window
-    from pyspark.sql.functions import col, desc, rank
+    from pyspark.sql.functions import col, current_timestamp, desc, rank, unix_timestamp
 
     # This process is needed because it is possible that we have rows with duplicate key.
     # We will only keep the key with the latest timestamp
     # Rank each row by the key columns ordered by timestmap. Take the largest timestamp
     time_stamp, _ = feature_set.get_timestamp_column()
     win_spec = Window.partitionBy([col(x) for x in key_index_column]).orderBy(desc(time_stamp))
     dataframe_to_store = (
         dataframe_to_store.withColumn("rank", rank().over(win_spec)).select("*").where("rank = 1").drop("rank")
     )
+    if feature_set.temporal_join_lookback:
+        cur_time_unix = unix_timestamp(current_timestamp())
+        temporal_lookback = feature_set.temporal_join_lookback.to_timedelta()
+        temporal_lookback_seconds = temporal_lookback.total_seconds()
+        df = dataframe_to_store.withColumn(
+            TIME_TO_LIVE, temporal_lookback_seconds - (cur_time_unix - unix_timestamp(dataframe_to_store[time_stamp]))
+        )
+        dataframe_to_store = df.filter(df[TIME_TO_LIVE] > 0)
+
+    number_of_materialized_rows = dataframe_to_store.count()
+
+    feature_set_time_stamp, _ = feature_set.get_timestamp_column()
+    prefix, suffix_columns = _get_lookup_key_pattern(feature_set)
 
     def saveIntoRedisRecurringJob(rdd, redis_connection_string, value_columns):
         # Save every record into redis without caring about timestamp.
         # Replacement operation
         from redis import Redis
 
         redis_client = Redis.from_url(redis_connection_string)
         pipe = redis_client.pipeline()
         for record in rdd:
-            primaryKey = _get_lookup_key(feature_set, record)
+            primaryKey = _get_lookup_key_udf(prefix, suffix_columns, record)
             # Set is upsert. Replace if exist
             for value_column in value_columns:
                 pipe.hset(primaryKey, value_column, str(record[value_column]))
+            if TIME_TO_LIVE in record:
+                pipe.expire(primaryKey, int(record[TIME_TO_LIVE]))
         pipe.execute()
 
     def saveIntoRedisBackfillJob(rdd, redis_connection_string, value_columns):
         # Fetch each record from redis and compare timestamp before saving
         from dateutil.parser import parse
         from redis import Redis
 
         redis_client = Redis.from_url(redis_connection_string)
         pipe = redis_client.pipeline()
-        time_stamp, _ = feature_set.get_timestamp_column()
+        time_stamp = feature_set_time_stamp
         for record in rdd:
-            primaryKey = _get_lookup_key(feature_set, record)
+            primaryKey = _get_lookup_key_udf(prefix, suffix_columns, record)
             # TODO change timestamp to the correct timestamp column value
             redis_record_timestamp = redis_client.hget(primaryKey, time_stamp)
             if redis_record_timestamp is not None:
                 redis_record_timestamp = parse(redis_record_timestamp)
             current_record_timestamp = parse(str(record[time_stamp]))
             if redis_record_timestamp is None or current_record_timestamp >= redis_record_timestamp:
                 for value_column in value_columns:
                     pipe.hset(primaryKey, value_column, str(record[value_column]))
+            if TIME_TO_LIVE in record:
+                pipe.expire(primaryKey, int(record[TIME_TO_LIVE]))
         pipe.execute()
 
     if not feature_set.online_store.target:
         raise Exception(
             f'Featureset "{feature_set.arm_id}" belongs to a featurestore that does not specify an online store connection.'
         )
 
     if feature_set.online_store.type != OnlineStoreType.REDIS:
         raise Exception(
             f'Featureset "{feature_set.arm_id}" specifies an online store connection of type "{feature_set.online_store.type}". Only "redis" online stores are currently supported.'
         )
 
-    from azureml.featurestore._identity import AzureMLHoboSparkOnBehalfOfCredential
-    from azureml.featurestore.online._redis_client_pool import _get_redis_connection_string
+    from azure.ai.ml.identity import AzureMLOnBehalfOfCredential
 
     # Get the redis connection string
-    credential = AzureMLHoboSparkOnBehalfOfCredential()
+    credential = AzureMLOnBehalfOfCredential()
     redis_connection_string = _get_redis_connection_string(feature_set.online_store.target, credential)
 
     if upsert:
         dataframe_to_store.rdd.foreachPartition(
             lambda x: saveIntoRedisBackfillJob(x, redis_connection_string, value_columns)
         )
     else:
         dataframe_to_store.rdd.foreachPartition(
             lambda x: saveIntoRedisRecurringJob(x, redis_connection_string, value_columns)
         )
+
+    return number_of_materialized_rows
```

## azureml/featurestore/online/_utils.py

```diff
@@ -1,16 +1,31 @@
 # ---------------------------------------------------------
 # Copyright (c) Microsoft Corporation. All rights reserved.
 # ---------------------------------------------------------
 
 
 def _get_lookup_key(featureset, row):
+    prefix, suffix_column_name = _get_lookup_key_pattern(featureset)
+
+    return _get_lookup_key_udf(prefix, suffix_column_name, row)
+
+
+def _get_lookup_key_udf(prefix, suffix_column_names, row):
     suffix_parts = []
 
-    for entity in featureset.entities:
-        for index_column in entity.index_columns:
-            suffix_parts.append(index_column.name)
-            suffix_parts.append(row[index_column.name])
+    for index_column in suffix_column_names:
+        suffix_parts.append(index_column)
+        suffix_parts.append(row[index_column])
 
-    prefix = f"featurestore:{featureset.feature_store_guid}:featureset:{featureset.name}:version:{featureset.version}"
     suffix = ":".join(suffix_parts)
     return f"{prefix}:{suffix}"
+
+def _get_lookup_key_pattern(featureset):
+    prefix = f"featurestore:{featureset.feature_store_guid}:featureset:{featureset.name}:version:{featureset.version}"
+
+    suffix_column_names = []
+
+    for entity in featureset.entities:
+        for index_column in entity.index_columns:
+            suffix_column_names.append(index_column.name)
+    
+    return prefix, suffix_column_names
```

## Comparing `azureml_featurestore-0.1.0b1.dist-info/DESCRIPTION.rst` & `azureml_featurestore-0.1.0b2.dist-info/DESCRIPTION.rst`

 * *Files 20% similar despite different names*

```diff
@@ -14,12 +14,20 @@
 You can install the package via ` pip install azureml-featurestore `
 
 To learn more about Azure ML managed feature store visit https://aka.ms/featurestore-get-started
 
 
 # Change Log
 
+## 0.1.0b2 (2023.06.13)
+
+**New Features:**
+
+- [Private preview] Added online store support. Online store supports materialization and online feature values retrieval from Redis cache for batch scoring.
+
+- Various bug fixes
+
 ## 0.1.0b1 (2023.05.15)
 
 **New Features:**
 
 Initial release.
```

## Comparing `azureml_featurestore-0.1.0b1.dist-info/metadata.json` & `azureml_featurestore-0.1.0b2.dist-info/metadata.json`

 * *Files 8% similar despite different names*

### Pretty-printed

 * *Similarity: 0.9567307692307693%*

 * *Differences: {"'run_requires'": "{0: {'requires': {insert: [(0, 'azure-ai-ml (~=1.8.0)')], delete: [0]}}}",*

 * * "'version'": "'0.1.0b2'"}*

```diff
@@ -37,15 +37,15 @@
     "license": "MIT License",
     "metadata_version": "2.0",
     "name": "azureml-featurestore",
     "requires_python": "<4.0,>=3.8",
     "run_requires": [
         {
             "requires": [
-                "azure-ai-ml (~=1.7.0)",
+                "azure-ai-ml (~=1.8.0)",
                 "azure-identity",
                 "mltable (~=1.1.0)"
             ]
         },
         {
             "extra": "online",
             "requires": [
@@ -53,9 +53,9 @@
                 "pandas (>=1.5.3)",
                 "pyarrow (>=9.0.0)",
                 "redis (>=4.5.1)"
             ]
         }
     ],
     "summary": "Azure Machine Learning Feature Store SDK",
-    "version": "0.1.0b1"
+    "version": "0.1.0b2"
 }
```

## Comparing `azureml_featurestore-0.1.0b1.dist-info/METADATA` & `azureml_featurestore-0.1.0b2.dist-info/METADATA`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.0
 Name: azureml-featurestore
-Version: 0.1.0b1
+Version: 0.1.0b2
 Summary: Azure Machine Learning Feature Store SDK
 Home-page: https://aka.ms/featurestore-get-started
 Author: Microsoft Corporation
 License: MIT License
 Keywords: AzureMachineLearning
 Classifier: Development Status :: 4 - Beta
 Classifier: Intended Audience :: Developers
@@ -14,15 +14,15 @@
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: License :: OSI Approved :: MIT License
 Requires-Python: <4.0,>=3.8
 Description-Content-Type: text/markdown
 Provides-Extra: online
-Requires-Dist: azure-ai-ml (~=1.7.0)
+Requires-Dist: azure-ai-ml (~=1.8.0)
 Requires-Dist: azure-identity
 Requires-Dist: mltable (~=1.1.0)
 Provides-Extra: online
 Requires-Dist: azure-mgmt-redis (==14.1.0); extra == 'online'
 Requires-Dist: pandas (>=1.5.3); extra == 'online'
 Requires-Dist: pyarrow (>=9.0.0); extra == 'online'
 Requires-Dist: redis (>=4.5.1); extra == 'online'
@@ -43,12 +43,20 @@
 You can install the package via ` pip install azureml-featurestore `
 
 To learn more about Azure ML managed feature store visit https://aka.ms/featurestore-get-started
 
 
 # Change Log
 
+## 0.1.0b2 (2023.06.13)
+
+**New Features:**
+
+- [Private preview] Added online store support. Online store supports materialization and online feature values retrieval from Redis cache for batch scoring.
+
+- Various bug fixes
+
 ## 0.1.0b1 (2023.05.15)
 
 **New Features:**
 
 Initial release.
```

## Comparing `azureml_featurestore-0.1.0b1.dist-info/RECORD` & `azureml_featurestore-0.1.0b2.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,55 +1,56 @@
 azureml/__init__.py,sha256=d5b08hPILQxNDQ9p22-0d5Fl-90qmLEP7axZw6c0Ryw,267
-azureml/featurestore/__init__.py,sha256=9zKXTqTH9aH2SelKYf9sqDd90M-erWntAbuyaQxIvCU,565
-azureml/featurestore/_feature_set.py,sha256=GpiDTEUop_1vzQQQmVgbh1QDjhimn8GdC_JVfO5N2d0,21083
-azureml/featurestore/_version.py,sha256=Sp93v603w-9MX3yAroUxdy5mZxkR-HVFir24Sl38pqw,21
+azureml/featurestore/__init__.py,sha256=ylJkgOMGGuhcMDPqQTq_L046QWfueUh2OsSgGc0POvA,917
+azureml/featurestore/_feature_set.py,sha256=FGzW1WyCyN75lYZjTgElMk_oBTK2ozgduOZp9tNteFg,21202
+azureml/featurestore/_version.py,sha256=FfhzaecOVVhP5q8jtt3aeOI4NfHslas-hIspKiWrpuA,21
 azureml/featurestore/abstract_feature_store.py,sha256=HyH-wGhFvD0312T691b4H2YSyXdqZzQQgoq5GAp6qvU,1108
-azureml/featurestore/feature_set_spec.py,sha256=uT6S8GzggFPfFsvm_AiStowv_iBHpth6fS2M0Hnu8AU,25597
-azureml/featurestore/feature_store_client.py,sha256=EXykizSr_WF7aElfFFR4cD4_d1X0xL02Bu-U4xs0HbE,27132
+azureml/featurestore/feature_set_spec.py,sha256=EzkmBl2pZeXVgJBX09Iod8uKdlu2f235O9DUh3tiP38,25045
+azureml/featurestore/feature_store_client.py,sha256=JWnEZiTDtM5x2YgwUVPHbm-tQHhF0tZ_SlLGDqDWBIw,26844
 azureml/featurestore/_identity/__init__.py,sha256=ySOoGimIHBlawW86oB_PtMBD7KZf9z4eVwKBRR1TNIY,410
-azureml/featurestore/_identity/aml_hobospark_on_behalf_of.py,sha256=8kC5huO_JBcWqzVZc5171QWgkpjYYEnk-50QJJEuRGE,6332
+azureml/featurestore/_identity/aml_hobospark_on_behalf_of.py,sha256=jD_CsUBz2rIxS8_ld11eeu3bE9zzg2HZQ8F9bm6K1w0,6334
 azureml/featurestore/_offline_query/__init__.py,sha256=Q5ZiXWuLnEFEcp0JcWjUfF5i5qj5SSFP_S1bE6Dmqvk,455
 azureml/featurestore/_offline_query/offline_retrieval_job.py,sha256=yBxyvbn__cFZGZS7cZmwbRDQk9snKWjvpMMHWCcHjMo,401
-azureml/featurestore/_offline_query/point_at_time.py,sha256=X9fRoZp4t4_b9Mrgtvke9AkUBc-THpu7uUI5zsHp_sU,10924
+azureml/featurestore/_offline_query/point_at_time.py,sha256=bHRVx5Ug3TQiFceW2_MRRNoex80iyWO7YhceKruuQcU,10931
 azureml/featurestore/_utils/__init__.py,sha256=d5b08hPILQxNDQ9p22-0d5Fl-90qmLEP7axZw6c0Ryw,267
-azureml/featurestore/_utils/_constants.py,sha256=ubHabZKacvk_n4aUFK60dhga0T9W36_u0OySbBcOrYo,1549
+azureml/featurestore/_utils/_constants.py,sha256=QkBJDIzHHi8sBQkHY0NSyubkwoFmpdH71EcObgLNXMY,1694
 azureml/featurestore/_utils/_preview_method.py,sha256=Tzop4bo0FIMJ4MrOoOpqOqDaznKaK5XamRfnDD2cO5M,398
 azureml/featurestore/_utils/arm_id_utils.py,sha256=vgC0CkCMxz8aeVhZABPIO6lm9n6rsXkoX02HYMl71uk,7359
-azureml/featurestore/_utils/materialize.py,sha256=oJkPGlBbLmctvD1czfGs9VfHvfKC4CSLs6JuCbJ0sLQ,5210
-azureml/featurestore/_utils/spark_utils.py,sha256=x2lyrwsLxrBpgDso66HtLVNN4Pvo6hDd9QB0EZnkeJY,9810
+azureml/featurestore/_utils/error_constants.py,sha256=3dZzSmutEGFo3FmGsyi5qhMT4nU1ps5Qk3pbz_5_k_o,2564
+azureml/featurestore/_utils/materialize.py,sha256=HH-mq_suK-Pq4fxys6MhdX7NiWgDuhAX698IcINXXzU,5732
+azureml/featurestore/_utils/spark_utils.py,sha256=DIJZvOsIxnPBmaxvr4jGofmi_wBWIAUxSuxH8esaBoU,9728
 azureml/featurestore/_utils/type_map.py,sha256=-8261UWjeCmYvIvyfT_b361D6uq70bo_TKBTsq-yFik,2370
-azureml/featurestore/_utils/utils.py,sha256=cfKPBCKHMsKapXMk7tZK9JI0g_DzOckyFXRjpd-70QQ,15837
+azureml/featurestore/_utils/utils.py,sha256=Ve4EBnL-7A7pPmSNDNsbpfDfSIysy_HJS7dtjvpI0Bc,15684
 azureml/featurestore/contracts/__init__.py,sha256=XQSdeiXkhQz-pnsGpzjlHyEYl68HcS3hC86_0yrLTdk,681
 azureml/featurestore/contracts/column.py,sha256=tpF4lio6z-8y50Ynh_km_qUv7UHDBl3RnyHjlRQ1C-M,1243
 azureml/featurestore/contracts/datetimeoffset.py,sha256=zTYYHbuPLAAK1YuLXQMWDlhWnIYpjtImBfbJg4Gtao8,1058
 azureml/featurestore/contracts/feature.py,sha256=BAKqtAQJLowuuXEszG_dsF5xRSDEb7NpDHII1ixWK34,3593
-azureml/featurestore/contracts/feature_retrieval_spec.py,sha256=HFasOUwpzINXq1kgcik-BJxpBgpqZnj0VRioagJwX1s,9275
+azureml/featurestore/contracts/feature_retrieval_spec.py,sha256=5VzYQxU_NKv3roARIghn_7k58EPgEsDBcmOnaWIhHrk,9258
 azureml/featurestore/contracts/feature_source.py,sha256=HOF3LfzTmZOea9Liz6hCzyRR3SzrdyFoimUE-U2lxCA,2846
 azureml/featurestore/contracts/offline_store.py,sha256=QSVo_s_WV6Ajc-ba6CRuFs8okE0_gw7R0fyajpTsDF8,2019
 azureml/featurestore/contracts/online_store.py,sha256=ZY5QlYQQT2WcVc6fwc3RYHzHESEH_mcwlYybUlFtUnU,646
 azureml/featurestore/contracts/partition.py,sha256=99PGUsY7a0gY8HYazLR-qjb68RNNDNb1Wbc8_SM-15E,694
 azureml/featurestore/contracts/store_connection.py,sha256=Jtt2sCgCtz8pKh6cQT4lnaL7IQ1_tPm341Gv4fmW6Ps,1230
 azureml/featurestore/contracts/timestamp_column.py,sha256=vDoNgsMbKbd527VKmmMPRAlbNnXiSaiAa0UrLPBk_as,753
-azureml/featurestore/contracts/transformation_code.py,sha256=KVAAxajRxBTCIgIY9cQAtCfeoVqj6Il9ZbIgJjhJXOc,2578
+azureml/featurestore/contracts/transformation_code.py,sha256=C4Yablh-LDWjyB-U3qPrQbDwgFRGD07kTvWnr0VhSNo,2530
 azureml/featurestore/grpc/__init__.py,sha256=5MSlWALp5Ta6ZqnzvuEFoQTiZmIphmkNxFAThYLiBMU,294
 azureml/featurestore/grpc/_flight_feature_retrieval_client.py,sha256=8UlYrzJ8aFu8-63_cYt0UYrUjbyZdMic5Jxlqw0Oe64,1364
 azureml/featurestore/grpc/_flight_feature_retrieval_server.py,sha256=hxHPmLiIOUVE-6na0Q53bUTp7c_qOsVtCzO9MTaHKUQ,3276
 azureml/featurestore/grpc/_flight_helper.py,sha256=uhOZjxyPJXgYg86cLpCEEFwLuuf_BUeKRntA0OIAOaY,5923
 azureml/featurestore/grpc/_inline_feature_retrieval_client.py,sha256=Yx-lxrx5mW1NgBFpqstGBAqfk7dV09Ux4bCSVLZEe48,934
 azureml/featurestore/offline_store/__init__.py,sha256=ZoGnMoKGPd8cOJXODpYbg1ebH3t1wwTDJ7OBtvACdEQ,391
 azureml/featurestore/offline_store/azure_data_lake_offline_store.py,sha256=SL7SsC-onnkqyHVzR7dBTPdm11qvmeqOn3aUzCzHplc,9020
 azureml/featurestore/offline_store/partition/__init__.py,sha256=iUJezOoUcCW5AK9LI-CJBaL8sdxUVk8fAoaOqm1vqUc,367
 azureml/featurestore/offline_store/partition/timestamp_partition.py,sha256=yrDH_0Skft27Yp8noNMV97LVx178zp2tdzPA8imFb9s,2381
 azureml/featurestore/online/__init__.py,sha256=GQg_cumJmcHWx9a02jhxSJLfXfIJmlA7RjYGa_W-4yg,242
 azureml/featurestore/online/_online_feature_getter.py,sha256=iff73LgZHYJyQBWtUakrqUVLVK6M92OXxnaK99yDzAc,5515
-azureml/featurestore/online/_online_feature_materialization.py,sha256=ODmxrp2Xn0f5FfiWRA8elVILiyeb8NPeVof0p-uYY5A,4492
+azureml/featurestore/online/_online_feature_materialization.py,sha256=IFkeAysD022RxkZ-5Fx4FmvuNfZsvwXcyo8BKuAPf98,5571
 azureml/featurestore/online/_redis_client_pool.py,sha256=aBSngwCq20fQFlqiKD9VIoUSW1ulWtD7WGuYpmzLkDQ,1669
-azureml/featurestore/online/_utils.py,sha256=7EBkS5SInxr60hCKiek9yJnh2m7epq0u69_7JyOmhOg,643
+azureml/featurestore/online/_utils.py,sha256=vGo7Vur-skAY-hefHOrAbB-X5ZNgcRcHG9B7foqT8Nk,1055
 azureml/featurestore/schema/__init__.py,sha256=d5b08hPILQxNDQ9p22-0d5Fl-90qmLEP7axZw6c0Ryw,267
 azureml/featurestore/schema/feature_retrieval_spec_schema.py,sha256=I3e63kdZQUHIlfx5Y3cEkUyQqIqJwvXtSWhv21nZxgg,1655
 azureml/featurestore/schema/feature_set_schema.py,sha256=36GiLZrjZPWqejS_PA7iOUdOZT2E-HU5Ti2yWlYVj2Y,5944
-azureml_featurestore-0.1.0b1.dist-info/DESCRIPTION.rst,sha256=PkWAo850fH4MwuhY_4SXEX20inXQqGFWGZAzZ5OXzOY,823
-azureml_featurestore-0.1.0b1.dist-info/METADATA,sha256=tSdzVt6c0_707uhOghCK8Szi33D2edU4oR5S9BDwHWw,1921
-azureml_featurestore-0.1.0b1.dist-info/RECORD,,
-azureml_featurestore-0.1.0b1.dist-info/WHEEL,sha256=Vlaj2XNMTTJ893zWX-JvKeZUIs7q5E7d7Gise2Vouzc,97
-azureml_featurestore-0.1.0b1.dist-info/metadata.json,sha256=buHEOUWi1BPajl1pC_YIh2zbfvAjkM0nmImgQ-NWGRE,1142
-azureml_featurestore-0.1.0b1.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
+azureml_featurestore-0.1.0b2.dist-info/DESCRIPTION.rst,sha256=F6R6tlHXSN2NIDDf3lCz_IqwdoN5V2-j4dDtPE49V4U,1047
+azureml_featurestore-0.1.0b2.dist-info/METADATA,sha256=rXQrCejZu4Eskmagp0Aco02Fi0cN59ZL4VLiGxoFKiY,2145
+azureml_featurestore-0.1.0b2.dist-info/RECORD,,
+azureml_featurestore-0.1.0b2.dist-info/WHEEL,sha256=Vlaj2XNMTTJ893zWX-JvKeZUIs7q5E7d7Gise2Vouzc,97
+azureml_featurestore-0.1.0b2.dist-info/metadata.json,sha256=iDFDzwS1pWREm4kKWsAn7Zbm6U6X-rSJT4RU_8_MUfM,1142
+azureml_featurestore-0.1.0b2.dist-info/top_level.txt,sha256=ZOeEa0TAXo6i5wOjwBoqfIGEuxOcKuscGgNSpizqREY,8
```

