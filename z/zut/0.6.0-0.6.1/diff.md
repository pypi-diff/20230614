# Comparing `tmp/zut-0.6.0-py3-none-any.whl.zip` & `tmp/zut-0.6.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,55 +1,59 @@
-Zip file size: 61454 bytes, number of entries: 53
--rw-r--r--  2.0 unx     1879 b- defN 23-Jun-10 13:53 zut/__init__.py
--rw-r--r--  2.0 unx      160 b- defN 23-Jun-10 14:06 zut/_version.py
--rw-r--r--  2.0 unx      263 b- defN 23-Jun-10 13:46 zut/apps.py
--rw-r--r--  2.0 unx      915 b- defN 23-Jun-10 13:46 zut/colors.py
--rw-r--r--  2.0 unx     5337 b- defN 23-Jun-10 13:46 zut/commands.py
--rw-r--r--  2.0 unx     9123 b- defN 23-Jun-10 13:46 zut/credentials.py
--rw-r--r--  2.0 unx     1175 b- defN 23-Jun-10 13:53 zut/csv.py
--rw-r--r--  2.0 unx     1436 b- defN 23-Jun-10 13:46 zut/datetime.py
--rw-r--r--  2.0 unx    16858 b- defN 23-Jun-10 13:53 zut/excel.py
--rw-r--r--  2.0 unx     8086 b- defN 23-Jun-10 13:46 zut/filesh.py
--rw-r--r--  2.0 unx     2508 b- defN 23-Jun-10 13:46 zut/git.py
--rw-r--r--  2.0 unx     1494 b- defN 23-Jun-10 13:46 zut/gpg.py
--rw-r--r--  2.0 unx     4770 b- defN 23-Jun-10 13:46 zut/json.py
--rw-r--r--  2.0 unx     4401 b- defN 23-Jun-10 13:46 zut/logging.py
--rw-r--r--  2.0 unx     3441 b- defN 23-Jun-10 13:46 zut/misc.py
--rw-r--r--  2.0 unx     3030 b- defN 23-Jun-10 13:46 zut/numeric.py
--rw-r--r--  2.0 unx     1987 b- defN 23-Jun-10 13:46 zut/tabular.py
--rw-r--r--  2.0 unx     1949 b- defN 23-Jun-10 13:46 zut/text.py
--rw-r--r--  2.0 unx     1581 b- defN 23-Jun-10 13:46 zut/db/__init__.py
--rw-r--r--  2.0 unx    13475 b- defN 23-Jun-10 13:46 zut/db/commons.py
--rw-r--r--  2.0 unx     2288 b- defN 23-Jun-10 13:46 zut/db/mssql.py
--rw-r--r--  2.0 unx     8293 b- defN 23-Jun-10 13:46 zut/db/pg.py
--rw-r--r--  2.0 unx       41 b- defN 23-Jun-10 13:46 zut/db/sql_pg/010_extensions.sql
--rw-r--r--  2.0 unx      464 b- defN 23-Jun-10 13:46 zut/db/sql_pg/020_slugify.sql
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-10 13:46 zut/django/management/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-10 13:46 zut/django/management/commands/__init__.py
--rw-r--r--  2.0 unx     6745 b- defN 23-Jun-10 13:46 zut/django/management/commands/reinit.py
--rw-r--r--  2.0 unx      454 b- defN 23-Jun-10 13:46 zut/django/management/commands/seed_zut.py
--rw-r--r--  2.0 unx     3191 b- defN 23-Jun-10 13:46 zut/django/middleware/__init__.py
--rw-r--r--  2.0 unx       70 b- defN 23-Jun-10 13:46 zut/django/templatetags/__init__.py
--rw-r--r--  2.0 unx     2718 b- defN 23-Jun-10 13:46 zut/django/templatetags/static_lib.py
--rw-r--r--  2.0 unx     1609 b- defN 23-Jun-10 13:46 zut/django/views/MarkdownTemplateView.py
--rw-r--r--  2.0 unx       55 b- defN 23-Jun-10 13:46 zut/django/views/__init__.py
--rw-r--r--  2.0 unx     1722 b- defN 23-Jun-10 13:53 zut/inout/InCsv.py
--rw-r--r--  2.0 unx     1697 b- defN 23-Jun-10 13:46 zut/inout/InDb.py
--rw-r--r--  2.0 unx     1401 b- defN 23-Jun-10 13:46 zut/inout/InExcel.py
--rw-r--r--  2.0 unx     4023 b- defN 23-Jun-10 13:46 zut/inout/InTable.py
--rw-r--r--  2.0 unx     4547 b- defN 23-Jun-10 13:46 zut/inout/OutCsv.py
--rw-r--r--  2.0 unx     2513 b- defN 23-Jun-10 13:46 zut/inout/OutDb.py
--rw-r--r--  2.0 unx     3151 b- defN 23-Jun-10 13:46 zut/inout/OutExcel.py
--rw-r--r--  2.0 unx     3138 b- defN 23-Jun-10 13:53 zut/inout/OutFile.py
--rw-r--r--  2.0 unx     9354 b- defN 23-Jun-10 13:46 zut/inout/OutTable.py
--rw-r--r--  2.0 unx     1121 b- defN 23-Jun-10 13:46 zut/inout/OutTabulate.py
--rw-r--r--  2.0 unx     6198 b- defN 23-Jun-10 13:46 zut/inout/__init__.py
--rw-r--r--  2.0 unx     2734 b- defN 23-Jun-10 13:46 zut/inout/utils.py
--rw-r--r--  2.0 unx     8906 b- defN 23-Jun-10 13:46 zut/network/__init__.py
--rw-r--r--  2.0 unx     3729 b- defN 23-Jun-10 13:46 zut/network/commons.py
--rw-r--r--  2.0 unx     2887 b- defN 23-Jun-10 13:46 zut/network/winhttp.py
--rw-r--r--  2.0 unx     1048 b- defN 23-Jun-10 14:06 zut-0.6.0.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx     4693 b- defN 23-Jun-10 14:06 zut-0.6.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-10 14:06 zut-0.6.0.dist-info/WHEEL
--rw-r--r--  2.0 unx        4 b- defN 23-Jun-10 14:06 zut-0.6.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     4079 b- defN 23-Jun-10 14:06 zut-0.6.0.dist-info/RECORD
-53 files, 176833 bytes uncompressed, 55140 bytes compressed:  68.8%
+Zip file size: 69108 bytes, number of entries: 57
+-rw-rw-rw-  2.0 fat     2054 b- defN 23-Jun-12 11:26 zut/__init__.py
+-rw-rw-rw-  2.0 fat      164 b- defN 23-Jun-12 11:30 zut/_version.py
+-rw-rw-rw-  2.0 fat      276 b- defN 23-Jun-12 06:54 zut/apps.py
+-rw-rw-rw-  2.0 fat      949 b- defN 23-Jun-12 06:54 zut/colors.py
+-rw-rw-rw-  2.0 fat     5710 b- defN 23-Jun-12 11:26 zut/commands.py
+-rw-rw-rw-  2.0 fat     1524 b- defN 23-Jun-12 11:26 zut/config.py
+-rw-rw-rw-  2.0 fat     9366 b- defN 23-Jun-12 06:54 zut/credentials.py
+-rw-rw-rw-  2.0 fat     1218 b- defN 23-Jun-12 06:54 zut/csv.py
+-rw-rw-rw-  2.0 fat     1488 b- defN 23-Jun-12 06:54 zut/datetime.py
+-rw-rw-rw-  2.0 fat    17303 b- defN 23-Jun-12 06:54 zut/excel.py
+-rw-rw-rw-  2.0 fat     8086 b- defN 23-Jun-12 06:54 zut/filesh.py
+-rw-rw-rw-  2.0 fat     2590 b- defN 23-Jun-12 06:54 zut/git.py
+-rw-rw-rw-  2.0 fat     1534 b- defN 23-Jun-12 06:54 zut/gpg.py
+-rw-rw-rw-  2.0 fat     4943 b- defN 23-Jun-12 06:54 zut/json.py
+-rw-rw-rw-  2.0 fat     4530 b- defN 23-Jun-12 06:54 zut/logging.py
+-rw-rw-rw-  2.0 fat     3765 b- defN 23-Jun-12 11:26 zut/misc.py
+-rw-rw-rw-  2.0 fat     3116 b- defN 23-Jun-12 06:54 zut/numeric.py
+-rw-rw-rw-  2.0 fat     2054 b- defN 23-Jun-12 06:54 zut/tabular.py
+-rw-rw-rw-  2.0 fat     2019 b- defN 23-Jun-12 06:54 zut/text.py
+-rw-rw-rw-  2.0 fat     1665 b- defN 23-Jun-12 11:26 zut/db/__init__.py
+-rw-rw-rw-  2.0 fat    15036 b- defN 23-Jun-12 11:26 zut/db/commons.py
+-rw-rw-rw-  2.0 fat     2650 b- defN 23-Jun-12 11:26 zut/db/mssql.py
+-rw-rw-rw-  2.0 fat     8672 b- defN 23-Jun-12 11:26 zut/db/pg.py
+-rw-rw-rw-  2.0 fat       42 b- defN 23-Jun-12 06:54 zut/db/sql_pg/010_extensions.sql
+-rw-rw-rw-  2.0 fat      479 b- defN 23-Jun-12 06:54 zut/db/sql_pg/020_slugify.sql
+-rw-rw-rw-  2.0 fat     1157 b- defN 23-Jun-12 11:26 zut/django/migration_tools.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-12 06:54 zut/django/management/__init__.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-12 06:54 zut/django/management/commands/__init__.py
+-rw-rw-rw-  2.0 fat     8265 b- defN 23-Jun-12 11:26 zut/django/management/commands/reinit.py
+-rw-rw-rw-  2.0 fat     3271 b- defN 23-Jun-12 06:54 zut/django/middleware/__init__.py
+-rw-rw-rw-  2.0 fat       71 b- defN 23-Jun-12 06:54 zut/django/templatetags/__init__.py
+-rw-rw-rw-  2.0 fat     2808 b- defN 23-Jun-12 06:54 zut/django/templatetags/static_lib.py
+-rw-rw-rw-  2.0 fat     9036 b- defN 23-Jun-12 11:26 zut/django/tracking/__init__.py
+-rw-rw-rw-  2.0 fat     4666 b- defN 23-Jun-12 11:26 zut/django/tracking/models.py
+-rw-rw-rw-  2.0 fat      621 b- defN 23-Jun-12 11:26 zut/django/tracking/sql_pg/tracking_history_view.sql
+-rw-rw-rw-  2.0 fat     1652 b- defN 23-Jun-12 06:54 zut/django/views/MarkdownTemplateView.py
+-rw-rw-rw-  2.0 fat       56 b- defN 23-Jun-12 06:54 zut/django/views/__init__.py
+-rw-rw-rw-  2.0 fat     1779 b- defN 23-Jun-12 06:54 zut/inout/InCsv.py
+-rw-rw-rw-  2.0 fat     2169 b- defN 23-Jun-12 11:26 zut/inout/InDb.py
+-rw-rw-rw-  2.0 fat     1449 b- defN 23-Jun-12 06:54 zut/inout/InExcel.py
+-rw-rw-rw-  2.0 fat     4133 b- defN 23-Jun-12 11:26 zut/inout/InTable.py
+-rw-rw-rw-  2.0 fat     4669 b- defN 23-Jun-12 06:54 zut/inout/OutCsv.py
+-rw-rw-rw-  2.0 fat     2901 b- defN 23-Jun-12 11:26 zut/inout/OutDb.py
+-rw-rw-rw-  2.0 fat     3251 b- defN 23-Jun-12 06:54 zut/inout/OutExcel.py
+-rw-rw-rw-  2.0 fat     3235 b- defN 23-Jun-12 11:26 zut/inout/OutFile.py
+-rw-rw-rw-  2.0 fat     9635 b- defN 23-Jun-12 06:54 zut/inout/OutTable.py
+-rw-rw-rw-  2.0 fat     1165 b- defN 23-Jun-12 06:54 zut/inout/OutTabulate.py
+-rw-rw-rw-  2.0 fat     7369 b- defN 23-Jun-12 11:26 zut/inout/__init__.py
+-rw-rw-rw-  2.0 fat     2812 b- defN 23-Jun-12 11:26 zut/inout/utils.py
+-rw-rw-rw-  2.0 fat     9147 b- defN 23-Jun-12 06:54 zut/network/__init__.py
+-rw-rw-rw-  2.0 fat     3850 b- defN 23-Jun-12 06:54 zut/network/commons.py
+-rw-rw-rw-  2.0 fat     2962 b- defN 23-Jun-12 06:54 zut/network/winhttp.py
+-rw-rw-rw-  2.0 fat     1066 b- defN 23-Jun-12 11:30 zut-0.6.1.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     4840 b- defN 23-Jun-12 11:30 zut-0.6.1.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jun-12 11:30 zut-0.6.1.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        4 b- defN 23-Jun-12 11:30 zut-0.6.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     4419 b- defN 23-Jun-12 11:30 zut-0.6.1.dist-info/RECORD
+57 files, 203783 bytes uncompressed, 62266 bytes compressed:  69.4%
```

## zipnote {}

```diff
@@ -9,14 +9,17 @@
 
 Filename: zut/colors.py
 Comment: 
 
 Filename: zut/commands.py
 Comment: 
 
+Filename: zut/config.py
+Comment: 
+
 Filename: zut/credentials.py
 Comment: 
 
 Filename: zut/csv.py
 Comment: 
 
 Filename: zut/datetime.py
@@ -66,35 +69,44 @@
 
 Filename: zut/db/sql_pg/010_extensions.sql
 Comment: 
 
 Filename: zut/db/sql_pg/020_slugify.sql
 Comment: 
 
+Filename: zut/django/migration_tools.py
+Comment: 
+
 Filename: zut/django/management/__init__.py
 Comment: 
 
 Filename: zut/django/management/commands/__init__.py
 Comment: 
 
 Filename: zut/django/management/commands/reinit.py
 Comment: 
 
-Filename: zut/django/management/commands/seed_zut.py
-Comment: 
-
 Filename: zut/django/middleware/__init__.py
 Comment: 
 
 Filename: zut/django/templatetags/__init__.py
 Comment: 
 
 Filename: zut/django/templatetags/static_lib.py
 Comment: 
 
+Filename: zut/django/tracking/__init__.py
+Comment: 
+
+Filename: zut/django/tracking/models.py
+Comment: 
+
+Filename: zut/django/tracking/sql_pg/tracking_history_view.sql
+Comment: 
+
 Filename: zut/django/views/MarkdownTemplateView.py
 Comment: 
 
 Filename: zut/django/views/__init__.py
 Comment: 
 
 Filename: zut/inout/InCsv.py
@@ -138,23 +150,23 @@
 
 Filename: zut/network/commons.py
 Comment: 
 
 Filename: zut/network/winhttp.py
 Comment: 
 
-Filename: zut-0.6.0.dist-info/LICENSE.txt
+Filename: zut-0.6.1.dist-info/LICENSE.txt
 Comment: 
 
-Filename: zut-0.6.0.dist-info/METADATA
+Filename: zut-0.6.1.dist-info/METADATA
 Comment: 
 
-Filename: zut-0.6.0.dist-info/WHEEL
+Filename: zut-0.6.1.dist-info/WHEEL
 Comment: 
 
-Filename: zut-0.6.0.dist-info/top_level.txt
+Filename: zut-0.6.1.dist-info/top_level.txt
 Comment: 
 
-Filename: zut-0.6.0.dist-info/RECORD
+Filename: zut-0.6.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## zut/__init__.py

```diff
@@ -1,31 +1,32 @@
-from __future__ import annotations
-
-try:
-    # Version generated by setuptools_scm during build
-    from ._version import __version__
-except ImportError:
-    __version__ = None
-
-# Top-level API (available directly from "zut" namespace). Does not require any dependency.
-from .colors import Colors
-from .commands import add_func_command, add_module_command, add_package_commands, run_command, exec_command
-from .csv import get_default_csv_delimiter
-from .datetime import now_aware, make_aware, is_aware
-from .filesh import configure_smb_credentials  # other functions should be called directly with the module, e.g. `from zut import filesh`, then `filesh.open_file(...)`
-from .git import get_git_tags, get_git_hash, git_has_changes, check_git_version_tag
-from .gpg import download_gpg_key, verify_gpg_signature
-from .inout import out_file, out_table, in_table, transfer_table, OutFile, OutTable, OutTabulate, OutCsv, OutExcel, OutDb, InTable, InCsv, InExcel, InDb
-from .json import ExtendedJSONDecoder, ExtendedJSONEncoder
-from .logging import configure_logging, DEFAULT_LOGGING_DICTCONFIG
-from .misc import ZUT_ROOT, Literal, Protocol, Self, check_completed_subprocess, get_venv, is_list_or_tuple_of
-from .network import configure_proxy, check_connectivity, check_socket
-from .numeric import human_bytes, parse_decimal
-from .text import slugify, slugify_snake, remove_whitespaces, remove_consecutive_whitespaces, reconfigure_encoding
-
-# NOTE: Inner API: the following features require extra dependencies.
-# zut.credentials       -> [credentials]
-# zut.db                -> [pg] or [mssql]
-# zut.django            -> [django]: django, djangorestframework (for zut.django.middleware), cmarkgfm (for zut.django.views.MarkdownTemplateView)
-# zut.excel             -> [excel]
-# zut.filesh         -> [smb] (for operations on Samba/Windows shares)
-# zut.network.winhttp   -> [winhttp]
+from __future__ import annotations
+
+try:
+    # Version generated by setuptools_scm during build
+    from ._version import __version__
+except ImportError:
+    __version__ = None
+
+# Top-level API (available directly from "zut" namespace). Does not require any dependency.
+from .colors import Colors
+from .commands import add_func_command, add_module_command, add_package_commands, run_command, exec_command, get_command_exit_code, ArgumentParser, RawTextHelpFormatter
+from .config import get_config, ExtendedConfigParser
+from .csv import get_default_csv_delimiter
+from .datetime import now_aware, make_aware, is_aware
+from .filesh import configure_smb_credentials  # other functions should be called directly with the module, e.g. `from zut import filesh`, then `filesh.open_file(...)`
+from .git import get_git_tags, get_git_hash, git_has_changes, check_git_version_tag
+from .gpg import download_gpg_key, verify_gpg_signature
+from .inout import out_file, out_table, in_table, transfer_table, OutFile, OutTable, OutTabulate, OutCsv, OutExcel, OutDb, InTable, InCsv, InExcel, InDb, normalize_inout
+from .json import ExtendedJSONDecoder, ExtendedJSONEncoder
+from .logging import configure_logging, DEFAULT_LOGGING_DICTCONFIG
+from .misc import ZUT_ROOT, Literal, Protocol, Self, check_completed_subprocess, get_venv, is_in_venv, is_list_or_tuple_of
+from .network import configure_proxy, check_connectivity, check_socket
+from .numeric import human_bytes, parse_decimal
+from .text import slugify, slugify_snake, remove_whitespaces, remove_consecutive_whitespaces, reconfigure_encoding
+
+# NOTE: Inner API: the following features require extra dependencies.
+# zut.credentials       -> [credentials]
+# zut.db                -> [pg] or [mssql]
+# zut.django            -> [django]: django, djangorestframework (for zut.django.middleware), cmarkgfm (for zut.django.views.MarkdownTemplateView)
+# zut.excel             -> [excel]
+# zut.filesh         -> [smb] (for operations on Samba/Windows shares)
+# zut.network.winhttp   -> [winhttp]
```

## zut/_version.py

```diff
@@ -1,4 +1,4 @@
-# file generated by setuptools_scm
-# don't change, don't track in version control
-__version__ = version = '0.6.0'
-__version_tuple__ = version_tuple = (0, 6, 0)
+# file generated by setuptools_scm
+# don't change, don't track in version control
+__version__ = version = '0.6.1'
+__version_tuple__ = version_tuple = (0, 6, 1)
```

## zut/apps.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-"""
-Indicate Django application directory. Allows zut to be added in INSTALLED_APP with simple name:
-
-    INSTALLED_APP = [
-        ...
-        'zut',
-        ...
-    ]
-"""
-from django.apps import AppConfig
-
-class ZutAppConfig(AppConfig):
-    name = "zut.django"
+"""
+Indicate Django application directory. Allows zut to be added in INSTALLED_APP with simple name:
+
+    INSTALLED_APP = [
+        ...
+        'zut',
+        ...
+    ]
+"""
+from django.apps import AppConfig
+
+class ZutAppConfig(AppConfig):
+    name = "zut.django"
```

## zut/colors.py

 * *Ordering differences only*

```diff
@@ -1,34 +1,34 @@
-from __future__ import annotations
-import ctypes
-import sys
-
-
-class Colors:
-    """ ANSI color codes """
-    RESET = "\033[0m"
-
-    BLACK = "\033[0;30m"
-    RED = "\033[0;31m"
-    GREEN = "\033[0;32m"
-    YELLOW = "\033[0;33m"
-    BLUE = "\033[0;34m"
-    PURPLE = "\033[0;35m"
-    CYAN = "\033[0;36m"
-    WHITE = "\033[0;37m"
-    GRAY = "\033[0;90m"
-    BOLD_RED = '\033[0;1;31m'
-
-    # Platform/terminal specifics
-    DISABLED = False
-    if not (sys.stdout.isatty() and sys.stderr.isatty()):
-        # Disable colors if we don't write to a terminal
-        DISABLED = True
-        for _ in dir():
-            if isinstance(_, str) and _[0] != "_" and _ not in ['DISABLED']:
-                locals()[_] = ""
-    
-    elif sys.platform == 'win32':
-        # Set Windows console in VT mode
-        kernel32 = ctypes.windll.kernel32
-        kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7)
-        del kernel32
+from __future__ import annotations
+import ctypes
+import sys
+
+
+class Colors:
+    """ ANSI color codes """
+    RESET = "\033[0m"
+
+    BLACK = "\033[0;30m"
+    RED = "\033[0;31m"
+    GREEN = "\033[0;32m"
+    YELLOW = "\033[0;33m"
+    BLUE = "\033[0;34m"
+    PURPLE = "\033[0;35m"
+    CYAN = "\033[0;36m"
+    WHITE = "\033[0;37m"
+    GRAY = "\033[0;90m"
+    BOLD_RED = '\033[0;1;31m'
+
+    # Platform/terminal specifics
+    DISABLED = False
+    if not (sys.stdout.isatty() and sys.stderr.isatty()):
+        # Disable colors if we don't write to a terminal
+        DISABLED = True
+        for _ in dir():
+            if isinstance(_, str) and _[0] != "_" and _ not in ['DISABLED']:
+                locals()[_] = ""
+    
+    elif sys.platform == 'win32':
+        # Set Windows console in VT mode
+        kernel32 = ctypes.windll.kernel32
+        kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7)
+        del kernel32
```

## zut/commands.py

```diff
@@ -1,163 +1,167 @@
-"""
-Add and execute commands easily, based on argparse.
-Usefull for non-Django applications.
-For Django applications, use including command management instead.
-"""
-from __future__ import annotations
-import logging, sys
-from argparse import ArgumentParser, RawTextHelpFormatter, _SubParsersAction
-from types import FunctionType, ModuleType
-from pathlib import Path
-from importlib import import_module
-from importlib.util import find_spec
-from .colors import Colors
-
-logger = logging.getLogger(__name__)
-
-
-def add_func_command(parser: ArgumentParser, func: FunctionType, add_arguments: FunctionType = None, name: str = None, doc: str = None):
-    """
-    Add the given function as a subcommand of the parser.
-    """
-    if name is None:
-        name = func.__name__
-    if doc is None:
-        doc = func.__doc__
-
-    subparsers = get_subparsers(parser)
-    cmdparser: ArgumentParser = subparsers.add_parser(name, help=get_help_text(doc), description=get_description_text(doc), formatter_class=RawTextHelpFormatter)
-    cmdparser.set_defaults(func=func)
-
-    if add_arguments:
-        add_arguments(cmdparser)
-
-    return cmdparser
-
-
-def add_module_command(parser: ArgumentParser, module: str|ModuleType, name: str = None, doc: str = None):
-    """
-    Add the given module as a subcommand of the parser.
-    
-    The command function must be named `handler` and the arguments definition function, if any, must be named `add_arguments`.
-    """
-    if not isinstance(module, ModuleType):
-        module = import_module(module)
-
-    func = getattr(module, 'handle')
-
-    if name is None:
-        name = module.__name__.split(".")[-1]
-        if name.endswith('cmd') and len(name) > len('cmd'):
-            name = name[0:-len('cmd')]
-    
-    add_arguments = getattr(module, 'add_arguments', None)
-    add_func_command(parser, func, add_arguments=add_arguments, name=name, doc=doc)
-
-
-def add_package_commands(parser: ArgumentParser, package: str):
-    """
-    Add all modules in the given package as subcommands of the parser.
-    """
-    package_spec = find_spec(package)
-    if not package_spec:
-        raise KeyError(f"package not found: {package}")
-    if not package_spec.origin:
-        raise KeyError(f"not a package: {package} (did you forget __init__.py ?)")
-    package_path = Path(package_spec.origin).parent
-    
-    for module_path in package_path.iterdir():
-        if module_path.is_dir() or module_path.name.startswith("_") or not module_path.name.endswith(".py"):
-            continue
-
-        module = module_path.stem
-        add_module_command(parser, f"{package}.{module}")
-
-
-def get_subparsers(parser: ArgumentParser) -> _SubParsersAction:
-    """
-    Get or create the subparsers object associated with the given parser.
-    """
-    if isinstance(parser, _SubParsersAction):
-        return parser
-    elif parser._subparsers is not None:
-        return next(filter(lambda action: isinstance(action, _SubParsersAction), parser._subparsers._actions))
-    else:
-        return parser.add_subparsers()
-
-
-def get_help_text(docstring: str):
-    if docstring is None:
-        return None
-    
-    docstring = docstring.strip()
-    try:
-        return docstring[0:docstring.index('\n')].strip()
-    except:
-        return docstring
-
-
-def get_description_text(docstring: str):
-    if docstring is None:
-        return None
-    
-    description = None
-    indent_size = 0
-    
-    for line in docstring.splitlines(keepends=False):
-        if description:
-            description += '\n' + line[indent_size:]
-        else:
-            indent_size = 0
-            for char in line:
-                if char not in [' ', '\t']:
-                    description = line[indent_size:]
-                    break
-                else:
-                    indent_size += 1
-
-    return description
-
-
-def run_command(parser: ArgumentParser, *args, default_func: FunctionType = None, default_add_arguments: FunctionType = None):
-    """
-    Run the command-line application, returning command result.
-    """    
-    func_args, unknown = parser.parse_known_args(*args)
-    func_args = vars(func_args)
-    func = func_args.pop('func', None)
-
-    if func:
-        if unknown:
-            parser.print_usage(file=sys.stderr)
-            print(f"{parser.prog}: error: unrecognized arguments: {' '.join(unknown)}", file=sys.stderr)
-            return 2
-
-    elif default_func:
-        default_parser = ArgumentParser(prog=f"{parser.prog} (default)", formatter_class=RawTextHelpFormatter)
-
-        if default_add_arguments:
-            default_add_arguments(default_parser)
-
-        func_args = vars(default_parser.parse_args(*args))
-        func = default_func
-
-    else:
-        print(f"{Colors.RED}missing command name{Colors.RESET}", file=sys.stderr)
-        return 2
-
-    try:
-        return func(**func_args)
-    except KeyboardInterrupt:
-        logging.getLogger(__name__).error("interrupted")
-        return 1
-
-
-def exec_command(parser: ArgumentParser, *args, default_func: FunctionType = None, default_add_arguments: FunctionType = None):
-    """
-    Run the command-line application and exit with appropriate return code.
-    """
-    r = run_command(*args, parser=parser, default_func=default_func, default_add_arguments=default_add_arguments)
-
-    if not isinstance(r, int):
-        r = 0 if r is None or r is True else 1
-    exit(r)
+"""
+Add and execute commands easily, based on argparse.
+Usefull for non-Django applications.
+For Django applications, use including command management instead.
+"""
+from __future__ import annotations
+import logging, sys
+from argparse import ArgumentParser, RawTextHelpFormatter, _SubParsersAction
+from types import FunctionType, ModuleType
+from pathlib import Path
+from importlib import import_module
+from importlib.util import find_spec
+from .colors import Colors
+
+logger = logging.getLogger(__name__)
+
+
+def add_func_command(parser: ArgumentParser, func: FunctionType, add_arguments: FunctionType = None, name: str = None, doc: str = None):
+    """
+    Add the given function as a subcommand of the parser.
+    """
+    if name is None:
+        name = func.__name__
+    if doc is None:
+        doc = func.__doc__
+
+    subparsers = get_subparsers(parser)
+    cmdparser: ArgumentParser = subparsers.add_parser(name, help=get_help_text(doc), description=get_description_text(doc), formatter_class=RawTextHelpFormatter)
+    cmdparser.set_defaults(func=func)
+
+    if add_arguments:
+        add_arguments(cmdparser)
+
+    return cmdparser
+
+
+def add_module_command(parser: ArgumentParser, module: str|ModuleType, name: str = None, doc: str = None):
+    """
+    Add the given module as a subcommand of the parser.
+    
+    The command function must be named `handler` and the arguments definition function, if any, must be named `add_arguments`.
+    """
+    if not isinstance(module, ModuleType):
+        module = import_module(module)
+
+    func = getattr(module, 'handle')
+
+    if name is None:
+        name = module.__name__.split(".")[-1]
+        if name.endswith('cmd') and len(name) > len('cmd'):
+            name = name[0:-len('cmd')]
+    
+    add_arguments = getattr(module, 'add_arguments', None)
+    add_func_command(parser, func, add_arguments=add_arguments, name=name, doc=doc)
+
+
+def add_package_commands(parser: ArgumentParser, package: str):
+    """
+    Add all modules in the given package as subcommands of the parser.
+    """
+    package_spec = find_spec(package)
+    if not package_spec:
+        raise KeyError(f"package not found: {package}")
+    if not package_spec.origin:
+        raise KeyError(f"not a package: {package} (did you forget __init__.py ?)")
+    package_path = Path(package_spec.origin).parent
+    
+    for module_path in package_path.iterdir():
+        if module_path.is_dir() or module_path.name.startswith("_") or not module_path.name.endswith(".py"):
+            continue
+
+        module = module_path.stem
+        add_module_command(parser, f"{package}.{module}")
+
+
+def get_subparsers(parser: ArgumentParser) -> _SubParsersAction:
+    """
+    Get or create the subparsers object associated with the given parser.
+    """
+    if isinstance(parser, _SubParsersAction):
+        return parser
+    elif parser._subparsers is not None:
+        return next(filter(lambda action: isinstance(action, _SubParsersAction), parser._subparsers._actions))
+    else:
+        return parser.add_subparsers()
+
+
+def get_help_text(docstring: str):
+    if docstring is None:
+        return None
+    
+    docstring = docstring.strip()
+    try:
+        return docstring[0:docstring.index('\n')].strip()
+    except:
+        return docstring
+
+
+def get_description_text(docstring: str):
+    if docstring is None:
+        return None
+    
+    description = None
+    indent_size = 0
+    
+    for line in docstring.splitlines(keepends=False):
+        if description:
+            description += '\n' + line[indent_size:]
+        else:
+            indent_size = 0
+            for char in line:
+                if char not in [' ', '\t']:
+                    description = line[indent_size:]
+                    break
+                else:
+                    indent_size += 1
+
+    return description
+
+
+def run_command(parser: ArgumentParser, *args, default_func: FunctionType = None, default_add_arguments: FunctionType = None):
+    """
+    Run the command-line application, returning command result.
+    """    
+    func_args, unknown = parser.parse_known_args(*args)
+    func_args = vars(func_args)
+    func = func_args.pop('func', None)
+
+    if func:
+        if unknown:
+            parser.print_usage(file=sys.stderr)
+            print(f"{parser.prog}: error: unrecognized arguments: {' '.join(unknown)}", file=sys.stderr)
+            return 2
+
+    elif default_func:
+        default_parser = ArgumentParser(prog=f"{parser.prog} (default)", formatter_class=RawTextHelpFormatter)
+
+        if default_add_arguments:
+            default_add_arguments(default_parser)
+
+        func_args = vars(default_parser.parse_args(*args))
+        func = default_func
+
+    else:
+        print(f"{Colors.RED}missing command name{Colors.RESET}", file=sys.stderr)
+        return 2
+
+    try:
+        return func(**func_args)
+    except KeyboardInterrupt:
+        logging.getLogger(__name__).error("interrupted")
+        return 1
+    
+
+def get_command_exit_code(command_return_value) -> int:    
+    if not isinstance(command_return_value, int):
+        command_return_value = 0 if command_return_value is None or command_return_value is True else 1
+    return command_return_value
+
+
+def exec_command(parser: ArgumentParser, *args, default_func: FunctionType = None, default_add_arguments: FunctionType = None):
+    """
+    Run the command-line application and exit with appropriate return code.
+    """
+    r = run_command(*args, parser=parser, default_func=default_func, default_add_arguments=default_add_arguments)
+    r = get_command_exit_code(r)
+    exit(r)
```

## zut/credentials.py

 * *Ordering differences only*

```diff
@@ -1,243 +1,243 @@
-from __future__ import annotations
-
-import os
-import subprocess
-import sys
-from argparse import ArgumentParser
-from datetime import datetime
-from getpass import getpass
-
-
-if sys.platform == "win32":
-    import win32cred
-    from keyring.backends.Windows import WinVaultKeyring
-
-    # See: https://docs.microsoft.com/en-us/windows/win32/api/wincred/ns-wincred-credentiala
-    CRED_TYPE_GENERIC = 1 # The credential is a generic credential. The credential will not be used by any particular authentication package. The credential will be stored securely but has no other significant characteristics.
-    CRED_TYPE_DOMAIN_PASSWORD = 2 # The credential is a password credential and is specific to Microsoft's authentication packages. The NTLM, Kerberos, and Negotiate authentication packages will automatically use this credential when connecting to the named target.
-    CRED_TYPE_DOMAIN_CERTIFICATE = 3 # The credential is a certificate credential and is specific to Microsoft's authentication packages. The Kerberos, Negotiate, and Schannel authentication packages automatically use this credential when connecting to the named target.
-
-    enumerate_additional_props = ["LastWritten", "TargetAlias", "Comment", "Type", "Flags", "CredentialBlob"]
-    ENUMERATE_HEADERS = ["Service", "Username", *enumerate_additional_props]
-
-    def enumerate_credentials(service=None, username=None):
-        if service is not None:
-            service = service.lower()
-        if username is not None:
-            username = username.lower()
-        
-        for cred in win32cred.CredEnumerate():
-            cred_service = cred["TargetName"]
-            cred_username = cred["UserName"]
-
-            # Filter
-            if service is not None:
-                if not service in cred_service.lower():
-                    continue
-            if username is not None:
-                if cred_username is None or not username in cred_username.lower():
-                    continue
-
-            # Additional properties
-            props = []
-            for prop in enumerate_additional_props:
-                if prop == "CredentialBlob":
-                    props.append("X" if cred[prop] else "")
-                elif prop == "Type":
-                    indication = ""
-                    if cred["Type"] == CRED_TYPE_GENERIC:
-                        indication = "generic"
-                    elif cred["Type"] == CRED_TYPE_DOMAIN_PASSWORD:
-                        indication = "domain password"
-                    elif cred["Type"] == CRED_TYPE_DOMAIN_CERTIFICATE:
-                        indication = "domain certificate"
-                    props.append(str(cred[prop]) + (" (%s)" % indication if indication else ""))
-                else:
-                    props.append(cred[prop])
-
-            yield [cred_service, cred_username, *props]
-    
-
-    def get_username(service):
-        for cred in win32cred.CredEnumerate():
-            if cred["Type"] == CRED_TYPE_GENERIC and cred["TargetName"] == service and cred["UserName"]:
-                return cred["UserName"]
-
-        # not found in credential manager
-        return None
-
-
-    _keyring = None
-
-    def _get_keyring():
-        global _keyring
-        if _keyring is None:
-            _keyring = WinVaultKeyring()
-        return _keyring
-
-    def get_password(service, username):
-        return _get_keyring().get_password(service, username)
-
-
-    def set_password(service, username, password):
-        return _get_keyring().set_password(service, username, password)
-
-
-    def delete_password(service, username):
-        return _get_keyring().delete_password(service, username)
-
-
-else: # sys.platform != "win32"
-    import re
-    from glob import glob
-
-    _several_slashes_re = re.compile(r"/{2,}")
-
-    def _sanitize_part(part):
-        """ Replace several slashes with only one """
-        if not part:
-            return part
-        part = part.replace("://", ":")
-        return _several_slashes_re.sub("/", part)
-        
-    def _get_key(service, username):
-        return os.path.join(
-            _sanitize_part(service),
-            _sanitize_part(username),
-        ).rstrip("/")
-    
-    ENUMERATE_HEADERS = ["Service", "Username", "LastWritten", "Path"]
-
-    def enumerate_credentials(service=None, username=None):
-        if service is not None:
-            service = _sanitize_part(service.lower())
-        if username is not None:
-            username = _sanitize_part(username.lower())
-
-        prefix = os.path.expanduser("~/.password-store/")
-        for path in glob(f"{prefix}**/*.gpg", recursive=True):
-            name_without_extension = path[:-4]
-            name_parts = name_without_extension[len(prefix):].rsplit("/", 1)
-            if len(name_parts) == 0:             
-                cred_service = ""
-                cred_username = ""
-            elif len(name_parts) == 1:                
-                cred_service = name_parts[0]
-                cred_username = ""
-            else:
-                cred_service = "/".join(name_parts[:-1])
-                cred_username = name_parts[-1]
-
-            # Filter
-            if service is not None:
-                if not service in cred_service.lower():
-                    continue
-            if username is not None:
-                if cred_username or not username in cred_username.lower():
-                    continue
-
-            last_written = datetime.fromtimestamp(os.path.getmtime(path))
-            yield [cred_service, cred_username, last_written, path]
-
-        
-    def get_username(service):
-        service = _sanitize_part(service)
-        for cred in enumerate_credentials():
-            if cred[0] == service:
-                return cred[1]
-
-        # not found in credential manager
-        return None
-
-    
-    _not_in_the_password_store_re = re.compile(r"^Error: .+ is not in the password store.$")
-    def get_password(service, username):
-        key = _get_key(service, username)
-        try:
-            cp = subprocess.run(["pass", "show", key], capture_output=True, check=True, text=True)
-        except subprocess.CalledProcessError as err:
-            if cp.returncode == 1 and _not_in_the_password_store_re.match(cp.stderr):
-                return None
-            else:
-                raise err
-        return cp.stdout.splitlines()[0]
-
-
-    def set_password(service, username, password):
-        if not service:
-            raise ValueError("service cannot be empty")
-
-        password = password.splitlines()[0]
-        inp = '%s\n' % password
-        inp *= 2
-        
-        key = _get_key(service, username)
-        subprocess.run(['pass', 'insert', '--force', key], input=inp, capture_output=True, check=True, text=True)
-
-
-    def delete_password(service, username):
-        key = _get_key(service, username)
-        subprocess.run(['pass', 'rm', '--force', key], check=True, capture_output=True)
-
-
-def add_arguments(parser: ArgumentParser):
-    parser.add_argument("action", nargs="?", choices=["list", "ls", "get", "set", "delete", "del", "rm"], default="list", help="action to perform")
-    parser.add_argument("service", nargs="?", help="service name")
-    parser.add_argument("username", nargs="?", help="user name")
-
-
-def handle(action: str = None, service: str = None, username: str = None):
-    """
-    List, get or set credentials from the credentials manager.
-    """    
-    if action == "get":
-        # Get a credential
-        if not service:
-            service = input("Service: ")
-        if not username:
-            username = input(f"Username for service \"{service}\": ")
-        print(get_password(service, username))
-
-    elif action == "set":
-        # Set a credential
-        if not service:
-            service = input("Service: ")
-        if not username:
-            username = input(f"Username for service \"{service}\": ")
-        set_password(service, username, getpass(f"Password for service \"{service}\" (username: \"{username}\"): "))
-
-    elif action in ["delete", "del", "rm"]:
-        # Delete a credential
-        if not service:
-            service = input("Service: ")
-        if not username:
-            username = input(f"Username for service \"{service}\": ")
-        delete_password(service, username)
-
-    else: # list
-        from tabulate import tabulate
-
-        # Search/list available credentials
-        if sys.platform == "win32":
-            print("See: rundll32.exe keymgr.dll, KRShowKeyMgr")
-        else:
-            print("See: find ~/.password-store -type f -name '*.gpg'")
-            
-        rows = []
-        for cred in enumerate_credentials(service=service, username=username):
-            rows.append(cred)
-
-        rows.sort(key=lambda row: row[0].lower())
-
-        # Display LastWritten as simplified local time
-        try:
-            i = ENUMERATE_HEADERS.index("LastWritten")            
-            for row in rows:
-                localdt = row[i].astimezone()
-                row[i] = localdt.strftime("%H:%M" if localdt.date() == datetime.now().date() else "%Y-%m-%d")
-
-        except ValueError:
-            pass
-
-
-        print(tabulate(rows, ENUMERATE_HEADERS))
+from __future__ import annotations
+
+import os
+import subprocess
+import sys
+from argparse import ArgumentParser
+from datetime import datetime
+from getpass import getpass
+
+
+if sys.platform == "win32":
+    import win32cred
+    from keyring.backends.Windows import WinVaultKeyring
+
+    # See: https://docs.microsoft.com/en-us/windows/win32/api/wincred/ns-wincred-credentiala
+    CRED_TYPE_GENERIC = 1 # The credential is a generic credential. The credential will not be used by any particular authentication package. The credential will be stored securely but has no other significant characteristics.
+    CRED_TYPE_DOMAIN_PASSWORD = 2 # The credential is a password credential and is specific to Microsoft's authentication packages. The NTLM, Kerberos, and Negotiate authentication packages will automatically use this credential when connecting to the named target.
+    CRED_TYPE_DOMAIN_CERTIFICATE = 3 # The credential is a certificate credential and is specific to Microsoft's authentication packages. The Kerberos, Negotiate, and Schannel authentication packages automatically use this credential when connecting to the named target.
+
+    enumerate_additional_props = ["LastWritten", "TargetAlias", "Comment", "Type", "Flags", "CredentialBlob"]
+    ENUMERATE_HEADERS = ["Service", "Username", *enumerate_additional_props]
+
+    def enumerate_credentials(service=None, username=None):
+        if service is not None:
+            service = service.lower()
+        if username is not None:
+            username = username.lower()
+        
+        for cred in win32cred.CredEnumerate():
+            cred_service = cred["TargetName"]
+            cred_username = cred["UserName"]
+
+            # Filter
+            if service is not None:
+                if not service in cred_service.lower():
+                    continue
+            if username is not None:
+                if cred_username is None or not username in cred_username.lower():
+                    continue
+
+            # Additional properties
+            props = []
+            for prop in enumerate_additional_props:
+                if prop == "CredentialBlob":
+                    props.append("X" if cred[prop] else "")
+                elif prop == "Type":
+                    indication = ""
+                    if cred["Type"] == CRED_TYPE_GENERIC:
+                        indication = "generic"
+                    elif cred["Type"] == CRED_TYPE_DOMAIN_PASSWORD:
+                        indication = "domain password"
+                    elif cred["Type"] == CRED_TYPE_DOMAIN_CERTIFICATE:
+                        indication = "domain certificate"
+                    props.append(str(cred[prop]) + (" (%s)" % indication if indication else ""))
+                else:
+                    props.append(cred[prop])
+
+            yield [cred_service, cred_username, *props]
+    
+
+    def get_username(service):
+        for cred in win32cred.CredEnumerate():
+            if cred["Type"] == CRED_TYPE_GENERIC and cred["TargetName"] == service and cred["UserName"]:
+                return cred["UserName"]
+
+        # not found in credential manager
+        return None
+
+
+    _keyring = None
+
+    def _get_keyring():
+        global _keyring
+        if _keyring is None:
+            _keyring = WinVaultKeyring()
+        return _keyring
+
+    def get_password(service, username):
+        return _get_keyring().get_password(service, username)
+
+
+    def set_password(service, username, password):
+        return _get_keyring().set_password(service, username, password)
+
+
+    def delete_password(service, username):
+        return _get_keyring().delete_password(service, username)
+
+
+else: # sys.platform != "win32"
+    import re
+    from glob import glob
+
+    _several_slashes_re = re.compile(r"/{2,}")
+
+    def _sanitize_part(part):
+        """ Replace several slashes with only one """
+        if not part:
+            return part
+        part = part.replace("://", ":")
+        return _several_slashes_re.sub("/", part)
+        
+    def _get_key(service, username):
+        return os.path.join(
+            _sanitize_part(service),
+            _sanitize_part(username),
+        ).rstrip("/")
+    
+    ENUMERATE_HEADERS = ["Service", "Username", "LastWritten", "Path"]
+
+    def enumerate_credentials(service=None, username=None):
+        if service is not None:
+            service = _sanitize_part(service.lower())
+        if username is not None:
+            username = _sanitize_part(username.lower())
+
+        prefix = os.path.expanduser("~/.password-store/")
+        for path in glob(f"{prefix}**/*.gpg", recursive=True):
+            name_without_extension = path[:-4]
+            name_parts = name_without_extension[len(prefix):].rsplit("/", 1)
+            if len(name_parts) == 0:             
+                cred_service = ""
+                cred_username = ""
+            elif len(name_parts) == 1:                
+                cred_service = name_parts[0]
+                cred_username = ""
+            else:
+                cred_service = "/".join(name_parts[:-1])
+                cred_username = name_parts[-1]
+
+            # Filter
+            if service is not None:
+                if not service in cred_service.lower():
+                    continue
+            if username is not None:
+                if cred_username or not username in cred_username.lower():
+                    continue
+
+            last_written = datetime.fromtimestamp(os.path.getmtime(path))
+            yield [cred_service, cred_username, last_written, path]
+
+        
+    def get_username(service):
+        service = _sanitize_part(service)
+        for cred in enumerate_credentials():
+            if cred[0] == service:
+                return cred[1]
+
+        # not found in credential manager
+        return None
+
+    
+    _not_in_the_password_store_re = re.compile(r"^Error: .+ is not in the password store.$")
+    def get_password(service, username):
+        key = _get_key(service, username)
+        try:
+            cp = subprocess.run(["pass", "show", key], capture_output=True, check=True, text=True)
+        except subprocess.CalledProcessError as err:
+            if cp.returncode == 1 and _not_in_the_password_store_re.match(cp.stderr):
+                return None
+            else:
+                raise err
+        return cp.stdout.splitlines()[0]
+
+
+    def set_password(service, username, password):
+        if not service:
+            raise ValueError("service cannot be empty")
+
+        password = password.splitlines()[0]
+        inp = '%s\n' % password
+        inp *= 2
+        
+        key = _get_key(service, username)
+        subprocess.run(['pass', 'insert', '--force', key], input=inp, capture_output=True, check=True, text=True)
+
+
+    def delete_password(service, username):
+        key = _get_key(service, username)
+        subprocess.run(['pass', 'rm', '--force', key], check=True, capture_output=True)
+
+
+def add_arguments(parser: ArgumentParser):
+    parser.add_argument("action", nargs="?", choices=["list", "ls", "get", "set", "delete", "del", "rm"], default="list", help="action to perform")
+    parser.add_argument("service", nargs="?", help="service name")
+    parser.add_argument("username", nargs="?", help="user name")
+
+
+def handle(action: str = None, service: str = None, username: str = None):
+    """
+    List, get or set credentials from the credentials manager.
+    """    
+    if action == "get":
+        # Get a credential
+        if not service:
+            service = input("Service: ")
+        if not username:
+            username = input(f"Username for service \"{service}\": ")
+        print(get_password(service, username))
+
+    elif action == "set":
+        # Set a credential
+        if not service:
+            service = input("Service: ")
+        if not username:
+            username = input(f"Username for service \"{service}\": ")
+        set_password(service, username, getpass(f"Password for service \"{service}\" (username: \"{username}\"): "))
+
+    elif action in ["delete", "del", "rm"]:
+        # Delete a credential
+        if not service:
+            service = input("Service: ")
+        if not username:
+            username = input(f"Username for service \"{service}\": ")
+        delete_password(service, username)
+
+    else: # list
+        from tabulate import tabulate
+
+        # Search/list available credentials
+        if sys.platform == "win32":
+            print("See: rundll32.exe keymgr.dll, KRShowKeyMgr")
+        else:
+            print("See: find ~/.password-store -type f -name '*.gpg'")
+            
+        rows = []
+        for cred in enumerate_credentials(service=service, username=username):
+            rows.append(cred)
+
+        rows.sort(key=lambda row: row[0].lower())
+
+        # Display LastWritten as simplified local time
+        try:
+            i = ENUMERATE_HEADERS.index("LastWritten")            
+            for row in rows:
+                localdt = row[i].astimezone()
+                row[i] = localdt.strftime("%H:%M" if localdt.date() == datetime.now().date() else "%Y-%m-%d")
+
+        except ValueError:
+            pass
+
+
+        print(tabulate(rows, ENUMERATE_HEADERS))
```

## zut/csv.py

 * *Ordering differences only*

```diff
@@ -1,44 +1,44 @@
-from __future__ import annotations
-import csv
-from io import IOBase
-from locale import getlocale
-from pathlib import Path
-
-from .text import reconfigure_encoding
-from . import filesh
-
-def get_default_csv_delimiter():
-    locale = getlocale()
-    if locale and locale[0] and locale[0].startswith('fr'):
-        return ';'
-    else:
-        return ','
-
-
-def get_csv_headers(csv_file: str|Path|IOBase, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"'):
-    if delimiter is None:
-        delimiter = get_default_csv_delimiter()
-        
-    fp = None
-    fp_to_close = None
-
-    try:        
-        if isinstance(csv_file, (str,Path)):
-            fp = filesh.open_file(csv_file, 'r', newline='', encoding=encoding)
-            fp_to_close = fp
-            encoding = reconfigure_encoding(fp, encoding)
-        else:
-            fp = csv_file
-            fp.seek(0)
-            
-        reader = csv.reader(fp, delimiter=delimiter, quotechar=quotechar)
-        try:
-            return next(reader)
-        except StopIteration:
-            return None
-
-    finally:
-        if fp_to_close:
-            fp_to_close.close()
-        else:
+from __future__ import annotations
+import csv
+from io import IOBase
+from locale import getlocale
+from pathlib import Path
+
+from .text import reconfigure_encoding
+from . import filesh
+
+def get_default_csv_delimiter():
+    locale = getlocale()
+    if locale and locale[0] and locale[0].startswith('fr'):
+        return ';'
+    else:
+        return ','
+
+
+def get_csv_headers(csv_file: str|Path|IOBase, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"'):
+    if delimiter is None:
+        delimiter = get_default_csv_delimiter()
+        
+    fp = None
+    fp_to_close = None
+
+    try:        
+        if isinstance(csv_file, (str,Path)):
+            fp = filesh.open_file(csv_file, 'r', newline='', encoding=encoding)
+            fp_to_close = fp
+            encoding = reconfigure_encoding(fp, encoding)
+        else:
+            fp = csv_file
+            fp.seek(0)
+            
+        reader = csv.reader(fp, delimiter=delimiter, quotechar=quotechar)
+        try:
+            return next(reader)
+        except StopIteration:
+            return None
+
+    finally:
+        if fp_to_close:
+            fp_to_close.close()
+        else:
             fp.seek(0)
```

## zut/datetime.py

 * *Ordering differences only*

```diff
@@ -1,52 +1,52 @@
-from __future__ import annotations
-from datetime import datetime, time, timezone, timedelta
-from .misc import Literal
-
-
-def is_aware(value: datetime|time):
-    if value is None:
-        return False
-    return value.utcoffset() is not None
-
-
-def make_aware(value: datetime|time, tz: Literal['local']|timezone = 'local'):
-    if value is None:
-        return None
-    return value.astimezone(None if tz == 'local' else tz)
-
-
-def now_aware(tz: Literal['local']|timezone = 'local', no_microsecond = False):
-    now = datetime.now(timezone.utc).astimezone(None if tz == 'local' else tz)
-    if no_microsecond:
-        now = now.replace(microsecond=0)
-    return now
-
-
-def duration_iso_string(duration):
-    # Adapted from: django.utils.duration.duration_iso_string
-    if duration < timedelta(0):
-        sign = "-"
-        duration *= -1
-    else:
-        sign = ""
-
-    days, hours, minutes, seconds, microseconds = _get_duration_components(duration)
-    ms = ".{:06d}".format(microseconds) if microseconds else ""
-    return "{}P{}DT{:02d}H{:02d}M{:02d}{}S".format(
-        sign, days, hours, minutes, seconds, ms
-    )
-
-
-def _get_duration_components(duration: timedelta):
-    days = duration.days
-    seconds = duration.seconds
-    microseconds = duration.microseconds
-
-    minutes = seconds // 60
-    seconds = seconds % 60
-
-    hours = minutes // 60
-    minutes = minutes % 60
-
-    return days, hours, minutes, seconds, microseconds
-
+from __future__ import annotations
+from datetime import datetime, time, timezone, timedelta
+from .misc import Literal
+
+
+def is_aware(value: datetime|time):
+    if value is None:
+        return False
+    return value.utcoffset() is not None
+
+
+def make_aware(value: datetime|time, tz: Literal['local']|timezone = 'local'):
+    if value is None:
+        return None
+    return value.astimezone(None if tz == 'local' else tz)
+
+
+def now_aware(tz: Literal['local']|timezone = 'local', no_microsecond = False):
+    now = datetime.now(timezone.utc).astimezone(None if tz == 'local' else tz)
+    if no_microsecond:
+        now = now.replace(microsecond=0)
+    return now
+
+
+def duration_iso_string(duration):
+    # Adapted from: django.utils.duration.duration_iso_string
+    if duration < timedelta(0):
+        sign = "-"
+        duration *= -1
+    else:
+        sign = ""
+
+    days, hours, minutes, seconds, microseconds = _get_duration_components(duration)
+    ms = ".{:06d}".format(microseconds) if microseconds else ""
+    return "{}P{}DT{:02d}H{:02d}M{:02d}{}S".format(
+        sign, days, hours, minutes, seconds, ms
+    )
+
+
+def _get_duration_components(duration: timedelta):
+    days = duration.days
+    seconds = duration.seconds
+    microseconds = duration.microseconds
+
+    minutes = seconds // 60
+    seconds = seconds % 60
+
+    hours = minutes // 60
+    minutes = minutes % 60
+
+    return days, hours, minutes, seconds, microseconds
+
```

## zut/excel.py

 * *Ordering differences only*

```diff
@@ -1,445 +1,445 @@
-from __future__ import annotations
-import logging
-from pathlib import Path
-from typing import Any
-from .tabular import Row
-from .misc import Literal
-from . import filesh
-from openpyxl import load_workbook, Workbook, DEFUSEDXML
-from openpyxl.worksheet.worksheet import Worksheet
-from openpyxl.worksheet.table import Table, TableColumn, TableFormula, TableStyleInfo
-from openpyxl.worksheet.formula import DataTableFormula, ArrayFormula
-from openpyxl.worksheet.filters import AutoFilter
-from openpyxl.cell.cell import Cell
-from openpyxl.styles.differential import DifferentialStyle, DifferentialStyleList
-from openpyxl.styles.fills import PatternFill
-from openpyxl.utils import range_boundaries, get_column_letter
-
-logger = logging.getLogger(__name__)
-
-
-class ExcelWorkbook:
-    _workbook_cache: dict[str,ExcelWorkbook] = {}
-    """ Workbooks per canonical path """
-    
-    _defusedxml_alert_emitted = False
-
-    def __init__(self, path: str|Path):        
-        if not DEFUSEDXML and not self.__class__._defusedxml_alert_emitted:
-            logger.warning("By default openpyxl does not guard against quadratic blowup or billion laughs xml attacks. To guard against these attacks install defusedxml.")
-            self.__class__._defusedxml_alert_emitted = True
-
-        self.path = Path(path) if not isinstance(path, Path) else path
-        
-        if filesh.exists(self.path):
-            logger.debug("load workbook %s", self.path)
-            with filesh.open_file(self.path, 'rb') as fp:
-                self._pyxl_workbook: Workbook = load_workbook(fp)
-            self._create_next_table_in_active_sheet = False
-        else:
-            logger.debug("create workbook for %s", self.path)
-            self._pyxl_workbook = Workbook()
-            self._create_next_table_in_active_sheet = True
-
-        self._tables: dict[str,ExcelTable] = {}
-        self._needs_save = False
-
-
-    @classmethod
-    def get_or_create_cached(cls, path: str|Path) -> ExcelWorkbook:
-        path = Path(path) if not isinstance(path, Path) else path
-        canonical_path = str(path.resolve())
-        if not canonical_path in cls._workbook_cache:
-            cls._workbook_cache[canonical_path] = ExcelWorkbook(path)
-        return cls._workbook_cache[canonical_path]
-
-
-    def close(self):
-        if not self._needs_save:
-            return
-        
-        for table in self._tables.values():
-            table.redefine()
-        
-        logger.debug("save workbook %s", self.path)
-        with filesh.open_file(self.path, 'wb') as fp:
-            self._pyxl_workbook.save(fp)
-
-        self._needs_save = False
-        
-
-    def get_table(self, name: str, default: Any|Literal['__raise__'] = '__raise__') -> ExcelTable:
-        if name in self._tables:
-            return self._tables[name]
-        
-        for sheet_name in self._pyxl_workbook.sheetnames:
-            pyxl_worksheet: Worksheet = self._pyxl_workbook[sheet_name]
-            if name in pyxl_worksheet.tables:
-                pyxl_table = pyxl_worksheet.tables[name]
-                self._tables[name] = ExcelTable(pyxl_table, pyxl_worksheet, self)
-                return self._tables[name]
-            
-        if default == '__raise__':
-            raise KeyError(f"no table found with name \"{name}\" in workbook \"{self.path}\"")
-        else:
-            return default
-        
-
-    def create_table(self, name: str, no_headers: bool = False) -> ExcelTable:
-        for sheet_name in self._pyxl_workbook.sheetnames:
-            pyxl_worksheet: Worksheet = self._pyxl_workbook[sheet_name]
-            if name in pyxl_worksheet.tables:
-                raise ValueError(f"table {name} already exist")
-    
-        self._needs_save = True
-        
-        if self._create_next_table_in_active_sheet:
-            pyxl_worksheet: Worksheet = self._pyxl_workbook.active
-            pyxl_worksheet.title = name
-            self._create_next_table_in_active_sheet = False
-        else:
-            pyxl_worksheet: Worksheet = self._pyxl_workbook.create_sheet(title=name)
-
-        self._tables[name] = ExcelTable(name, pyxl_worksheet, self)
-        return self._tables[name]
-
-
-class ExcelTable:
-    def __init__(self, pyxl_table: Table|str, pyxl_worksheet: Worksheet, workbook: ExcelWorkbook, no_headers: bool = None):
-        self.workbook = workbook
-        self._pyxl_worksheet = pyxl_worksheet        
-        self._column_formats: dict[int,dict[str,Any]] = {}
-
-        if isinstance(pyxl_table, str):
-            self._pyxl_table: Table = None
-            self.name: str = pyxl_table
-            if no_headers:
-                self.has_headers = False
-                self._min_row = 1
-            else:
-                self.has_headers = True
-                self._min_row = 2
-            self._min_col = 1
-            self._row_count = 0
-            self._column_names: list[str] = []
-
-        elif isinstance(pyxl_table, Table):
-            self._pyxl_table: Table = pyxl_table
-            self.name: str = pyxl_table.name
-            
-            self._min_col, min_row, max_col, max_row = range_boundaries(pyxl_table.ref)
-
-            if pyxl_table.headerRowCount == 0:
-                self.has_headers = False
-                self._min_row = min_row
-            elif pyxl_table.headerRowCount == 1:
-                self.has_headers = True
-                self._min_row = min_row + 1
-            else:
-                raise ValueError(f'invalid headerRowCount: {pyxl_table.headerRowCount}')
-            
-            self._row_count = max_row - self._min_row + 1
-            col_count = max_col - self._min_col + 1
-        
-            self._column_names: list[str] = list(self._pyxl_table.column_names)
-            # NOTE: if no headers, default names are returned, example in French: ['Colonne1', 'Colonne2']
-            if len(self._column_names) != col_count:
-                raise ValueError(f'invalid column_names length ({len(self._column_names)}, expected {col_count}): {self._column_names}')
-        else:
-            raise ValueError(f"invalid type for pyxl_table: {type(pyxl_table).__name__}")
-
-
-    @property
-    def column_names(self) -> list[str]:
-        return self._column_names
-
-    @property
-    def min_row(self) -> int:
-        return self._min_row
-    
-    @property
-    def min_col(self) -> int:
-        return self._min_col
-    
-    @property
-    def row_count(self) -> int:
-        return self._row_count
-    
-    @property
-    def col_count(self) -> int:
-        return len(self._column_names)
-    
-    @property
-    def ref(self) -> str:
-        if self.col_count == 0:
-            raise ValueError(f"cannot get table ref: table does not contain any column")
-        if self.row_count == 0 and not self.has_headers:
-            raise ValueError(f"cannot get table ref: table does not contain any line")
-        
-        min_row = self._min_row
-        if self.has_headers:
-            min_row -= 1
-        
-        return f"{get_column_letter(self._min_col)}{min_row}:{get_column_letter(self._min_col + self.col_count - 1)}{self._min_row + self._row_count - 1}"
-
-
-    def get_row(self, index: int) -> Row:
-        if index == -1:
-            if self.row_count == 0:
-                raise ValueError(f"cannot get last row: table does not contain any row")
-            index = self.row_count - 1
-        elif index < 0:
-            raise ValueError(f"invalid row index: {index}")
-        
-        if index >= self.row_count:
-            raise ValueError(f"cannot get row at index {index}: table contains {self.row_count} rows")
-
-        return Row(
-            get=lambda col_index: self.get_value(index, col_index),
-            set=lambda col_index, value: self.set_value(index, col_index, value),
-            headers=self.column_names
-        )
-    
-
-    def insert_row(self) -> Row:
-        self.workbook._needs_save = True
-        row_index = self._row_count
-
-        # ensure empty values
-        for col_index in range(0, self.col_count):
-            cell = self._pyxl_worksheet.cell(self._min_row + row_index, self._min_col + col_index)
-            if cell.value is not None:
-                raise ValueError(f'cannot insert row: row {row_index} is not empty')
-
-        self._row_count += 1
-
-        # erase old styles and apply column format
-        for col_index in range(0, self.col_count):
-            self._erase_cell(row_index, col_index)
-            
-        return self.get_row(row_index)
-    
-
-    def insert_col(self, name: str):
-        self.workbook._needs_save = True
-        if not name:
-            raise ValueError(f'name cannot be empty')
-        if name in self.column_names:
-            raise ValueError(f'column name already used: {name}')
-            
-        col_index = self.col_count
-
-        # ensure empty values
-        for row_index in range(0, self._row_count):
-            cell = self._pyxl_worksheet.cell(self._min_row + row_index, self._min_col + col_index)
-            if cell.value is not None:
-                raise ValueError(f'cannot insert column: column {col_index} is not empty')
-
-        self.column_names.append(name) # implies self.col_count += 1
-
-        if self.has_headers:
-            cell = self._pyxl_worksheet.cell(self._min_row - 1, self._min_col + col_index)
-            cell.value = name
-
-        # erase old styles and apply column format
-        for row_index in range(0, self._row_count):
-            self._erase_cell(row_index, col_index)
-            
-
-    def truncate(self):
-        self.workbook._needs_save = True
-        prev_row_count = self._row_count
-        self._row_count = 0
-
-        for row_index in range(0, prev_row_count):
-            for col_index in range(0, self.col_count):
-                self._erase_cell(row_index, col_index, allow_outside=True)
-
-
-    def redefine(self):
-        self.workbook._needs_save = True
-        new_ref = self.ref
-        if self._pyxl_table is not None and new_ref == self._pyxl_table.ref:
-            return
-        
-        logger.debug("define table %s: %s => %s", self.name, self._pyxl_table.ref if self._pyxl_table is not None else None, new_ref)
-
-        newcolumns = []
-
-        for i in range(0, self.col_count):
-            if self.has_headers:
-                name = self._pyxl_worksheet.cell(self._min_row - 1, self._min_col + i).value
-            else:
-                name = self.column_names[i] if i < len(self.column_names) else None
-            newcolumn = TableColumn(id=i+1, name=name)
-            newcolumns.append(newcolumn)
-
-            if self._pyxl_table is not None and i < len(self._pyxl_table.tableColumns):
-                prevcolumn: TableColumn = self._pyxl_table.tableColumns[i]
-                newcolumn.dataCellStyle = prevcolumn.dataCellStyle
-                newcolumn.dataDxfId = prevcolumn.dataDxfId # refers to workbook._differential_styles
-                newcolumn.calculatedColumnFormula = prevcolumn.calculatedColumnFormula
-
-
-        newtable_kwargs = {
-            'name': self.name,
-            'displayName': self.name,
-            'ref': new_ref,
-            'tableColumns': newcolumns,
-            'headerRowCount': 1 if self.has_headers else 0,
-        }
-
-        if self._pyxl_table is not None:
-            newtable_kwargs['autoFilter'] = self._pyxl_table.autoFilter
-            newtable_kwargs['sortState'] = self._pyxl_table.sortState
-            newtable_kwargs['tableStyleInfo'] = self._pyxl_table.tableStyleInfo
-        else:
-            newtable_kwargs['autoFilter'] = AutoFilter()
-            newtable_kwargs['tableStyleInfo'] = TableStyleInfo(name="TableStyleMedium2", showFirstColumn=False, showLastColumn=False, showRowStripes=True, showColumnStripes=False)
-
-        newtable = Table(**newtable_kwargs)
-
-        self._pyxl_table = newtable
-        
-        if self.name in self._pyxl_worksheet.tables:
-            del self._pyxl_worksheet.tables[self.name]
-        self._pyxl_worksheet.add_table(newtable)
-
-
-    def get_value(self, row_index: int, col_index: int) -> Any:
-        self._check_indexes(row_index, col_index)
-        cell = self._pyxl_worksheet.cell(self._min_row + row_index, self._min_col + col_index)
-        value = cell.value
-        # TODO: formatting
-        return value
-    
-
-    def set_value(self, row_index: int, col_index: int, value: Any):
-        self._check_indexes(row_index, col_index)
-        self.workbook._needs_save = True
-        cell = self._pyxl_worksheet.cell(self._min_row + row_index, self._min_col + col_index)
-
-        self._apply_column_format(cell)
-
-        try:
-            cell.value = value
-        except ValueError as err:
-            if str(err).startswith('Cannot convert'):
-                cell.value = str(value)
-            else:
-                raise
-        
-
-    def _check_indexes(self, row_index: int, col_index: int):
-        if row_index == -1:
-            row_index = self.row_count - 1
-        if col_index == -1:
-            col_index = self.col_count - 1
-
-        if row_index < 0 or row_index >= self.row_count:
-            raise ValueError(f"invalid row index: {row_index} (row count: {self.row_count})")
-        if col_index < 0 or col_index >= self.col_count:
-            raise ValueError(f"invalid row index: {col_index} (row count: {self.col_count})")
-
-
-    def _erase_cell(self, row_index: int, col_index: int, allow_outside: bool = False) -> Cell:
-        cell = self._pyxl_worksheet.cell(self._min_row + row_index, self._min_col + col_index)
-        cell.style = 'Normal'
-        cell.value = None
-
-        if not allow_outside or (row_index < self._row_count and col_index < self.col_count):
-            self._apply_column_format(cell)
-        
-        return cell
-
-
-    def _apply_column_format(self, cell: Cell):
-        index = cell.col_idx - self.min_col
-        if index in self._column_formats:
-            fmt = self._column_formats[index]
-        else:
-            fmt = self._build_column_format(index)
-            self._column_formats[index] = fmt
-
-        if 'formula' in fmt:
-            formula = fmt['formula']
-            if isinstance(formula, ArrayFormula):
-                pass # TODO: not supported yet
-            else:
-                cell.value = formula
-
-        if 'style' in fmt:
-            cell.style = fmt['style']
-
-        for fmt_key, fmt_value in fmt.items():
-            if fmt_key in ['formula', 'style']:
-                continue
-            setattr(cell, fmt_key, fmt_value)
-
-
-    def _build_column_format(self, index: int):
-        fmt: dict[str,Any] = {}
-
-        if self._pyxl_table is None or index >= len(self._pyxl_table.tableColumns):
-            return fmt
-        
-        column: TableColumn = self._pyxl_table.tableColumns[index]
-
-        # Read dataCellStyle
-        if column.dataCellStyle:
-            fmt['style'] = column.dataCellStyle
-        
-        # Read dxf
-        if column.dataDxfId is not None:
-            dxf: DifferentialStyle = self.workbook._pyxl_workbook._differential_styles[column.dataDxfId]
-
-            if dxf.numFmt:
-                fmt['number_format'] = dxf.numFmt.formatCode
-            else:
-                if not 'style' in fmt:
-                    fmt['number_format'] = self._DEFAULT_NUMBER_FORMAT
-
-            fmt['alignment'] = dxf.alignment if dxf.alignment else self._DEFAULT_ALIGNMENT
-            fmt['border'] = dxf.border if dxf.border else self._DEFAULT_BORDER
-            fmt['font'] = dxf.font if dxf.font else self._DEFAULT_FONT
-            fmt['protection'] = dxf.protection if dxf.protection else self._DEFAULT_PROTECTION
-            fmt['fill'] = PatternFill(fill_type=dxf.fill.fill_type, bgColor=dxf.fill.fgColor, fgColor=dxf.fill.bgColor) if dxf.fill else self._DEFAULT_FILL # NOTE: fgcolor and bgcolor are inversed in DifferentialStyle
-
-        # Read formula
-        if column.calculatedColumnFormula:
-            formula = column.calculatedColumnFormula
-            if formula.array:
-                fmt['formula'] = ArrayFormula(formula.attr_text)
-            else:
-                fmt['formula'] = '=' + formula.attr_text
-        
-        # Return built format
-        return fmt
-    
-
-    _DEFAULT_NUMBER_FORMAT = 'General'
-    _DEFAULT_FILL = PatternFill(fill_type=None)
-    _DEFAULT_ALIGNMENT = None # openpyxl.styles.alignment.Alignment
-    _DEFAULT_BORDER = None # openpyxl.styles.alignment.Border
-    _DEFAULT_FONT = None # openpyxl.styles.fonts.Font
-    _DEFAULT_PROTECTION = None # openpyxl.styles.protection.Protection
-
-
-    def __len__(self):
-        return self._row_count
-
-    def __iter__(self):
-        return self.Iterator(self)
-
-    class Iterator:
-        def __init__(self, table: ExcelTable):
-            self.next_index = 0
-            self.table = table
-
-        def __next__(self):
-            if self.next_index >= self.table.row_count:
-                raise StopIteration()
-            
-            row = self.table.get_row(self.next_index)
-            self.next_index += 1
-            return row
+from __future__ import annotations
+import logging
+from pathlib import Path
+from typing import Any
+from .tabular import Row
+from .misc import Literal
+from . import filesh
+from openpyxl import load_workbook, Workbook, DEFUSEDXML
+from openpyxl.worksheet.worksheet import Worksheet
+from openpyxl.worksheet.table import Table, TableColumn, TableFormula, TableStyleInfo
+from openpyxl.worksheet.formula import DataTableFormula, ArrayFormula
+from openpyxl.worksheet.filters import AutoFilter
+from openpyxl.cell.cell import Cell
+from openpyxl.styles.differential import DifferentialStyle, DifferentialStyleList
+from openpyxl.styles.fills import PatternFill
+from openpyxl.utils import range_boundaries, get_column_letter
+
+logger = logging.getLogger(__name__)
+
+
+class ExcelWorkbook:
+    _workbook_cache: dict[str,ExcelWorkbook] = {}
+    """ Workbooks per canonical path """
+    
+    _defusedxml_alert_emitted = False
+
+    def __init__(self, path: str|Path):        
+        if not DEFUSEDXML and not self.__class__._defusedxml_alert_emitted:
+            logger.warning("By default openpyxl does not guard against quadratic blowup or billion laughs xml attacks. To guard against these attacks install defusedxml.")
+            self.__class__._defusedxml_alert_emitted = True
+
+        self.path = Path(path) if not isinstance(path, Path) else path
+        
+        if filesh.exists(self.path):
+            logger.debug("load workbook %s", self.path)
+            with filesh.open_file(self.path, 'rb') as fp:
+                self._pyxl_workbook: Workbook = load_workbook(fp)
+            self._create_next_table_in_active_sheet = False
+        else:
+            logger.debug("create workbook for %s", self.path)
+            self._pyxl_workbook = Workbook()
+            self._create_next_table_in_active_sheet = True
+
+        self._tables: dict[str,ExcelTable] = {}
+        self._needs_save = False
+
+
+    @classmethod
+    def get_or_create_cached(cls, path: str|Path) -> ExcelWorkbook:
+        path = Path(path) if not isinstance(path, Path) else path
+        canonical_path = str(path.resolve())
+        if not canonical_path in cls._workbook_cache:
+            cls._workbook_cache[canonical_path] = ExcelWorkbook(path)
+        return cls._workbook_cache[canonical_path]
+
+
+    def close(self):
+        if not self._needs_save:
+            return
+        
+        for table in self._tables.values():
+            table.redefine()
+        
+        logger.debug("save workbook %s", self.path)
+        with filesh.open_file(self.path, 'wb') as fp:
+            self._pyxl_workbook.save(fp)
+
+        self._needs_save = False
+        
+
+    def get_table(self, name: str, default: Any|Literal['__raise__'] = '__raise__') -> ExcelTable:
+        if name in self._tables:
+            return self._tables[name]
+        
+        for sheet_name in self._pyxl_workbook.sheetnames:
+            pyxl_worksheet: Worksheet = self._pyxl_workbook[sheet_name]
+            if name in pyxl_worksheet.tables:
+                pyxl_table = pyxl_worksheet.tables[name]
+                self._tables[name] = ExcelTable(pyxl_table, pyxl_worksheet, self)
+                return self._tables[name]
+            
+        if default == '__raise__':
+            raise KeyError(f"no table found with name \"{name}\" in workbook \"{self.path}\"")
+        else:
+            return default
+        
+
+    def create_table(self, name: str, no_headers: bool = False) -> ExcelTable:
+        for sheet_name in self._pyxl_workbook.sheetnames:
+            pyxl_worksheet: Worksheet = self._pyxl_workbook[sheet_name]
+            if name in pyxl_worksheet.tables:
+                raise ValueError(f"table {name} already exist")
+    
+        self._needs_save = True
+        
+        if self._create_next_table_in_active_sheet:
+            pyxl_worksheet: Worksheet = self._pyxl_workbook.active
+            pyxl_worksheet.title = name
+            self._create_next_table_in_active_sheet = False
+        else:
+            pyxl_worksheet: Worksheet = self._pyxl_workbook.create_sheet(title=name)
+
+        self._tables[name] = ExcelTable(name, pyxl_worksheet, self)
+        return self._tables[name]
+
+
+class ExcelTable:
+    def __init__(self, pyxl_table: Table|str, pyxl_worksheet: Worksheet, workbook: ExcelWorkbook, no_headers: bool = None):
+        self.workbook = workbook
+        self._pyxl_worksheet = pyxl_worksheet        
+        self._column_formats: dict[int,dict[str,Any]] = {}
+
+        if isinstance(pyxl_table, str):
+            self._pyxl_table: Table = None
+            self.name: str = pyxl_table
+            if no_headers:
+                self.has_headers = False
+                self._min_row = 1
+            else:
+                self.has_headers = True
+                self._min_row = 2
+            self._min_col = 1
+            self._row_count = 0
+            self._column_names: list[str] = []
+
+        elif isinstance(pyxl_table, Table):
+            self._pyxl_table: Table = pyxl_table
+            self.name: str = pyxl_table.name
+            
+            self._min_col, min_row, max_col, max_row = range_boundaries(pyxl_table.ref)
+
+            if pyxl_table.headerRowCount == 0:
+                self.has_headers = False
+                self._min_row = min_row
+            elif pyxl_table.headerRowCount == 1:
+                self.has_headers = True
+                self._min_row = min_row + 1
+            else:
+                raise ValueError(f'invalid headerRowCount: {pyxl_table.headerRowCount}')
+            
+            self._row_count = max_row - self._min_row + 1
+            col_count = max_col - self._min_col + 1
+        
+            self._column_names: list[str] = list(self._pyxl_table.column_names)
+            # NOTE: if no headers, default names are returned, example in French: ['Colonne1', 'Colonne2']
+            if len(self._column_names) != col_count:
+                raise ValueError(f'invalid column_names length ({len(self._column_names)}, expected {col_count}): {self._column_names}')
+        else:
+            raise ValueError(f"invalid type for pyxl_table: {type(pyxl_table).__name__}")
+
+
+    @property
+    def column_names(self) -> list[str]:
+        return self._column_names
+
+    @property
+    def min_row(self) -> int:
+        return self._min_row
+    
+    @property
+    def min_col(self) -> int:
+        return self._min_col
+    
+    @property
+    def row_count(self) -> int:
+        return self._row_count
+    
+    @property
+    def col_count(self) -> int:
+        return len(self._column_names)
+    
+    @property
+    def ref(self) -> str:
+        if self.col_count == 0:
+            raise ValueError(f"cannot get table ref: table does not contain any column")
+        if self.row_count == 0 and not self.has_headers:
+            raise ValueError(f"cannot get table ref: table does not contain any line")
+        
+        min_row = self._min_row
+        if self.has_headers:
+            min_row -= 1
+        
+        return f"{get_column_letter(self._min_col)}{min_row}:{get_column_letter(self._min_col + self.col_count - 1)}{self._min_row + self._row_count - 1}"
+
+
+    def get_row(self, index: int) -> Row:
+        if index == -1:
+            if self.row_count == 0:
+                raise ValueError(f"cannot get last row: table does not contain any row")
+            index = self.row_count - 1
+        elif index < 0:
+            raise ValueError(f"invalid row index: {index}")
+        
+        if index >= self.row_count:
+            raise ValueError(f"cannot get row at index {index}: table contains {self.row_count} rows")
+
+        return Row(
+            get=lambda col_index: self.get_value(index, col_index),
+            set=lambda col_index, value: self.set_value(index, col_index, value),
+            headers=self.column_names
+        )
+    
+
+    def insert_row(self) -> Row:
+        self.workbook._needs_save = True
+        row_index = self._row_count
+
+        # ensure empty values
+        for col_index in range(0, self.col_count):
+            cell = self._pyxl_worksheet.cell(self._min_row + row_index, self._min_col + col_index)
+            if cell.value is not None:
+                raise ValueError(f'cannot insert row: row {row_index} is not empty')
+
+        self._row_count += 1
+
+        # erase old styles and apply column format
+        for col_index in range(0, self.col_count):
+            self._erase_cell(row_index, col_index)
+            
+        return self.get_row(row_index)
+    
+
+    def insert_col(self, name: str):
+        self.workbook._needs_save = True
+        if not name:
+            raise ValueError(f'name cannot be empty')
+        if name in self.column_names:
+            raise ValueError(f'column name already used: {name}')
+            
+        col_index = self.col_count
+
+        # ensure empty values
+        for row_index in range(0, self._row_count):
+            cell = self._pyxl_worksheet.cell(self._min_row + row_index, self._min_col + col_index)
+            if cell.value is not None:
+                raise ValueError(f'cannot insert column: column {col_index} is not empty')
+
+        self.column_names.append(name) # implies self.col_count += 1
+
+        if self.has_headers:
+            cell = self._pyxl_worksheet.cell(self._min_row - 1, self._min_col + col_index)
+            cell.value = name
+
+        # erase old styles and apply column format
+        for row_index in range(0, self._row_count):
+            self._erase_cell(row_index, col_index)
+            
+
+    def truncate(self):
+        self.workbook._needs_save = True
+        prev_row_count = self._row_count
+        self._row_count = 0
+
+        for row_index in range(0, prev_row_count):
+            for col_index in range(0, self.col_count):
+                self._erase_cell(row_index, col_index, allow_outside=True)
+
+
+    def redefine(self):
+        self.workbook._needs_save = True
+        new_ref = self.ref
+        if self._pyxl_table is not None and new_ref == self._pyxl_table.ref:
+            return
+        
+        logger.debug("define table %s: %s => %s", self.name, self._pyxl_table.ref if self._pyxl_table is not None else None, new_ref)
+
+        newcolumns = []
+
+        for i in range(0, self.col_count):
+            if self.has_headers:
+                name = self._pyxl_worksheet.cell(self._min_row - 1, self._min_col + i).value
+            else:
+                name = self.column_names[i] if i < len(self.column_names) else None
+            newcolumn = TableColumn(id=i+1, name=name)
+            newcolumns.append(newcolumn)
+
+            if self._pyxl_table is not None and i < len(self._pyxl_table.tableColumns):
+                prevcolumn: TableColumn = self._pyxl_table.tableColumns[i]
+                newcolumn.dataCellStyle = prevcolumn.dataCellStyle
+                newcolumn.dataDxfId = prevcolumn.dataDxfId # refers to workbook._differential_styles
+                newcolumn.calculatedColumnFormula = prevcolumn.calculatedColumnFormula
+
+
+        newtable_kwargs = {
+            'name': self.name,
+            'displayName': self.name,
+            'ref': new_ref,
+            'tableColumns': newcolumns,
+            'headerRowCount': 1 if self.has_headers else 0,
+        }
+
+        if self._pyxl_table is not None:
+            newtable_kwargs['autoFilter'] = self._pyxl_table.autoFilter
+            newtable_kwargs['sortState'] = self._pyxl_table.sortState
+            newtable_kwargs['tableStyleInfo'] = self._pyxl_table.tableStyleInfo
+        else:
+            newtable_kwargs['autoFilter'] = AutoFilter()
+            newtable_kwargs['tableStyleInfo'] = TableStyleInfo(name="TableStyleMedium2", showFirstColumn=False, showLastColumn=False, showRowStripes=True, showColumnStripes=False)
+
+        newtable = Table(**newtable_kwargs)
+
+        self._pyxl_table = newtable
+        
+        if self.name in self._pyxl_worksheet.tables:
+            del self._pyxl_worksheet.tables[self.name]
+        self._pyxl_worksheet.add_table(newtable)
+
+
+    def get_value(self, row_index: int, col_index: int) -> Any:
+        self._check_indexes(row_index, col_index)
+        cell = self._pyxl_worksheet.cell(self._min_row + row_index, self._min_col + col_index)
+        value = cell.value
+        # TODO: formatting
+        return value
+    
+
+    def set_value(self, row_index: int, col_index: int, value: Any):
+        self._check_indexes(row_index, col_index)
+        self.workbook._needs_save = True
+        cell = self._pyxl_worksheet.cell(self._min_row + row_index, self._min_col + col_index)
+
+        self._apply_column_format(cell)
+
+        try:
+            cell.value = value
+        except ValueError as err:
+            if str(err).startswith('Cannot convert'):
+                cell.value = str(value)
+            else:
+                raise
+        
+
+    def _check_indexes(self, row_index: int, col_index: int):
+        if row_index == -1:
+            row_index = self.row_count - 1
+        if col_index == -1:
+            col_index = self.col_count - 1
+
+        if row_index < 0 or row_index >= self.row_count:
+            raise ValueError(f"invalid row index: {row_index} (row count: {self.row_count})")
+        if col_index < 0 or col_index >= self.col_count:
+            raise ValueError(f"invalid row index: {col_index} (row count: {self.col_count})")
+
+
+    def _erase_cell(self, row_index: int, col_index: int, allow_outside: bool = False) -> Cell:
+        cell = self._pyxl_worksheet.cell(self._min_row + row_index, self._min_col + col_index)
+        cell.style = 'Normal'
+        cell.value = None
+
+        if not allow_outside or (row_index < self._row_count and col_index < self.col_count):
+            self._apply_column_format(cell)
+        
+        return cell
+
+
+    def _apply_column_format(self, cell: Cell):
+        index = cell.col_idx - self.min_col
+        if index in self._column_formats:
+            fmt = self._column_formats[index]
+        else:
+            fmt = self._build_column_format(index)
+            self._column_formats[index] = fmt
+
+        if 'formula' in fmt:
+            formula = fmt['formula']
+            if isinstance(formula, ArrayFormula):
+                pass # TODO: not supported yet
+            else:
+                cell.value = formula
+
+        if 'style' in fmt:
+            cell.style = fmt['style']
+
+        for fmt_key, fmt_value in fmt.items():
+            if fmt_key in ['formula', 'style']:
+                continue
+            setattr(cell, fmt_key, fmt_value)
+
+
+    def _build_column_format(self, index: int):
+        fmt: dict[str,Any] = {}
+
+        if self._pyxl_table is None or index >= len(self._pyxl_table.tableColumns):
+            return fmt
+        
+        column: TableColumn = self._pyxl_table.tableColumns[index]
+
+        # Read dataCellStyle
+        if column.dataCellStyle:
+            fmt['style'] = column.dataCellStyle
+        
+        # Read dxf
+        if column.dataDxfId is not None:
+            dxf: DifferentialStyle = self.workbook._pyxl_workbook._differential_styles[column.dataDxfId]
+
+            if dxf.numFmt:
+                fmt['number_format'] = dxf.numFmt.formatCode
+            else:
+                if not 'style' in fmt:
+                    fmt['number_format'] = self._DEFAULT_NUMBER_FORMAT
+
+            fmt['alignment'] = dxf.alignment if dxf.alignment else self._DEFAULT_ALIGNMENT
+            fmt['border'] = dxf.border if dxf.border else self._DEFAULT_BORDER
+            fmt['font'] = dxf.font if dxf.font else self._DEFAULT_FONT
+            fmt['protection'] = dxf.protection if dxf.protection else self._DEFAULT_PROTECTION
+            fmt['fill'] = PatternFill(fill_type=dxf.fill.fill_type, bgColor=dxf.fill.fgColor, fgColor=dxf.fill.bgColor) if dxf.fill else self._DEFAULT_FILL # NOTE: fgcolor and bgcolor are inversed in DifferentialStyle
+
+        # Read formula
+        if column.calculatedColumnFormula:
+            formula = column.calculatedColumnFormula
+            if formula.array:
+                fmt['formula'] = ArrayFormula(formula.attr_text)
+            else:
+                fmt['formula'] = '=' + formula.attr_text
+        
+        # Return built format
+        return fmt
+    
+
+    _DEFAULT_NUMBER_FORMAT = 'General'
+    _DEFAULT_FILL = PatternFill(fill_type=None)
+    _DEFAULT_ALIGNMENT = None # openpyxl.styles.alignment.Alignment
+    _DEFAULT_BORDER = None # openpyxl.styles.alignment.Border
+    _DEFAULT_FONT = None # openpyxl.styles.fonts.Font
+    _DEFAULT_PROTECTION = None # openpyxl.styles.protection.Protection
+
+
+    def __len__(self):
+        return self._row_count
+
+    def __iter__(self):
+        return self.Iterator(self)
+
+    class Iterator:
+        def __init__(self, table: ExcelTable):
+            self.next_index = 0
+            self.table = table
+
+        def __next__(self):
+            if self.next_index >= self.table.row_count:
+                raise StopIteration()
+            
+            row = self.table.get_row(self.next_index)
+            self.next_index += 1
+            return row
```

## zut/git.py

 * *Ordering differences only*

```diff
@@ -1,82 +1,82 @@
-from __future__ import annotations
-import subprocess, re, logging
-from .colors import Colors
-
-logger = logging.getLogger(__name__)
-
-
-def get_git_tags(points_at: str = None, pattern: str = None) -> list[str]:
-    """
-    Get list of defined tags.
-    Use points_at="HEAD" for tags pointing on last commit.
-    """
-    cmd = ['git', 'tag', '--list']
-    if points_at:
-        cmd.append("--points-at")
-        cmd.append(points_at)
-    if pattern:
-        cmd.append(pattern)
-
-    cp = subprocess.run(cmd, check=True, text=True, capture_output=True)
-    tags = cp.stdout.strip()
-    if not tags:
-        return []
-    
-    return tags.splitlines()
-
-
-def get_git_hash(ref: str = "HEAD") -> None|str:
-    """
-    Get commit hash of a reference (branch, tag, etc).
-    Use ref="HEAD" for last commit and ref=None for last commit only if there is no changes.
-    """
-    try:
-        cp = subprocess.run(['git', 'rev-list', '-n', '1', ref], check=True, text=True, capture_output=True)
-    except subprocess.CalledProcessError:
-        if cp.returncode == 128:
-            return None
-
-    return cp.stdout.strip()
-
-
-def git_has_changes():
-    """
-    Indicate whether working directory has changes since last commit.
-    """
-    cp = subprocess.run(['git', 'status', '--porcelain'], check=True, text=True, capture_output=True)
-    return cp.stdout != ""
-
-
-def check_git_version_tag(version: str) -> bool:
-    if not isinstance(version, str):
-        raise ValueError(f"invalid type for argument \"version\": {type(version)}")
-    
-    ok = True
-
-    # Check version
-    if not re.match(r"^\d+\.\d+\.\d+(?:\-[a-z0-9\-]+)?$", version):
-        logger.error(f"version \"{version}\" does not match required regex")
-        ok = False
-
-    # Compare version with git tags
-    tags = get_git_tags(pattern="v*")
-    if not tags or not f"v{version}" in tags:
-        logger.warning(f"tag v{version} not found")
-        ok = False
-    
-    else:
-        # Ensure corresponding version tag matches current hash
-        tag_hash = get_git_hash(f"v{version}")
-        
-        if tag_hash:
-            head_hash = get_git_hash()
-            
-            if tag_hash != head_hash:
-                logger.error(f"tag v{version} points at {Colors.BLUE}{tag_hash}{Colors.RESET}, head points at {Colors.BLUE}{head_hash}{Colors.RESET}")
-                ok = False
-            
-            elif git_has_changes():
-                logger.error(f"git has changes since HEAD (tag v{version})")
-                ok = False
-
-    return ok
+from __future__ import annotations
+import subprocess, re, logging
+from .colors import Colors
+
+logger = logging.getLogger(__name__)
+
+
+def get_git_tags(points_at: str = None, pattern: str = None) -> list[str]:
+    """
+    Get list of defined tags.
+    Use points_at="HEAD" for tags pointing on last commit.
+    """
+    cmd = ['git', 'tag', '--list']
+    if points_at:
+        cmd.append("--points-at")
+        cmd.append(points_at)
+    if pattern:
+        cmd.append(pattern)
+
+    cp = subprocess.run(cmd, check=True, text=True, capture_output=True)
+    tags = cp.stdout.strip()
+    if not tags:
+        return []
+    
+    return tags.splitlines()
+
+
+def get_git_hash(ref: str = "HEAD") -> None|str:
+    """
+    Get commit hash of a reference (branch, tag, etc).
+    Use ref="HEAD" for last commit and ref=None for last commit only if there is no changes.
+    """
+    try:
+        cp = subprocess.run(['git', 'rev-list', '-n', '1', ref], check=True, text=True, capture_output=True)
+    except subprocess.CalledProcessError:
+        if cp.returncode == 128:
+            return None
+
+    return cp.stdout.strip()
+
+
+def git_has_changes():
+    """
+    Indicate whether working directory has changes since last commit.
+    """
+    cp = subprocess.run(['git', 'status', '--porcelain'], check=True, text=True, capture_output=True)
+    return cp.stdout != ""
+
+
+def check_git_version_tag(version: str) -> bool:
+    if not isinstance(version, str):
+        raise ValueError(f"invalid type for argument \"version\": {type(version)}")
+    
+    ok = True
+
+    # Check version
+    if not re.match(r"^\d+\.\d+\.\d+(?:\-[a-z0-9\-]+)?$", version):
+        logger.error(f"version \"{version}\" does not match required regex")
+        ok = False
+
+    # Compare version with git tags
+    tags = get_git_tags(pattern="v*")
+    if not tags or not f"v{version}" in tags:
+        logger.warning(f"tag v{version} not found")
+        ok = False
+    
+    else:
+        # Ensure corresponding version tag matches current hash
+        tag_hash = get_git_hash(f"v{version}")
+        
+        if tag_hash:
+            head_hash = get_git_hash()
+            
+            if tag_hash != head_hash:
+                logger.error(f"tag v{version} points at {Colors.BLUE}{tag_hash}{Colors.RESET}, head points at {Colors.BLUE}{head_hash}{Colors.RESET}")
+                ok = False
+            
+            elif git_has_changes():
+                logger.error(f"git has changes since HEAD (tag v{version})")
+                ok = False
+
+    return ok
```

## zut/gpg.py

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-from __future__ import annotations
-import subprocess, logging
-from tempfile import TemporaryDirectory
-from pathlib import Path
-from .misc import check_completed_subprocess
-from .network import get_configured_proxy_url
-
-logger = logging.getLogger(__name__)
-
-
-def download_gpg_key(keyid: str, target_path: Path, keyserver: str = None, include_proxy_password: bool = False):
-    with TemporaryDirectory() as tmpdir:
-        # Retrieve the key
-        cmd = ["gpg", "--homedir", tmpdir]
-        
-        if keyserver:
-            cmd += ["--keyserver", keyserver]
-            
-            if keyserver.startswith("hkp://"):
-                proxy_url = get_configured_proxy_url(keyserver, include_password=include_proxy_password)
-                if proxy_url:
-                    cmd += ["--keyserver-options", f"http-proxy={proxy_url}"]
-
-        cmd += ["--recv-keys", keyid]
-        subprocess.run(cmd, capture_output=True, text=True, check=True)
-
-        # Export the key
-        cmd = ["gpg", "--homedir", tmpdir, "--output", target_path, "--export", keyid]
-        subprocess.run(cmd, capture_output=True, text=True, check=True)
-
-
-def verify_gpg_signature(sign_path: Path, public_key_path: Path):
-    cmd = ["gpg", "--no-default-keyring", "--keyring", public_key_path, "--verify", sign_path]
-    cp = subprocess.run(cmd, capture_output=True, text=True)
-    
-    if cp.returncode != 0:
-        check_completed_subprocess(cp, logger, label='gpg verify')
-        return False
-    
-    return True
+from __future__ import annotations
+import subprocess, logging
+from tempfile import TemporaryDirectory
+from pathlib import Path
+from .misc import check_completed_subprocess
+from .network import get_configured_proxy_url
+
+logger = logging.getLogger(__name__)
+
+
+def download_gpg_key(keyid: str, target_path: Path, keyserver: str = None, include_proxy_password: bool = False):
+    with TemporaryDirectory() as tmpdir:
+        # Retrieve the key
+        cmd = ["gpg", "--homedir", tmpdir]
+        
+        if keyserver:
+            cmd += ["--keyserver", keyserver]
+            
+            if keyserver.startswith("hkp://"):
+                proxy_url = get_configured_proxy_url(keyserver, include_password=include_proxy_password)
+                if proxy_url:
+                    cmd += ["--keyserver-options", f"http-proxy={proxy_url}"]
+
+        cmd += ["--recv-keys", keyid]
+        subprocess.run(cmd, capture_output=True, text=True, check=True)
+
+        # Export the key
+        cmd = ["gpg", "--homedir", tmpdir, "--output", target_path, "--export", keyid]
+        subprocess.run(cmd, capture_output=True, text=True, check=True)
+
+
+def verify_gpg_signature(sign_path: Path, public_key_path: Path):
+    cmd = ["gpg", "--no-default-keyring", "--keyring", public_key_path, "--verify", sign_path]
+    cp = subprocess.run(cmd, capture_output=True, text=True)
+    
+    if cp.returncode != 0:
+        check_completed_subprocess(cp, logger, label='gpg verify')
+        return False
+    
+    return True
```

## zut/json.py

 * *Ordering differences only*

```diff
@@ -1,173 +1,173 @@
-from __future__ import annotations
-import json
-import re
-from datetime import datetime, time, date, timedelta
-from decimal import Decimal
-from uuid import UUID
-from .datetime import is_aware, duration_iso_string
-
-try:
-    from django.utils.functional import Promise
-except ImportError:
-    Promise = None
-
-
-class ExtendedJSONEncoder(json.JSONEncoder):
-    """
-    Adapted from: django.core.serializers.json.DjangoJSONEncoder
-    Usage example: json.dumps(data, indent=2, cls=ExtendedJSONEncoder)
-    """
-    def __init__(self, **kwargs):
-        super().__init__(**kwargs)
-
-    def default(self, o):
-        if isinstance(o, datetime):
-            r = o.isoformat()
-            if o.microsecond and o.microsecond % 1000 == 0:
-                r = r[:23] + r[26:]
-            if r.endswith("+00:00"):
-                r = r[:-6] + "Z"
-            return r
-        elif isinstance(o, date):
-            return o.isoformat()
-        elif isinstance(o, time):
-            if is_aware(o):
-                raise ValueError("JSON can't represent timezone-aware times.")
-            r = o.isoformat()
-            if o.microsecond and o.microsecond % 1000 == 0:
-                r = r[:12]
-            return f'T{r}'
-        elif isinstance(o, timedelta):
-            return duration_iso_string(o)
-        elif isinstance(o, (Decimal, UUID)):
-            return str(o)
-        elif Promise is not None and isinstance(o, Promise):
-            return str(o)
-        else:
-            return super().default(o)
-
-
-class ExtendedJSONDecoder(json.JSONDecoder):
-    """
-    Reverse of: ExtendedJSONEncoder.
-    Usage example: json.loads(data, cls=ExtendedJSONDecoder)
-    """
-    def __init__(self, **kwargs):
-        if not 'object_hook' in kwargs:
-            kwargs['object_hook'] = extended_json_decode_hook
-        super().__init__(**kwargs)
-
-
-def extended_json_decode_hook(obj):
-    """
-    Decode date and datetime objects with ISO format.
-    """
-    if isinstance(obj, dict):
-        for key, value in obj.items():
-            obj[key] = extended_json_decode_hook(value)
-
-    elif isinstance(obj, list):
-        for i, value in enumerate(obj):
-            obj[i] = extended_json_decode_hook(value)
-
-    elif isinstance(obj, str):
-        if len(obj) < 10:
-            return obj # shortcut
-        
-        value = _decode_date(obj)
-        if value:
-            return value
-        
-        value = _decode_datetime(obj)
-        if value:
-            return value
-        
-        value = _decode_timedelta(obj)
-        if value:
-            return value
-        
-    return obj
-
-
-def _decode_datetime(value: str) -> datetime|time|None:
-    m = re.match(r'^(\d{4}-\d{2}-\d{2})?(T\d{2}:\d{2}:\d{2})(\.\d{3,6})?(Z|[\+\-]\d{2}:\d{2})?$', value)
-    if not m:
-        return None
-    
-    datepart = m.group(1) or '' # optional
-    timepart = m.group(2) # mandatory
-    microsecondpart = m.group(3) or '' # optional
-    tz = m.group(4) or '' # optional
-
-    format_string = ''
-
-    if datepart:
-        format_string += '%Y-%m-%d'
-    
-    format_string += 'T%H:%M:%S'
-    
-    if microsecondpart:
-        format_string += '.%f'
-    
-    if tz:
-        if not datepart:
-            # invalid (timezone-aware time without date)
-            return None
-        
-        format_string += '%z'
-        # adapt timezone: replace 'Z' with +0000, or +XX:YY with +XXYY
-        if tz == 'Z':
-            tz = '+0000'
-        else:
-            tz = tz[:-3] + tz[-2:]
-    
-    try:
-        result = datetime.strptime(f"{datepart}{timepart}{microsecondpart}{tz}", format_string)
-    except ValueError: # example: invalid month, day, hour, etc
-        return None
-
-    if not datepart:
-        result = result.time()
-    return result
-
-
-def _decode_timedelta(value: str) -> timedelta|None:
-    m = re.match(r'^(\-)?P(\d+)DT(\d{2})H(\d{2})M(\d{2})(?:\.(\d{3,6}))?S$', value)
-    if not m:
-        return None
-    
-    sign = m.group(1) or ''
-    days = int(m.group(2))
-    hours = int(m.group(3))
-    minutes = int(m.group(4))
-    seconds = int(m.group(5))
-
-    if m.group(6):
-        microseconds = int(m.group(6).ljust(6, '0'))
-    else:
-        microseconds = 0
-
-    if sign == '-':
-        days *= -1
-        hours *= -1
-        minutes *= -1
-        seconds *= -1
-        microseconds *= -1
-
-    try:
-        return timedelta(days=days, hours=hours, minutes=minutes, seconds=seconds, microseconds=microseconds)
-    except ValueError: # example: invalid month, day, hour, etc
-        return None
-
-
-def _decode_date(value: str) -> date|None:
-    m = re.match(r'^\d{4}-\d{2}-\d{2}$', value)
-    if not m:
-        return None
-
-    try:
-        result = datetime.strptime(value, "%Y-%m-%d")
-    except ValueError: # example: invalid month or day
-        return None
-    
-    return result.date()
+from __future__ import annotations
+import json
+import re
+from datetime import datetime, time, date, timedelta
+from decimal import Decimal
+from uuid import UUID
+from .datetime import is_aware, duration_iso_string
+
+try:
+    from django.utils.functional import Promise
+except ImportError:
+    Promise = None
+
+
+class ExtendedJSONEncoder(json.JSONEncoder):
+    """
+    Adapted from: django.core.serializers.json.DjangoJSONEncoder
+    Usage example: json.dumps(data, indent=2, cls=ExtendedJSONEncoder)
+    """
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+
+    def default(self, o):
+        if isinstance(o, datetime):
+            r = o.isoformat()
+            if o.microsecond and o.microsecond % 1000 == 0:
+                r = r[:23] + r[26:]
+            if r.endswith("+00:00"):
+                r = r[:-6] + "Z"
+            return r
+        elif isinstance(o, date):
+            return o.isoformat()
+        elif isinstance(o, time):
+            if is_aware(o):
+                raise ValueError("JSON can't represent timezone-aware times.")
+            r = o.isoformat()
+            if o.microsecond and o.microsecond % 1000 == 0:
+                r = r[:12]
+            return f'T{r}'
+        elif isinstance(o, timedelta):
+            return duration_iso_string(o)
+        elif isinstance(o, (Decimal, UUID)):
+            return str(o)
+        elif Promise is not None and isinstance(o, Promise):
+            return str(o)
+        else:
+            return super().default(o)
+
+
+class ExtendedJSONDecoder(json.JSONDecoder):
+    """
+    Reverse of: ExtendedJSONEncoder.
+    Usage example: json.loads(data, cls=ExtendedJSONDecoder)
+    """
+    def __init__(self, **kwargs):
+        if not 'object_hook' in kwargs:
+            kwargs['object_hook'] = extended_json_decode_hook
+        super().__init__(**kwargs)
+
+
+def extended_json_decode_hook(obj):
+    """
+    Decode date and datetime objects with ISO format.
+    """
+    if isinstance(obj, dict):
+        for key, value in obj.items():
+            obj[key] = extended_json_decode_hook(value)
+
+    elif isinstance(obj, list):
+        for i, value in enumerate(obj):
+            obj[i] = extended_json_decode_hook(value)
+
+    elif isinstance(obj, str):
+        if len(obj) < 10:
+            return obj # shortcut
+        
+        value = _decode_date(obj)
+        if value:
+            return value
+        
+        value = _decode_datetime(obj)
+        if value:
+            return value
+        
+        value = _decode_timedelta(obj)
+        if value:
+            return value
+        
+    return obj
+
+
+def _decode_datetime(value: str) -> datetime|time|None:
+    m = re.match(r'^(\d{4}-\d{2}-\d{2})?(T\d{2}:\d{2}:\d{2})(\.\d{3,6})?(Z|[\+\-]\d{2}:\d{2})?$', value)
+    if not m:
+        return None
+    
+    datepart = m.group(1) or '' # optional
+    timepart = m.group(2) # mandatory
+    microsecondpart = m.group(3) or '' # optional
+    tz = m.group(4) or '' # optional
+
+    format_string = ''
+
+    if datepart:
+        format_string += '%Y-%m-%d'
+    
+    format_string += 'T%H:%M:%S'
+    
+    if microsecondpart:
+        format_string += '.%f'
+    
+    if tz:
+        if not datepart:
+            # invalid (timezone-aware time without date)
+            return None
+        
+        format_string += '%z'
+        # adapt timezone: replace 'Z' with +0000, or +XX:YY with +XXYY
+        if tz == 'Z':
+            tz = '+0000'
+        else:
+            tz = tz[:-3] + tz[-2:]
+    
+    try:
+        result = datetime.strptime(f"{datepart}{timepart}{microsecondpart}{tz}", format_string)
+    except ValueError: # example: invalid month, day, hour, etc
+        return None
+
+    if not datepart:
+        result = result.time()
+    return result
+
+
+def _decode_timedelta(value: str) -> timedelta|None:
+    m = re.match(r'^(\-)?P(\d+)DT(\d{2})H(\d{2})M(\d{2})(?:\.(\d{3,6}))?S$', value)
+    if not m:
+        return None
+    
+    sign = m.group(1) or ''
+    days = int(m.group(2))
+    hours = int(m.group(3))
+    minutes = int(m.group(4))
+    seconds = int(m.group(5))
+
+    if m.group(6):
+        microseconds = int(m.group(6).ljust(6, '0'))
+    else:
+        microseconds = 0
+
+    if sign == '-':
+        days *= -1
+        hours *= -1
+        minutes *= -1
+        seconds *= -1
+        microseconds *= -1
+
+    try:
+        return timedelta(days=days, hours=hours, minutes=minutes, seconds=seconds, microseconds=microseconds)
+    except ValueError: # example: invalid month, day, hour, etc
+        return None
+
+
+def _decode_date(value: str) -> date|None:
+    m = re.match(r'^\d{4}-\d{2}-\d{2}$', value)
+    if not m:
+        return None
+
+    try:
+        result = datetime.strptime(value, "%Y-%m-%d")
+    except ValueError: # example: invalid month or day
+        return None
+    
+    return result.date()
```

## zut/logging.py

 * *Ordering differences only*

```diff
@@ -1,129 +1,129 @@
-from __future__ import annotations
-import logging, logging.config, atexit, os
-from .colors import Colors
-
-
-def configure_logging(level: int|str = None, nocount: bool = False, config: dict = None):
-    merged = dict(DEFAULT_LOGGING_DICTCONFIG)
-
-    # Merge given config with default config
-    if config:
-        for prop, data in config.items():
-            if isinstance(data, dict) and prop in merged and isinstance(merged[prop], dict):
-                for key, value in data.items():
-                    if value is None:
-                        merged[prop].pop(key, None)
-                    else:
-                        merged[prop][key] = value
-            else:
-                merged[prop] = data
-
-    # Set root level if missing or if explicitely provided     
-    if level is not None or ('root' in merged and not 'level' in merged['root']):
-        if level is None:
-            level = os.environ.get('LOG_LEVEL', '').upper() or 'INFO'
-        
-        if isinstance(level, int):
-            level = logging.getLevelName(level)
-
-        merged['root']['level'] = level
-
-    # Remove count handler
-    if nocount:
-        if 'root' in merged and 'handlers' in merged['root']:
-            merged['root']['handlers'].pop('count', None)
-
-        if 'loggers' in merged:
-            for _, loggerconfig in merged['loggers'].items():
-                if 'handlers' in loggerconfig:
-                    loggerconfig['handlers'].pop('count', None)
-    
-    logging.config.dictConfig(merged)
-
-
-class ColoredRecord:
-    LEVELCOLORS = {
-        logging.DEBUG:     Colors.GRAY,
-        logging.INFO:      '',
-        logging.WARNING:   Colors.YELLOW,
-        logging.ERROR:     Colors.RED,
-        logging.CRITICAL:  Colors.BOLD_RED,
-    }
-
-    def __init__(self, record: logging.LogRecord):
-        # The internal dict is used by Python logging library when formatting the message.
-        # (inspired from library "colorlog").
-        self.__dict__.update(record.__dict__)
-        self.__dict__.update({
-            'levelcolor': self.LEVELCOLORS.get(record.levelno, ''),
-            'red': Colors.RED,
-            'green': Colors.GREEN,
-            'yellow': Colors.YELLOW,
-            'cyan': Colors.CYAN,
-            'gray': Colors.GRAY,
-            'bold_red': Colors.BOLD_RED,
-            'reset': Colors.RESET,
-        })
-
-
-class ColoredFormatter(logging.Formatter):
-    def formatMessage(self, record: logging.LogRecord) -> str:
-        """Format a message from a record object."""
-        wrapper = ColoredRecord(record)
-        message = super().formatMessage(wrapper)
-        return message
-
-
-class CountHandler(logging.Handler):
-    def __init__(self, level=logging.WARNING):
-        self.counts: dict[int, int] = {}
-        atexit.register(self.print_counts)
-        super().__init__(level=level)
-
-    def print_counts(self):
-        msg = ""
-
-        levelnos = sorted(self.counts.keys(), reverse=True)
-        for levelno in levelnos:
-            levelname = logging.getLevelName(levelno)
-            levelcolor = ColoredRecord.LEVELCOLORS.get(levelno, '')
-            msg += (", " if msg else "") + f"{levelcolor}%s{Colors.RESET}" % levelname + ": %d" % self.counts[levelno]
-
-        if msg:
-            print("Logged " + msg)
-
-    def emit(self, record: logging.LogRecord):
-        if record.levelno >= self.level:
-            if not record.levelno in self.counts:
-                self.counts[record.levelno] = 1
-            else:
-                self.counts[record.levelno] += 1
-
-
-DEFAULT_LOGGING_DICTCONFIG = {
-    'version': 1,
-    'disable_existing_loggers': False,
-    'formatters': {
-        'color': {
-            '()': ColoredFormatter.__module__ + '.' + ColoredFormatter.__qualname__,
-            'format': '%(levelcolor)s%(levelname)s%(reset)s %(gray)s[%(name)s]%(reset)s %(levelcolor)s%(message)s%(reset)s',
-        },
-    },
-    'handlers': {
-        'console': {
-            'class': 'logging.StreamHandler',
-            'formatter': 'color',
-        },
-        'count': {
-            'class': CountHandler.__module__ + '.' + CountHandler.__qualname__,
-            'level': 'WARNING',
-        },
-    },
-    'root': {
-        'handlers': ['console', 'count'],
-        'level': os.environ.get('LOG_LEVEL', '').upper() or 'INFO',
-    },
-    'loggers': {
-        'django': { 'level': os.environ.get('DJANGO_LOG_LEVEL', '').upper() or 'INFO', 'propagate': False },
-    },
-}
+from __future__ import annotations
+import logging, logging.config, atexit, os
+from .colors import Colors
+
+
+def configure_logging(level: int|str = None, nocount: bool = False, config: dict = None):
+    merged = dict(DEFAULT_LOGGING_DICTCONFIG)
+
+    # Merge given config with default config
+    if config:
+        for prop, data in config.items():
+            if isinstance(data, dict) and prop in merged and isinstance(merged[prop], dict):
+                for key, value in data.items():
+                    if value is None:
+                        merged[prop].pop(key, None)
+                    else:
+                        merged[prop][key] = value
+            else:
+                merged[prop] = data
+
+    # Set root level if missing or if explicitely provided     
+    if level is not None or ('root' in merged and not 'level' in merged['root']):
+        if level is None:
+            level = os.environ.get('LOG_LEVEL', '').upper() or 'INFO'
+        
+        if isinstance(level, int):
+            level = logging.getLevelName(level)
+
+        merged['root']['level'] = level
+
+    # Remove count handler
+    if nocount:
+        if 'root' in merged and 'handlers' in merged['root']:
+            merged['root']['handlers'].pop('count', None)
+
+        if 'loggers' in merged:
+            for _, loggerconfig in merged['loggers'].items():
+                if 'handlers' in loggerconfig:
+                    loggerconfig['handlers'].pop('count', None)
+    
+    logging.config.dictConfig(merged)
+
+
+class ColoredRecord:
+    LEVELCOLORS = {
+        logging.DEBUG:     Colors.GRAY,
+        logging.INFO:      '',
+        logging.WARNING:   Colors.YELLOW,
+        logging.ERROR:     Colors.RED,
+        logging.CRITICAL:  Colors.BOLD_RED,
+    }
+
+    def __init__(self, record: logging.LogRecord):
+        # The internal dict is used by Python logging library when formatting the message.
+        # (inspired from library "colorlog").
+        self.__dict__.update(record.__dict__)
+        self.__dict__.update({
+            'levelcolor': self.LEVELCOLORS.get(record.levelno, ''),
+            'red': Colors.RED,
+            'green': Colors.GREEN,
+            'yellow': Colors.YELLOW,
+            'cyan': Colors.CYAN,
+            'gray': Colors.GRAY,
+            'bold_red': Colors.BOLD_RED,
+            'reset': Colors.RESET,
+        })
+
+
+class ColoredFormatter(logging.Formatter):
+    def formatMessage(self, record: logging.LogRecord) -> str:
+        """Format a message from a record object."""
+        wrapper = ColoredRecord(record)
+        message = super().formatMessage(wrapper)
+        return message
+
+
+class CountHandler(logging.Handler):
+    def __init__(self, level=logging.WARNING):
+        self.counts: dict[int, int] = {}
+        atexit.register(self.print_counts)
+        super().__init__(level=level)
+
+    def print_counts(self):
+        msg = ""
+
+        levelnos = sorted(self.counts.keys(), reverse=True)
+        for levelno in levelnos:
+            levelname = logging.getLevelName(levelno)
+            levelcolor = ColoredRecord.LEVELCOLORS.get(levelno, '')
+            msg += (", " if msg else "") + f"{levelcolor}%s{Colors.RESET}" % levelname + ": %d" % self.counts[levelno]
+
+        if msg:
+            print("Logged " + msg)
+
+    def emit(self, record: logging.LogRecord):
+        if record.levelno >= self.level:
+            if not record.levelno in self.counts:
+                self.counts[record.levelno] = 1
+            else:
+                self.counts[record.levelno] += 1
+
+
+DEFAULT_LOGGING_DICTCONFIG = {
+    'version': 1,
+    'disable_existing_loggers': False,
+    'formatters': {
+        'color': {
+            '()': ColoredFormatter.__module__ + '.' + ColoredFormatter.__qualname__,
+            'format': '%(levelcolor)s%(levelname)s%(reset)s %(gray)s[%(name)s]%(reset)s %(levelcolor)s%(message)s%(reset)s',
+        },
+    },
+    'handlers': {
+        'console': {
+            'class': 'logging.StreamHandler',
+            'formatter': 'color',
+        },
+        'count': {
+            'class': CountHandler.__module__ + '.' + CountHandler.__qualname__,
+            'level': 'WARNING',
+        },
+    },
+    'root': {
+        'handlers': ['console', 'count'],
+        'level': os.environ.get('LOG_LEVEL', '').upper() or 'INFO',
+    },
+    'loggers': {
+        'django': { 'level': os.environ.get('DJANGO_LOG_LEVEL', '').upper() or 'INFO', 'propagate': False },
+    },
+}
```

## zut/misc.py

```diff
@@ -1,118 +1,126 @@
-from __future__ import annotations
-import sys
-import logging
-from pathlib import Path
-from subprocess import CompletedProcess, SubprocessError
-from .colors import Colors
-
-
-# Export root directory of the library
-ZUT_ROOT = Path(__file__).parent
-
-
-# Export typings from adequate library
-if sys.version_info[0:2] < (3, 8):
-    from typing_extensions import Literal, Protocol
-else:
-    from typing import Literal, Protocol
-    
-if sys.version_info[0:2] < (3, 11):
-    from typing_extensions import Self
-else:
-    from typing import Self
-
-
-logger = logging.getLogger(__name__)
-
-
-def is_list_or_tuple_of(instance, element_type: type|tuple[type]):
-    if not isinstance(instance, (list,tuple)):
-        return False
-
-    for element in instance:
-        if not isinstance(element, element_type):
-            return False
-        
-    return True
-
-
-def check_completed_subprocess(cp: CompletedProcess, logger: logging.Logger = None, *, label: str = None, level: int|str = None, accept_returncode: int|list[int]|bool = False, accept_stdout: bool = False, accept_stderr: bool = False, maxlen: int = 200):
-    if not label:
-        label = cp.args[0]
-
-    if not logger and level is not None:
-        logger = globals()["logger"]
-    elif logger and level is None:
-        level = logging.ERROR
-
-
-    def is_returncode_issue(returncode: int):
-        if accept_returncode is True:
-            return False
-        elif isinstance(accept_returncode, int):
-            return returncode != accept_returncode
-        elif isinstance(accept_returncode, (list,tuple)):
-            return returncode not in accept_returncode
-        else:
-            return returncode != 0
-    
-
-    def extract_stream(content: str|bytes, name: str, color: str):
-        if not isinstance(content, str):
-            try:
-                content = content.decode('utf-8')
-            except UnicodeDecodeError:
-                content = content.decode('cp1252')
-        
-        data = content.strip()
-        if maxlen and len(data) > maxlen:
-            data = data[0:maxlen] + ''
-
-        result = ''
-        for line in data.splitlines():
-            result += f"\n{color}[{label} {name}]{Colors.RESET} {line}"
-        return result
-    
-
-    issue = False
-
-    if is_returncode_issue(cp.returncode):
-        message = f"{label} returned {Colors.YELLOW}code {cp.returncode}{Colors.RESET}"
-        issue = True
-    else:
-        message = f"{label} returned {Colors.CYAN}code {cp.returncode}{Colors.RESET}"
-    
-
-    result = extract_stream(cp.stdout, 'stdout', Colors.CYAN if accept_stdout else Colors.YELLOW)
-    if result:
-        message += result
-        if not accept_stdout:
-            issue = True
-
-    message += extract_stream(cp.stderr, 'stderr', Colors.CYAN if accept_stderr else Colors.YELLOW)
-    if result:
-        message += result
-        if not accept_stderr:
-            issue = True
-
-    if issue:
-        if logger:
-            logger.log(level, message)
-        else:
-            raise SubprocessError(message)
-    else:
-        if logger:
-            logger.log(logging.DEBUG, message)
-
-    return issue
-
-
-def get_venv() -> Path|None:
-    """
-    Return the path to the virtual environment if Python runs inside a virtual environment, None otherwise.
-    """
-    base_prefix = getattr(sys, "base_prefix", None) or getattr(sys, "real_prefix", None) or sys.prefix
-    if base_prefix == sys.prefix:
-        return None
-    return Path(sys.prefix)
-
+from __future__ import annotations
+import sys
+import logging
+from pathlib import Path
+from subprocess import CompletedProcess, SubprocessError
+from .colors import Colors
+
+
+# Export root directory of the library
+ZUT_ROOT = Path(__file__).parent
+
+
+# Export typings from adequate library
+if sys.version_info[0:2] < (3, 8):
+    from typing_extensions import Literal, Protocol
+else:
+    from typing import Literal, Protocol
+    
+if sys.version_info[0:2] < (3, 11):
+    from typing_extensions import Self
+else:
+    from typing import Self
+
+
+logger = logging.getLogger(__name__)
+
+
+def is_list_or_tuple_of(instance, element_type: type|tuple[type]):
+    if not isinstance(instance, (list,tuple)):
+        return False
+
+    for element in instance:
+        if not isinstance(element, element_type):
+            return False
+        
+    return True
+
+
+def check_completed_subprocess(cp: CompletedProcess, logger: logging.Logger = None, *, label: str = None, level: int|str = None, accept_returncode: int|list[int]|bool = False, accept_stdout: bool = False, accept_stderr: bool = False, maxlen: int = 200):
+    if not label:
+        label = cp.args[0]
+
+    if not logger and level is not None:
+        logger = globals()["logger"]
+    elif logger and level is None:
+        level = logging.ERROR
+
+
+    def is_returncode_issue(returncode: int):
+        if accept_returncode is True:
+            return False
+        elif isinstance(accept_returncode, int):
+            return returncode != accept_returncode
+        elif isinstance(accept_returncode, (list,tuple)):
+            return returncode not in accept_returncode
+        else:
+            return returncode != 0
+    
+
+    def extract_stream(content: str|bytes, name: str, color: str):
+        if not isinstance(content, str):
+            try:
+                content = content.decode('utf-8')
+            except UnicodeDecodeError:
+                content = content.decode('cp1252')
+        
+        data = content.strip()
+        if maxlen and len(data) > maxlen:
+            data = data[0:maxlen] + ''
+
+        result = ''
+        for line in data.splitlines():
+            result += f"\n{color}[{label} {name}]{Colors.RESET} {line}"
+        return result
+    
+
+    issue = False
+
+    if is_returncode_issue(cp.returncode):
+        message = f"{label} returned {Colors.YELLOW}code {cp.returncode}{Colors.RESET}"
+        issue = True
+    else:
+        message = f"{label} returned {Colors.CYAN}code {cp.returncode}{Colors.RESET}"
+    
+
+    result = extract_stream(cp.stdout, 'stdout', Colors.CYAN if accept_stdout else Colors.YELLOW)
+    if result:
+        message += result
+        if not accept_stdout:
+            issue = True
+
+    message += extract_stream(cp.stderr, 'stderr', Colors.CYAN if accept_stderr else Colors.YELLOW)
+    if result:
+        message += result
+        if not accept_stderr:
+            issue = True
+
+    if issue:
+        if logger:
+            logger.log(level, message)
+        else:
+            raise SubprocessError(message)
+    else:
+        if logger:
+            logger.log(logging.DEBUG, message)
+
+    return issue
+
+
+def get_venv() -> Path|None:
+    """
+    Return the path to the virtual environment if Python runs inside a virtual environment, None otherwise.
+    """
+    base_prefix = getattr(sys, "base_prefix", None) or getattr(sys, "real_prefix", None) or sys.prefix
+    if base_prefix == sys.prefix:
+        return None
+    return Path(sys.prefix)
+
+
+def is_in_venv(path: Path|str) -> bool:
+    if not isinstance(path, Path):
+        path = Path(path)
+    venv = get_venv()
+    if venv is None:
+        return False
+    return venv in path.parents
```

## zut/numeric.py

 * *Ordering differences only*

```diff
@@ -1,86 +1,86 @@
-from __future__ import annotations
-from decimal import Decimal, InvalidOperation
-from locale import getlocale
-from .misc import Literal
-from .text import ValueString
-
-
-def human_bytes(value: int, *, unit: str = 'iB', divider: int = 1024, decimals: int = 2, decimal_separator: str = None, thousands_separator: str = None, max_multiple: Literal['K']|Literal['M']|Literal['G']|Literal['T'] = None):
-    return human_number(value, unit=unit, divider=divider, decimals=decimals, decimal_separator=decimal_separator, thousands_separator=thousands_separator, max_multiple=max_multiple)
-
-
-def human_number(value: int, *, unit: str = '', divider: int = 1000, decimals: int = 2, decimal_separator: str = None, thousands_separator: str = None, max_multiple: Literal['K']|Literal['M']|Literal['G']|Literal['T'] = None):
-    if value is None:
-        return ValueString('', None)
-
-    suffixes = []
-
-    # Append non-multiple suffix (bytes)
-    # (if unit is 'iB' we dont display the 'i' as it makes more sens to display "123 B" than "123 iB")
-    if unit:
-        suffixes.append(' ' + (unit[1:] if len(unit) >= 2 and unit[0] == 'i' else unit))
-    else:
-        suffixes.append('')
-
-    # Append multiple suffixes
-    for multiple in ['K', 'M', 'G', 'T']:
-        suffixes.append(f' {multiple}{unit}')
-        if max_multiple and max_multiple.upper() == multiple:
-            break
-
-    i = 0
-    suffix = suffixes[i]
-    divided_value = value
-
-    while divided_value > 1000 and i < len(suffixes) - 1:
-        divided_value /= divider
-        i += 1
-        suffix = suffixes[i]
-
-    # Format value
-    if i == 0:
-        formatted_value = '{value:,.0f}'.format(value=divided_value)
-    else:
-        formatted_value = ('{value:,.'+str(decimals)+'f}').format(value=divided_value)
-
-    #  Replace separators
-    if decimal_separator is not None or thousands_separator is not None:
-        chars = formatted_value
-        formatted_value = ''
-        for c in chars:
-            if c == ',' and thousands_separator is not None:
-                formatted_value += thousands_separator
-            elif c == '.' and decimal_separator is not None:
-                formatted_value += decimal_separator
-            else:
-                formatted_value += c
-
-    # Display formatted value with suffix
-    return ValueString(f'{formatted_value}{suffix}', value)
-
-
-def parse_decimal(value: float|Decimal|str|None) -> float|Decimal:
-    """
-    Parse a decimal with variable decimal separator: may be formatted with comma decimal separator instead of dot.
-    """
-    if value is None or value == "":
-        return None
-        
-    if isinstance(value, (float,Decimal)):
-        return value   
-
-    value = str(value).replace(',', '.')
-
-    try:
-        return Decimal(value)
-    except InvalidOperation:
-        raise ValueError(f"invalid decimal value: {value}") from None
-
-
-def get_default_decimal_separator():
-    locale = getlocale()
-    if locale and locale[0] and locale[0].startswith('fr'):
-        return ','
-    else:
-        return '.'
-
+from __future__ import annotations
+from decimal import Decimal, InvalidOperation
+from locale import getlocale
+from .misc import Literal
+from .text import ValueString
+
+
+def human_bytes(value: int, *, unit: str = 'iB', divider: int = 1024, decimals: int = 2, decimal_separator: str = None, thousands_separator: str = None, max_multiple: Literal['K']|Literal['M']|Literal['G']|Literal['T'] = None):
+    return human_number(value, unit=unit, divider=divider, decimals=decimals, decimal_separator=decimal_separator, thousands_separator=thousands_separator, max_multiple=max_multiple)
+
+
+def human_number(value: int, *, unit: str = '', divider: int = 1000, decimals: int = 2, decimal_separator: str = None, thousands_separator: str = None, max_multiple: Literal['K']|Literal['M']|Literal['G']|Literal['T'] = None):
+    if value is None:
+        return ValueString('', None)
+
+    suffixes = []
+
+    # Append non-multiple suffix (bytes)
+    # (if unit is 'iB' we dont display the 'i' as it makes more sens to display "123 B" than "123 iB")
+    if unit:
+        suffixes.append(' ' + (unit[1:] if len(unit) >= 2 and unit[0] == 'i' else unit))
+    else:
+        suffixes.append('')
+
+    # Append multiple suffixes
+    for multiple in ['K', 'M', 'G', 'T']:
+        suffixes.append(f' {multiple}{unit}')
+        if max_multiple and max_multiple.upper() == multiple:
+            break
+
+    i = 0
+    suffix = suffixes[i]
+    divided_value = value
+
+    while divided_value > 1000 and i < len(suffixes) - 1:
+        divided_value /= divider
+        i += 1
+        suffix = suffixes[i]
+
+    # Format value
+    if i == 0:
+        formatted_value = '{value:,.0f}'.format(value=divided_value)
+    else:
+        formatted_value = ('{value:,.'+str(decimals)+'f}').format(value=divided_value)
+
+    #  Replace separators
+    if decimal_separator is not None or thousands_separator is not None:
+        chars = formatted_value
+        formatted_value = ''
+        for c in chars:
+            if c == ',' and thousands_separator is not None:
+                formatted_value += thousands_separator
+            elif c == '.' and decimal_separator is not None:
+                formatted_value += decimal_separator
+            else:
+                formatted_value += c
+
+    # Display formatted value with suffix
+    return ValueString(f'{formatted_value}{suffix}', value)
+
+
+def parse_decimal(value: float|Decimal|str|None) -> float|Decimal:
+    """
+    Parse a decimal with variable decimal separator: may be formatted with comma decimal separator instead of dot.
+    """
+    if value is None or value == "":
+        return None
+        
+    if isinstance(value, (float,Decimal)):
+        return value   
+
+    value = str(value).replace(',', '.')
+
+    try:
+        return Decimal(value)
+    except InvalidOperation:
+        raise ValueError(f"invalid decimal value: {value}") from None
+
+
+def get_default_decimal_separator():
+    locale = getlocale()
+    if locale and locale[0] and locale[0].startswith('fr'):
+        return ','
+    else:
+        return '.'
+
```

## zut/tabular.py

 * *Ordering differences only*

```diff
@@ -1,67 +1,67 @@
-from __future__ import annotations
-from typing import Any, Callable, Iterable
-
-class Row:
-    """
-    A row of tabular data.
-    """
-    def __init__(self, get: Iterable|Callable[[int],Any], *, headers: list[str], set: Callable[[int,Any]] = None):
-        if callable(get):
-            self._values = None
-            self._get = get
-            self._set = set
-        else:
-            # "get" is an Iterable
-            if set:
-                raise ValueError(f"argument \"set\" cannot be given if \"get\" is not a callable")
-            self._values = get if isinstance(get, list) else list(get)
-            self._get = None
-            self._set = None
-
-        self.headers = headers
-        self._header_indexes: dict[str,int] = None
-        
-        
-    def __len__(self):
-        return len(self.values)
-
-
-    @property
-    def values(self) -> list[Any]:
-        if self._values is not None:
-            return self._values
-        else:
-            return [self._get(index) for index in range(0, len(self.headers))]
-
-
-    def __getitem__(self, key: int|str):
-        if not isinstance(key, int):
-            key = self._get_header_index(key)
-            
-        if self._values is not None:
-            return self._values[key]
-        else:
-            return self._get(key)
-        
-
-    def __setitem__(self, key: int|str, value):
-        if not self._set:
-            raise ValueError(f"this row cannot be set")
-        
-        if not isinstance(key, int):
-            key = self._get_header_index(key)
-        
-        self._set(key, value)
-
-
-    def _get_header_index(self, header: str):
-        if self._header_indexes is None:
-            if not self.headers:
-                raise ValueError(f"cannot use string keys (no headers)")
-            self._header_indexes = {header: i for i, header in enumerate(self.headers)}
-        return self._header_indexes[header]
-
-
-    def as_dict(self):
-        return {header: self[i] for i, header in enumerate(self.headers)}
-
+from __future__ import annotations
+from typing import Any, Callable, Iterable
+
+class Row:
+    """
+    A row of tabular data.
+    """
+    def __init__(self, get: Iterable|Callable[[int],Any], *, headers: list[str], set: Callable[[int,Any]] = None):
+        if callable(get):
+            self._values = None
+            self._get = get
+            self._set = set
+        else:
+            # "get" is an Iterable
+            if set:
+                raise ValueError(f"argument \"set\" cannot be given if \"get\" is not a callable")
+            self._values = get if isinstance(get, list) else list(get)
+            self._get = None
+            self._set = None
+
+        self.headers = headers
+        self._header_indexes: dict[str,int] = None
+        
+        
+    def __len__(self):
+        return len(self.values)
+
+
+    @property
+    def values(self) -> list[Any]:
+        if self._values is not None:
+            return self._values
+        else:
+            return [self._get(index) for index in range(0, len(self.headers))]
+
+
+    def __getitem__(self, key: int|str):
+        if not isinstance(key, int):
+            key = self._get_header_index(key)
+            
+        if self._values is not None:
+            return self._values[key]
+        else:
+            return self._get(key)
+        
+
+    def __setitem__(self, key: int|str, value):
+        if not self._set:
+            raise ValueError(f"this row cannot be set")
+        
+        if not isinstance(key, int):
+            key = self._get_header_index(key)
+        
+        self._set(key, value)
+
+
+    def _get_header_index(self, header: str):
+        if self._header_indexes is None:
+            if not self.headers:
+                raise ValueError(f"cannot use string keys (no headers)")
+            self._header_indexes = {header: i for i, header in enumerate(self.headers)}
+        return self._header_indexes[header]
+
+
+    def as_dict(self):
+        return {header: self[i] for i, header in enumerate(self.headers)}
+
```

## zut/text.py

 * *Ordering differences only*

```diff
@@ -1,70 +1,70 @@
-from __future__ import annotations
-import re
-import unicodedata
-from io import TextIOWrapper
-from typing import Generic, TypeVar
-
-T = TypeVar('T')
-
-
-class ValueString(str, Generic[T]):
-    """
-    A string internally associated to a value of a given type.
-    """
-    value: T
-
-    def __new__(cls, strvalue: str, value: T):
-        hb = super().__new__(cls, strvalue)
-        hb.value = value
-        return hb
-
-
-def slugify(value: str, allow_unicode: bool = False, separator: str = '-') -> str:
-    """ 
-    Generate a slug. Like `django.utils.text.slugify` with possibility to change the separator.
-    """
-    value = str(value)
-    if allow_unicode:
-        value = unicodedata.normalize("NFKC", value)
-    else:
-        value = unicodedata.normalize("NFKD", value).encode("ascii", "ignore").decode("ascii")
-    value = re.sub(r"[^\w\s-]", "", value.lower())
-    return re.sub(r"[-\s]+", separator, value).strip(f"{separator}_")
-
-
-def slugify_snake(value: str) -> str:
-    """
-    CamlCase => camel_case
-    """
-    value = str(value)
-    value = unicodedata.normalize("NFKD", value).encode("ascii", "ignore").decode("ascii")
-    value = re.sub(r"[^\w\s-]", "", value) # not .lower()
-    value = re.sub(r"[-_\s]+", '_', value).strip('_')
-    value = re.sub(r'(.)([A-Z][a-z]+)', r'\1_\2', value)
-    return re.sub(r'([a-z0-9])([A-Z])', r'\1_\2', value).lower()
-
-
-def remove_consecutive_whitespaces(s: str):
-    if s is None:
-        return None
-    return re.sub(r'\s+', ' ', s).strip()
-
-
-def remove_whitespaces(s):
-    if s is None:
-        return None
-    return re.sub(r'\s', '', s)
-
-
-def reconfigure_encoding(fp: TextIOWrapper, encoding: str) -> str:
-    """
-    Reconfigure utf-8 encoding to handle the BOM if any.
-    """
-    if encoding == 'utf-8':
-        text = fp.read()
-        if text and text[0] == '\ufeff':
-            encoding = 'utf-8-sig'
-            fp.reconfigure(encoding=encoding)
-        fp.seek(0)
-
-    return encoding
+from __future__ import annotations
+import re
+import unicodedata
+from io import TextIOWrapper
+from typing import Generic, TypeVar
+
+T = TypeVar('T')
+
+
+class ValueString(str, Generic[T]):
+    """
+    A string internally associated to a value of a given type.
+    """
+    value: T
+
+    def __new__(cls, strvalue: str, value: T):
+        hb = super().__new__(cls, strvalue)
+        hb.value = value
+        return hb
+
+
+def slugify(value: str, allow_unicode: bool = False, separator: str = '-') -> str:
+    """ 
+    Generate a slug. Like `django.utils.text.slugify` with possibility to change the separator.
+    """
+    value = str(value)
+    if allow_unicode:
+        value = unicodedata.normalize("NFKC", value)
+    else:
+        value = unicodedata.normalize("NFKD", value).encode("ascii", "ignore").decode("ascii")
+    value = re.sub(r"[^\w\s-]", "", value.lower())
+    return re.sub(r"[-\s]+", separator, value).strip(f"{separator}_")
+
+
+def slugify_snake(value: str) -> str:
+    """
+    CamlCase => camel_case
+    """
+    value = str(value)
+    value = unicodedata.normalize("NFKD", value).encode("ascii", "ignore").decode("ascii")
+    value = re.sub(r"[^\w\s-]", "", value) # not .lower()
+    value = re.sub(r"[-_\s]+", '_', value).strip('_')
+    value = re.sub(r'(.)([A-Z][a-z]+)', r'\1_\2', value)
+    return re.sub(r'([a-z0-9])([A-Z])', r'\1_\2', value).lower()
+
+
+def remove_consecutive_whitespaces(s: str):
+    if s is None:
+        return None
+    return re.sub(r'\s+', ' ', s).strip()
+
+
+def remove_whitespaces(s):
+    if s is None:
+        return None
+    return re.sub(r'\s', '', s)
+
+
+def reconfigure_encoding(fp: TextIOWrapper, encoding: str) -> str:
+    """
+    Reconfigure utf-8 encoding to handle the BOM if any.
+    """
+    if encoding == 'utf-8':
+        text = fp.read()
+        if text and text[0] == '\ufeff':
+            encoding = 'utf-8-sig'
+            fp.reconfigure(encoding=encoding)
+        fp.seek(0)
+
+    return encoding
```

## zut/db/__init__.py

```diff
@@ -1,47 +1,47 @@
-from __future__ import annotations
-import re
-from urllib.parse import urlparse
-from .commons import DbWrapper
-from .mssql import MssqlWrapper
-from .pg import PgWrapper
-
-
-def get_db_with_schema_and_table(src: str) -> tuple[DbWrapper, str, str]:
-    if src.startswith('db:'):
-        src = src[3:]
-
-    r = urlparse(src)
-
-    if r.scheme == 'pg':
-        wrapper_cls = PgWrapper
-    elif r.scheme == 'mssql':
-        wrapper_cls = MssqlWrapper
-    elif r.scheme:
-        raise ValueError(f"unsupported db engine: {r.scheme}")
-    else:
-        raise ValueError(f"invalid db src: no scheme in {src}")
-    
-    if not wrapper_cls.is_available():
-        raise ValueError(f"cannot use db {r.scheme} ({wrapper_cls.__name__} not available)")
-    
-    if r.fragment:
-        raise ValueError(f"invalid db src: unexpected fragment: {r.fragment}")
-    if r.query:
-        raise ValueError(f"invalid db src: unexpected query: {r.query}")
-    if r.params:
-        raise ValueError(f"invalid db src: unexpected params: {r.params}")
-    
-    m = re.match(r'^/(?P<database>[^/@\:]+)(/((?P<schema>[^/@\:\.]+)\.)?(?P<table>[^/@\:\.]+))?$', r.path)
-    if not m:
-        raise ValueError(f"invalid db src: invalid path: {r.path}")
-    
-    database = m['database']
-    table = m['table']
-    schema = (m['schema'] or wrapper_cls.default_schema_name) if table else None
-    
-    return wrapper_cls(database=database, host=r.hostname, port=r.port, user=r.username, password=r.password), schema, table
-
-
-def get_db(src: str) -> DbWrapper:
-    db, _, _ = get_db_with_schema_and_table(src)
-    return db
+from __future__ import annotations
+import re
+from urllib.parse import urlparse
+from .commons import DbWrapper
+from .mssql import MssqlWrapper
+from .pg import PgWrapper
+
+
+def create_db_wrapper_with_schema_and_table(uri: str) -> tuple[DbWrapper, str, str]:
+    if uri.startswith('db:'):
+        uri = uri[3:]
+
+    r = urlparse(uri)
+
+    if r.scheme == PgWrapper.scheme:
+        wrapper_cls = PgWrapper
+    elif r.scheme == MssqlWrapper.scheme:
+        wrapper_cls = MssqlWrapper
+    elif r.scheme:
+        raise ValueError(f"unsupported db engine: {r.scheme}")
+    else:
+        raise ValueError(f"invalid db src: no scheme in {uri}")
+    
+    if not wrapper_cls.is_available():
+        raise ValueError(f"cannot use db {r.scheme} ({wrapper_cls.__name__} not available)")
+    
+    if r.fragment:
+        raise ValueError(f"invalid db src: unexpected fragment: {r.fragment}")
+    if r.query:
+        raise ValueError(f"invalid db src: unexpected query: {r.query}")
+    if r.params:
+        raise ValueError(f"invalid db src: unexpected params: {r.params}")
+    
+    m = re.match(r'^/(?P<name>[^/@\:]+)(/((?P<schema>[^/@\:\.]+)\.)?(?P<table>[^/@\:\.]+))?$', r.path)
+    if not m:
+        raise ValueError(f"invalid db src: invalid path: {r.path}")
+    
+    name = m['name']
+    table = m['table']
+    schema = (m['schema'] or wrapper_cls.default_schema_name) if table else None
+    
+    return wrapper_cls(name=name, host=r.hostname, port=r.port, user=r.username, password=r.password), schema, table
+
+
+def create_db_wrapper(uri: str) -> DbWrapper:
+    db, _, _ = create_db_wrapper_with_schema_and_table(uri)
+    return db
```

## zut/db/commons.py

```diff
@@ -1,843 +1,940 @@
-00000000: 0a66 726f 6d20 5f5f 6675 7475 7265 5f5f  .from __future__
-00000010: 2069 6d70 6f72 7420 616e 6e6f 7461 7469   import annotati
-00000020: 6f6e 730a 6672 6f6d 2064 6174 6574 696d  ons.from datetim
-00000030: 6520 696d 706f 7274 2074 696d 657a 6f6e  e import timezon
-00000040: 650a 6672 6f6d 2069 6f20 696d 706f 7274  e.from io import
-00000050: 2049 4f42 6173 650a 696d 706f 7274 206c   IOBase.import l
-00000060: 6f67 6769 6e67 0a66 726f 6d20 7061 7468  ogging.from path
-00000070: 6c69 6220 696d 706f 7274 2050 6174 680a  lib import Path.
-00000080: 6672 6f6d 2074 7970 696e 6720 696d 706f  from typing impo
-00000090: 7274 2041 6e79 2c20 4765 6e65 7269 632c  rt Any, Generic,
-000000a0: 2054 7970 6556 6172 0a66 726f 6d20 2e2e   TypeVar.from ..
-000000b0: 6d69 7363 2069 6d70 6f72 7420 4c69 7465  misc import Lite
-000000c0: 7261 6c0a 0a6c 6f67 6765 7220 3d20 6c6f  ral..logger = lo
-000000d0: 6767 696e 672e 6765 744c 6f67 6765 7228  gging.getLogger(
-000000e0: 5f5f 6e61 6d65 5f5f 290a 0a54 5f43 6f6e  __name__)..T_Con
-000000f0: 6e65 6374 696f 6e20 3d20 5479 7065 5661  nection = TypeVa
-00000100: 7228 2754 5f43 6f6e 6e65 6374 696f 6e27  r('T_Connection'
-00000110: 290a 545f 4375 7273 6f72 203d 2054 7970  ).T_Cursor = Typ
-00000120: 6556 6172 2827 545f 4375 7273 6f72 2729  eVar('T_Cursor')
-00000130: 0a0a 636c 6173 7320 4462 5772 6170 7065  ..class DbWrappe
-00000140: 7228 4765 6e65 7269 635b 545f 436f 6e6e  r(Generic[T_Conn
-00000150: 6563 7469 6f6e 2c20 545f 4375 7273 6f72  ection, T_Cursor
-00000160: 5d29 3a0a 2020 2020 4063 6c61 7373 6d65  ]):.    @classme
-00000170: 7468 6f64 0a20 2020 2064 6566 2069 735f  thod.    def is_
-00000180: 6176 6169 6c61 626c 6528 636c 7329 3a0a  available(cls):.
-00000190: 2020 2020 2020 2020 7261 6973 6520 4e6f          raise No
-000001a0: 7449 6d70 6c65 6d65 6e74 6564 4572 726f  tImplementedErro
-000001b0: 7228 290a 2020 2020 0a20 2020 2064 6566  r().    .    def
-000001c0: 6175 6c74 5f73 6368 656d 615f 6e61 6d65  ault_schema_name
-000001d0: 3a20 7374 720a 0a20 2020 2064 6566 205f  : str..    def _
-000001e0: 5f69 6e69 745f 5f28 7365 6c66 2c20 2a2c  _init__(self, *,
-000001f0: 2064 6174 6162 6173 653a 2073 7472 203d   database: str =
-00000200: 204e 6f6e 652c 2068 6f73 743a 2073 7472   None, host: str
-00000210: 203d 204e 6f6e 652c 2070 6f72 743a 2069   = None, port: i
-00000220: 6e74 203d 204e 6f6e 652c 2075 7365 723a  nt = None, user:
-00000230: 2073 7472 203d 204e 6f6e 652c 2070 6173   str = None, pas
-00000240: 7377 6f72 643a 2073 7472 203d 204e 6f6e  sword: str = Non
-00000250: 652c 2064 6a61 6e67 6f5f 616c 6961 733a  e, django_alias:
-00000260: 2073 7472 203d 204e 6f6e 652c 2063 6f6e   str = None, con
-00000270: 6e65 6374 696f 6e3a 2054 5f43 6f6e 6e65  nection: T_Conne
-00000280: 6374 696f 6e20 3d20 4e6f 6e65 2c20 6e61  ction = None, na
-00000290: 6976 655f 747a 3a20 4c69 7465 7261 6c5b  ive_tz: Literal[
-000002a0: 276c 6f63 616c 275d 7c74 696d 657a 6f6e  'local']|timezon
-000002b0: 6520 3d20 276c 6f63 616c 2729 3a20 2020  e = 'local'):   
-000002c0: 200a 2020 2020 2020 2020 6966 206e 6f74   .        if not
-000002d0: 2073 656c 662e 6973 5f61 7661 696c 6162   self.is_availab
-000002e0: 6c65 2829 3a0a 2020 2020 2020 2020 2020  le():.          
-000002f0: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
-00000300: 6f72 2866 2263 616e 6e6f 7420 7573 6520  or(f"cannot use 
-00000310: 7b74 7970 6528 7365 6c66 292e 5f5f 6e61  {type(self).__na
-00000320: 6d65 5f5f 7d20 286e 6f74 2061 7661 696c  me__} (not avail
-00000330: 6162 6c65 2922 290a 2020 2020 2020 2020  able)").        
-00000340: 0a20 2020 2020 2020 2069 6620 646a 616e  .        if djan
-00000350: 676f 5f61 6c69 6173 2069 7320 6e6f 7420  go_alias is not 
-00000360: 4e6f 6e65 3a0a 2020 2020 2020 2020 2020  None:.          
-00000370: 2020 6672 6f6d 2064 6a61 6e67 6f2e 636f    from django.co
-00000380: 6e66 2069 6d70 6f72 7420 7365 7474 696e  nf import settin
-00000390: 6773 0a20 2020 2020 2020 2020 2020 2044  gs.            D
-000003a0: 4154 4142 4153 4553 203d 2073 6574 7469  ATABASES = setti
-000003b0: 6e67 732e 4441 5441 4241 5345 530a 2020  ngs.DATABASES.  
-000003c0: 2020 2020 2020 2020 2020 6966 206e 6f74            if not
-000003d0: 2064 6a61 6e67 6f5f 616c 6961 7320 696e   django_alias in
-000003e0: 2044 4154 4142 4153 4553 3a0a 2020 2020   DATABASES:.    
-000003f0: 2020 2020 2020 2020 2020 2020 7261 6973              rais
-00000400: 6520 5661 6c75 6545 7272 6f72 2866 226b  e ValueError(f"k
-00000410: 6579 205c 227b 646a 616e 676f 5f61 6c69  ey \"{django_ali
-00000420: 6173 7d5c 2220 6e6f 7420 666f 756e 6420  as}\" not found 
-00000430: 696e 2064 6a61 6e67 6f20 4441 5441 4241  in django DATABA
-00000440: 5345 5320 7365 7474 696e 6773 2229 0a20  SES settings"). 
-00000450: 2020 2020 2020 2020 2020 2064 6174 6162             datab
-00000460: 6173 655f 7365 7474 696e 6773 3a20 6469  ase_settings: di
-00000470: 6374 5b73 7472 2c41 6e79 5d20 3d20 4441  ct[str,Any] = DA
-00000480: 5441 4241 5345 535b 646a 616e 676f 5f61  TABASES[django_a
-00000490: 6c69 6173 5d0a 2020 2020 2020 2020 656c  lias].        el
-000004a0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            
-000004b0: 6461 7461 6261 7365 5f73 6574 7469 6e67  database_setting
-000004c0: 733a 2064 6963 745b 7374 722c 416e 795d  s: dict[str,Any]
-000004d0: 203d 207b 7d0a 0a20 2020 2020 2020 2073   = {}..        s
-000004e0: 656c 662e 5f64 6174 6162 6173 653a 2073  elf._database: s
-000004f0: 7472 203d 2064 6174 6162 6173 6520 6966  tr = database if
-00000500: 2064 6174 6162 6173 6520 6973 206e 6f74   database is not
-00000510: 204e 6f6e 6520 656c 7365 2064 6174 6162   None else datab
-00000520: 6173 655f 7365 7474 696e 6773 2e67 6574  ase_settings.get
-00000530: 2827 4e41 4d45 272c 204e 6f6e 6529 0a20  ('NAME', None). 
-00000540: 2020 2020 2020 2073 656c 662e 5f68 6f73         self._hos
-00000550: 743a 2073 7472 203d 2068 6f73 7420 6966  t: str = host if
-00000560: 2068 6f73 7420 6973 206e 6f74 204e 6f6e   host is not Non
-00000570: 6520 656c 7365 2064 6174 6162 6173 655f  e else database_
-00000580: 7365 7474 696e 6773 2e67 6574 2827 484f  settings.get('HO
-00000590: 5354 272c 204e 6f6e 6529 0a20 2020 2020  ST', None).     
-000005a0: 2020 2073 656c 662e 5f75 7365 723a 2073     self._user: s
-000005b0: 7472 203d 2075 7365 7220 6966 2075 7365  tr = user if use
-000005c0: 7220 6973 206e 6f74 204e 6f6e 6520 656c  r is not None el
-000005d0: 7365 2064 6174 6162 6173 655f 7365 7474  se database_sett
-000005e0: 696e 6773 2e67 6574 2827 5553 4552 272c  ings.get('USER',
-000005f0: 204e 6f6e 6529 0a20 2020 2020 2020 2073   None).        s
-00000600: 656c 662e 5f70 6173 7377 6f72 643a 2073  elf._password: s
-00000610: 7472 203d 2070 6173 7377 6f72 6420 6966  tr = password if
-00000620: 2070 6173 7377 6f72 6420 6973 206e 6f74   password is not
-00000630: 204e 6f6e 6520 656c 7365 2064 6174 6162   None else datab
-00000640: 6173 655f 7365 7474 696e 6773 2e67 6574  ase_settings.get
-00000650: 2827 5041 5353 574f 5244 272c 204e 6f6e  ('PASSWORD', Non
-00000660: 6529 0a20 2020 2020 2020 2073 656c 662e  e).        self.
-00000670: 5f70 6f72 743a 2073 7472 203d 2070 6f72  _port: str = por
-00000680: 7420 6966 2070 6f72 7420 6973 206e 6f74  t if port is not
-00000690: 204e 6f6e 6520 656c 7365 2064 6174 6162   None else datab
-000006a0: 6173 655f 7365 7474 696e 6773 2e67 6574  ase_settings.get
-000006b0: 2827 504f 5254 272c 204e 6f6e 6529 0a0a  ('PORT', None)..
-000006c0: 2020 2020 2020 2020 6966 2069 7369 6e73          if isins
-000006d0: 7461 6e63 6528 7365 6c66 2e5f 706f 7274  tance(self._port
-000006e0: 2c20 7374 7229 3a0a 2020 2020 2020 2020  , str):.        
-000006f0: 2020 2020 7365 6c66 2e5f 706f 7274 203d      self._port =
-00000700: 2069 6e74 2873 656c 662e 5f70 6f72 7429   int(self._port)
-00000710: 0a20 2020 2020 2020 200a 2020 2020 2020  .        .      
-00000720: 2020 7365 6c66 2e5f 6e61 6976 655f 747a    self._naive_tz
-00000730: 203d 206e 6169 7665 5f74 7a0a 2020 2020   = naive_tz.    
-00000740: 2020 2020 2222 2220 4966 206e 6f74 204e      """ If not N
-00000750: 6f6e 652c 2069 6e64 6963 6174 6520 7768  one, indicate wh
-00000760: 6963 6820 7469 6d65 7a6f 6e65 2077 696c  ich timezone wil
-00000770: 6c20 6265 2075 7365 6420 666f 7220 6e61  l be used for na
-00000780: 6976 6520 6461 7465 7469 6d65 732e 2022  ive datetimes. "
-00000790: 2222 0a0a 2020 2020 2020 2020 7365 6c66  ""..        self
-000007a0: 2e5f 636f 6e6e 6563 7469 6f6e 203d 2063  ._connection = c
-000007b0: 6f6e 6e65 6374 696f 6e0a 2020 2020 2020  onnection.      
-000007c0: 2020 7365 6c66 2e5f 636f 6e6e 6563 7469    self._connecti
-000007d0: 6f6e 5f69 735f 636c 6f73 6162 6c65 203d  on_is_closable =
-000007e0: 2063 6f6e 6e65 6374 696f 6e20 6973 204e   connection is N
-000007f0: 6f6e 650a 0a0a 2020 2020 6465 6620 5f5f  one...    def __
-00000800: 656e 7465 725f 5f28 7365 6c66 293a 0a20  enter__(self):. 
-00000810: 2020 2020 2020 2072 6574 7572 6e20 7365         return se
-00000820: 6c66 0a0a 0a20 2020 2064 6566 205f 5f65  lf...    def __e
-00000830: 7869 745f 5f28 7365 6c66 2c20 6578 635f  xit__(self, exc_
-00000840: 7479 7065 203d 204e 6f6e 652c 2065 7863  type = None, exc
-00000850: 5f76 616c 203d 204e 6f6e 652c 2065 7863  _val = None, exc
-00000860: 5f74 6220 3d20 4e6f 6e65 293a 0a20 2020  _tb = None):.   
-00000870: 2020 2020 2069 6620 7365 6c66 2e5f 636f       if self._co
-00000880: 6e6e 6563 7469 6f6e 2061 6e64 2073 656c  nnection and sel
-00000890: 662e 5f63 6f6e 6e65 6374 696f 6e5f 6973  f._connection_is
-000008a0: 5f63 6c6f 7361 626c 653a 0a20 2020 2020  _closable:.     
-000008b0: 2020 2020 2020 2073 656c 662e 5f63 6f6e         self._con
-000008c0: 6e65 6374 696f 6e2e 636c 6f73 6528 290a  nection.close().
-000008d0: 0a0a 2020 2020 4070 726f 7065 7274 790a  ..    @property.
-000008e0: 2020 2020 6465 6620 636f 6e6e 6563 7469      def connecti
-000008f0: 6f6e 2873 656c 6629 3a0a 2020 2020 2020  on(self):.      
-00000900: 2020 6966 206e 6f74 2073 656c 662e 5f63    if not self._c
-00000910: 6f6e 6e65 6374 696f 6e3a 0a20 2020 2020  onnection:.     
-00000920: 2020 2020 2020 2073 656c 662e 5f63 6f6e         self._con
-00000930: 6e65 6374 696f 6e20 3d20 7365 6c66 2e5f  nection = self._
-00000940: 6372 6561 7465 5f63 6f6e 6e65 6374 696f  create_connectio
-00000950: 6e28 290a 2020 2020 2020 2020 7265 7475  n().        retu
-00000960: 726e 2073 656c 662e 5f63 6f6e 6e65 6374  rn self._connect
-00000970: 696f 6e0a 0a0a 2020 2020 6465 6620 5f63  ion...    def _c
-00000980: 7265 6174 655f 636f 6e6e 6563 7469 6f6e  reate_connection
-00000990: 2873 656c 6629 202d 3e20 545f 436f 6e6e  (self) -> T_Conn
-000009a0: 6563 7469 6f6e 3a0a 2020 2020 2020 2020  ection:.        
-000009b0: 7261 6973 6520 4e6f 7449 6d70 6c65 6d65  raise NotImpleme
-000009c0: 6e74 6564 4572 726f 7228 290a 0a0a 2020  ntedError()...  
-000009d0: 2020 6465 6620 6375 7273 6f72 2873 656c    def cursor(sel
-000009e0: 6629 202d 3e20 545f 4375 7273 6f72 3a0a  f) -> T_Cursor:.
-000009f0: 2020 2020 2020 2020 7265 7475 726e 2073          return s
-00000a00: 656c 662e 636f 6e6e 6563 7469 6f6e 2e63  elf.connection.c
-00000a10: 7572 736f 7228 290a 2020 2020 0a20 2020  ursor().    .   
-00000a20: 200a 2020 2020 6465 6620 6c69 6d69 745f   .    def limit_
-00000a30: 7175 6572 7928 7365 6c66 2c20 7175 6572  query(self, quer
-00000a40: 793a 2073 7472 2c20 6c69 6d69 743a 2069  y: str, limit: i
-00000a50: 6e74 293a 0a20 2020 2020 2020 2069 6620  nt):.        if 
-00000a60: 6c69 6d69 7420 6973 204e 6f6e 653a 0a20  limit is None:. 
-00000a70: 2020 2020 2020 2020 2020 2072 6574 7572             retur
-00000a80: 6e20 7175 6572 790a 2020 2020 0a20 2020  n query.    .   
-00000a90: 2020 2020 2069 6620 6e6f 7420 6973 696e       if not isin
-00000aa0: 7374 616e 6365 286c 696d 6974 2c20 696e  stance(limit, in
-00000ab0: 7429 3a0a 2020 2020 2020 2020 2020 2020  t):.            
-00000ac0: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError
-00000ad0: 2866 2269 6e76 616c 6964 2074 7970 6520  (f"invalid type 
-00000ae0: 666f 7220 6c69 6d69 743a 207b 7479 7065  for limit: {type
-00000af0: 286c 696d 6974 292e 5f5f 6e61 6d65 5f5f  (limit).__name__
-00000b00: 7d20 2865 7870 6563 7465 6420 696e 7429  } (expected int)
-00000b10: 2229 0a20 2020 2020 2020 200a 2020 2020  ").        .    
-00000b20: 2020 2020 696d 706f 7274 2073 716c 7061      import sqlpa
-00000b30: 7273 6520 2320 6e6f 7420 6174 2074 6865  rse # not at the
-00000b40: 2074 6f70 2062 6563 6175 7365 2074 6865   top because the
-00000b50: 2065 6e64 7573 6572 206d 6967 6874 206e   enduser might n
-00000b60: 6f74 206e 6565 6420 7468 6973 2066 6561  ot need this fea
-00000b70: 7475 7265 0a0a 2020 2020 2020 2020 2320  ture..        # 
-00000b80: 5061 7273 6520 5351 4c20 746f 2072 656d  Parse SQL to rem
-00000b90: 6f76 6520 746f 6b65 6e20 6265 666f 7265  ove token before
-00000ba0: 2074 6865 2053 454c 4543 5420 6b65 7977   the SELECT keyw
-00000bb0: 6f72 640a 2020 2020 2020 2020 2320 6578  ord.        # ex
-00000bc0: 616d 706c 653a 2057 4954 4820 2843 5445  ample: WITH (CTE
-00000bd0: 2920 746f 6b65 6e73 0a20 2020 2020 2020  ) tokens.       
-00000be0: 2073 7461 7465 6d65 6e74 7320 3d20 7371   statements = sq
-00000bf0: 6c70 6172 7365 2e70 6172 7365 2871 7565  lparse.parse(que
-00000c00: 7279 290a 2020 2020 2020 2020 6966 206c  ry).        if l
-00000c10: 656e 2873 7461 7465 6d65 6e74 7329 2021  en(statements) !
-00000c20: 3d20 313a 0a20 2020 2020 2020 2020 2020  = 1:.           
-00000c30: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
-00000c40: 7228 6622 7175 6572 7920 636f 6e74 6169  r(f"query contai
-00000c50: 6e73 207b 6c65 6e28 7374 6174 656d 656e  ns {len(statemen
-00000c60: 7473 297d 2073 7461 7465 6d65 6e74 7322  ts)} statements"
-00000c70: 290a 0a20 2020 2020 2020 2023 2047 6574  )..        # Get
-00000c80: 2066 6972 7374 2044 4d4c 206b 6579 776f   first DML keywo
-00000c90: 7264 0a20 2020 2020 2020 2064 6d6c 5f6b  rd.        dml_k
-00000ca0: 6579 776f 7264 203d 204e 6f6e 650a 2020  eyword = None.  
-00000cb0: 2020 2020 2020 646d 6c5f 6b65 7977 6f72        dml_keywor
-00000cc0: 645f 696e 6465 7820 3d20 4e6f 6e65 0a20  d_index = None. 
-00000cd0: 2020 2020 2020 206f 7264 6572 5f62 795f         order_by_
-00000ce0: 696e 6465 7820 3d20 4e6f 6e65 0a20 2020  index = None.   
-00000cf0: 2020 2020 2066 6f72 2069 2c20 746f 6b65       for i, toke
-00000d00: 6e20 696e 2065 6e75 6d65 7261 7465 2873  n in enumerate(s
-00000d10: 7461 7465 6d65 6e74 735b 305d 2e74 6f6b  tatements[0].tok
-00000d20: 656e 7329 3a0a 2020 2020 2020 2020 2020  ens):.          
-00000d30: 2020 6966 2074 6f6b 656e 2e74 7479 7065    if token.ttype
-00000d40: 203d 3d20 7371 6c70 6172 7365 2e74 6f6b   == sqlparse.tok
-00000d50: 656e 732e 444d 4c3a 0a20 2020 2020 2020  ens.DML:.       
-00000d60: 2020 2020 2020 2020 2069 6620 646d 6c5f           if dml_
-00000d70: 6b65 7977 6f72 6420 6973 204e 6f6e 653a  keyword is None:
-00000d80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
-00000d90: 2020 2020 2064 6d6c 5f6b 6579 776f 7264       dml_keyword
-00000da0: 203d 2073 7472 2874 6f6b 656e 292e 7570   = str(token).up
-00000db0: 7065 7228 290a 2020 2020 2020 2020 2020  per().          
-00000dc0: 2020 2020 2020 2020 2020 646d 6c5f 6b65            dml_ke
-00000dd0: 7977 6f72 645f 696e 6465 7820 3d20 690a  yword_index = i.
-00000de0: 2020 2020 2020 2020 2020 2020 656c 6966              elif
-00000df0: 2074 6f6b 656e 2e74 7479 7065 203d 3d20   token.ttype == 
-00000e00: 7371 6c70 6172 7365 2e74 6f6b 656e 732e  sqlparse.tokens.
-00000e10: 4b65 7977 6f72 643a 0a20 2020 2020 2020  Keyword:.       
-00000e20: 2020 2020 2020 2020 2069 6620 6f72 6465           if orde
-00000e30: 725f 6279 5f69 6e64 6578 2069 7320 4e6f  r_by_index is No
-00000e40: 6e65 3a0a 2020 2020 2020 2020 2020 2020  ne:.            
-00000e50: 2020 2020 2020 2020 6b65 7977 6f72 6420          keyword 
-00000e60: 3d20 7374 7228 746f 6b65 6e29 2e75 7070  = str(token).upp
-00000e70: 6572 2829 0a20 2020 2020 2020 2020 2020  er().           
-00000e80: 2020 2020 2020 2020 2069 6620 6b65 7977           if keyw
-00000e90: 6f72 6420 3d3d 2022 4f52 4445 5220 4259  ord == "ORDER BY
-00000ea0: 223a 0a20 2020 2020 2020 2020 2020 2020  ":.             
-00000eb0: 2020 2020 2020 2020 2020 206f 7264 6572             order
-00000ec0: 5f62 795f 696e 6465 7820 3d20 690a 0a20  _by_index = i.. 
-00000ed0: 2020 2020 2020 2023 2043 6865 636b 2069         # Check i
-00000ee0: 6620 7468 6520 444d 4c20 6b65 7977 6f72  f the DML keywor
-00000ef0: 6420 6973 2053 454c 4543 540a 2020 2020  d is SELECT.    
-00000f00: 2020 2020 6966 206e 6f74 2064 6d6c 5f6b      if not dml_k
-00000f10: 6579 776f 7264 3a0a 2020 2020 2020 2020  eyword:.        
-00000f20: 2020 2020 7261 6973 6520 5661 6c75 6545      raise ValueE
-00000f30: 7272 6f72 2866 226e 6f20 5345 4c45 4354  rror(f"no SELECT
-00000f40: 2066 6f75 6e64 2028 7175 6572 7920 646f   found (query do
-00000f50: 6573 206e 6f74 2063 6f6e 7461 696e 2044  es not contain D
-00000f60: 4d4c 206b 6579 776f 7264 2922 290a 2020  ML keyword)").  
-00000f70: 2020 2020 2020 6966 2064 6d6c 5f6b 6579        if dml_key
-00000f80: 776f 7264 2021 3d20 2753 454c 4543 5427  word != 'SELECT'
-00000f90: 3a0a 2020 2020 2020 2020 2020 2020 7261  :.            ra
-00000fa0: 6973 6520 5661 6c75 6545 7272 6f72 2866  ise ValueError(f
-00000fb0: 2266 6972 7374 2044 4d4c 206b 6579 776f  "first DML keywo
-00000fc0: 7264 2069 7320 7b64 6d6c 5f6b 6579 776f  rd is {dml_keywo
-00000fd0: 7264 7d2c 2065 7870 6563 7465 6420 5345  rd}, expected SE
-00000fe0: 4c45 4354 2229 0a0a 2020 2020 2020 2020  LECT")..        
-00000ff0: 2320 4765 7420 7061 7274 2062 6566 6f72  # Get part befor
-00001000: 6520 5345 4c45 4354 2028 6578 616d 706c  e SELECT (exampl
-00001010: 653a 2057 4954 4829 0a20 2020 2020 2020  e: WITH).       
-00001020: 2069 6620 646d 6c5f 6b65 7977 6f72 645f   if dml_keyword_
-00001030: 696e 6465 7820 3e20 303a 0a20 2020 2020  index > 0:.     
-00001040: 2020 2020 2020 2074 6f6b 656e 7320 3d20         tokens = 
-00001050: 7374 6174 656d 656e 7473 5b30 5d2e 746f  statements[0].to
-00001060: 6b65 6e73 5b3a 646d 6c5f 6b65 7977 6f72  kens[:dml_keywor
-00001070: 645f 696e 6465 785d 0a20 2020 2020 2020  d_index].       
-00001080: 2020 2020 206c 696d 6974 6564 5f71 7565       limited_que
-00001090: 7279 203d 2027 272e 6a6f 696e 2873 7472  ry = ''.join(str
-000010a0: 2874 6f6b 656e 2920 666f 7220 746f 6b65  (token) for toke
-000010b0: 6e20 696e 2074 6f6b 656e 7329 0a20 2020  n in tokens).   
-000010c0: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     
-000010d0: 2020 2020 2020 206c 696d 6974 6564 5f71         limited_q
-000010e0: 7565 7279 203d 2027 270a 2020 2020 0a20  uery = ''.    . 
-000010f0: 2020 2020 2020 2023 2041 7070 656e 6420         # Append 
-00001100: 5345 4c45 4354 2062 6566 6f72 6520 4f52  SELECT before OR
-00001110: 4445 5220 4259 0a20 2020 2020 2020 2069  DER BY.        i
-00001120: 6620 6f72 6465 725f 6279 5f69 6e64 6578  f order_by_index
-00001130: 2069 7320 6e6f 7420 4e6f 6e65 3a0a 2020   is not None:.  
-00001140: 2020 2020 2020 2020 2020 746f 6b65 6e73            tokens
-00001150: 203d 2073 7461 7465 6d65 6e74 735b 305d   = statements[0]
-00001160: 2e74 6f6b 656e 735b 646d 6c5f 6b65 7977  .tokens[dml_keyw
-00001170: 6f72 645f 696e 6465 783a 6f72 6465 725f  ord_index:order_
-00001180: 6279 5f69 6e64 6578 5d0a 2020 2020 2020  by_index].      
-00001190: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        
-000011a0: 2020 2020 746f 6b65 6e73 203d 2073 7461      tokens = sta
-000011b0: 7465 6d65 6e74 735b 305d 2e74 6f6b 656e  tements[0].token
-000011c0: 735b 646d 6c5f 6b65 7977 6f72 645f 696e  s[dml_keyword_in
-000011d0: 6465 783a 5d0a 0a20 2020 2020 2020 206c  dex:]..        l
-000011e0: 696d 6974 6564 5f71 7565 7279 202b 3d20  imited_query += 
-000011f0: 7365 6c66 2e5f 6c69 6d69 745f 7061 7273  self._limit_pars
-00001200: 6564 5f71 7565 7279 2827 272e 6a6f 696e  ed_query(''.join
-00001210: 2873 7472 2874 6f6b 656e 2920 666f 7220  (str(token) for 
-00001220: 746f 6b65 6e20 696e 2074 6f6b 656e 7329  token in tokens)
-00001230: 2c20 6c69 6d69 743d 6c69 6d69 7429 0a0a  , limit=limit)..
-00001240: 2020 2020 2020 2020 2320 4170 7065 6e64          # Append
-00001250: 204f 5244 4552 2042 590a 2020 2020 2020   ORDER BY.      
-00001260: 2020 6966 206f 7264 6572 5f62 795f 696e    if order_by_in
-00001270: 6465 7820 6973 206e 6f74 204e 6f6e 653a  dex is not None:
-00001280: 0a20 2020 2020 2020 2020 2020 2074 6f6b  .            tok
-00001290: 656e 7320 3d20 7374 6174 656d 656e 7473  ens = statements
-000012a0: 5b30 5d2e 746f 6b65 6e73 5b6f 7264 6572  [0].tokens[order
-000012b0: 5f62 795f 696e 6465 783a 5d0a 2020 2020  _by_index:].    
-000012c0: 2020 2020 2020 2020 6c69 6d69 7465 645f          limited_
-000012d0: 7175 6572 7920 2b3d 2027 5c6e 2720 2b20  query += '\n' + 
-000012e0: 2727 2e6a 6f69 6e28 7374 7228 746f 6b65  ''.join(str(toke
-000012f0: 6e29 2066 6f72 2074 6f6b 656e 2069 6e20  n) for token in 
-00001300: 746f 6b65 6e73 290a 0a20 2020 2020 2020  tokens)..       
-00001310: 2072 6574 7572 6e20 6c69 6d69 7465 645f   return limited_
-00001320: 7175 6572 790a 2020 2020 0a0a 2020 2020  query.    ..    
-00001330: 6465 6620 5f6c 696d 6974 5f70 6172 7365  def _limit_parse
-00001340: 645f 7175 6572 7928 7365 6c66 2c20 7175  d_query(self, qu
-00001350: 6572 793a 2073 7472 2c20 6c69 6d69 743a  ery: str, limit:
-00001360: 2069 6e74 2920 2d3e 2073 7472 3a0a 2020   int) -> str:.  
-00001370: 2020 2020 2020 7261 6973 6520 4e6f 7449        raise NotI
-00001380: 6d70 6c65 6d65 6e74 6564 4572 726f 7228  mplementedError(
-00001390: 290a 0a0a 2020 2020 6465 6620 6275 696c  )...    def buil
-000013a0: 645f 7175 6572 795f 7769 7468 5f70 6f73  d_query_with_pos
-000013b0: 6974 696f 6e61 6c5f 7061 7261 6d73 2873  itional_params(s
-000013c0: 656c 662c 2071 7565 7279 3a20 7374 722c  elf, query: str,
-000013d0: 2070 6172 616d 733a 206c 6973 747c 7475   params: list|tu
-000013e0: 706c 657c 6469 6374 293a 0a20 2020 2020  ple|dict):.     
-000013f0: 2020 2069 6620 6973 696e 7374 616e 6365     if isinstance
-00001400: 2870 6172 616d 732c 2064 6963 7429 3a0a  (params, dict):.
-00001410: 2020 2020 2020 2020 2020 2020 6672 6f6d              from
-00001420: 2073 716c 7061 7261 6d73 2069 6d70 6f72   sqlparams impor
-00001430: 7420 5351 4c50 6172 616d 7320 2320 6e6f  t SQLParams # no
-00001440: 7420 6174 2074 6865 2074 6f70 2062 6563  t at the top bec
-00001450: 6175 7365 2074 6865 2065 6e64 7573 6572  ause the enduser
-00001460: 206d 6967 6874 206e 6f74 206e 6565 6420   might not need 
-00001470: 7468 6973 2066 6561 7475 7265 0a0a 2020  this feature..  
-00001480: 2020 2020 2020 2020 2020 6966 206e 6f74            if not
-00001490: 2068 6173 6174 7472 2873 656c 662e 5f5f   hasattr(self.__
-000014a0: 636c 6173 735f 5f2c 2027 5f70 6172 616d  class__, '_param
-000014b0: 735f 666f 726d 6174 7465 7227 293a 0a20  s_formatter'):. 
-000014c0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
-000014d0: 656c 662e 5f5f 636c 6173 735f 5f2e 5f70  elf.__class__._p
-000014e0: 6172 616d 735f 666f 726d 6174 7465 7220  arams_formatter 
-000014f0: 3d20 5351 4c50 6172 616d 7328 276e 616d  = SQLParams('nam
-00001500: 6564 272c 2027 716d 6172 6b27 290a 2020  ed', 'qmark').  
-00001510: 2020 2020 2020 2020 2020 7175 6572 792c            query,
-00001520: 2070 6172 616d 7320 3d20 7365 6c66 2e5f   params = self._
-00001530: 5f63 6c61 7373 5f5f 2e5f 7061 7261 6d73  _class__._params
-00001540: 5f66 6f72 6d61 7474 6572 2e66 6f72 6d61  _formatter.forma
-00001550: 7428 7175 6572 792c 2070 6172 616d 7329  t(query, params)
-00001560: 0a0a 2020 2020 2020 2020 7265 7475 726e  ..        return
-00001570: 2071 7565 7279 2c20 7061 7261 6d73 0a20   query, params. 
-00001580: 2020 2020 2020 200a 0a20 2020 2064 6566         ..    def
-00001590: 2067 6574 5f63 7572 736f 725f 636f 6c75   get_cursor_colu
-000015a0: 6d6e 7328 7365 6c66 2c20 6375 7273 6f72  mns(self, cursor
-000015b0: 3a20 545f 4375 7273 6f72 2920 2d3e 206c  : T_Cursor) -> l
-000015c0: 6973 745b 436f 6c75 6d6e 5d3a 0a20 2020  ist[Column]:.   
-000015d0: 2020 2020 2063 6f6c 756d 6e73 203d 205b       columns = [
-000015e0: 5d0a 0a20 2020 2020 2020 2066 6f72 2069  ]..        for i
-000015f0: 2c20 696e 666f 2069 6e20 656e 756d 6572  , info in enumer
-00001600: 6174 6528 6375 7273 6f72 2e64 6573 6372  ate(cursor.descr
-00001610: 6970 7469 6f6e 293a 0a20 2020 2020 2020  iption):.       
-00001620: 2020 2020 206e 616d 652c 2070 7974 686f       name, pytho
-00001630: 6e5f 7479 7065 2c20 6469 7370 6c61 795f  n_type, display_
-00001640: 7369 7a65 2c20 696e 7465 726e 616c 5f73  size, internal_s
-00001650: 697a 652c 2070 7265 6369 7369 6f6e 2c20  ize, precision, 
-00001660: 7363 616c 652c 206e 756c 6c5f 6f6b 203d  scale, null_ok =
-00001670: 2069 6e66 6f0a 2020 2020 2020 2020 2020   info.          
-00001680: 2020 636f 6c75 6d6e 203d 2043 6f6c 756d    column = Colum
-00001690: 6e28 6e61 6d65 2c20 706f 7369 7469 6f6e  n(name, position
-000016a0: 3d69 202b 2031 2c20 7079 7468 6f6e 5f74  =i + 1, python_t
-000016b0: 7970 653d 7079 7468 6f6e 5f74 7970 652c  ype=python_type,
-000016c0: 2064 6973 706c 6179 5f73 697a 653d 6469   display_size=di
-000016d0: 7370 6c61 795f 7369 7a65 2c20 696e 7465  splay_size, inte
-000016e0: 726e 616c 5f73 697a 653d 696e 7465 726e  rnal_size=intern
-000016f0: 616c 5f73 697a 652c 2070 7265 6369 7369  al_size, precisi
-00001700: 6f6e 3d70 7265 6369 7369 6f6e 2c20 7363  on=precision, sc
-00001710: 616c 653d 7363 616c 652c 206e 756c 6c5f  ale=scale, null_
-00001720: 6f6b 3d6e 756c 6c5f 6f6b 290a 2020 2020  ok=null_ok).    
-00001730: 2020 2020 2020 2020 7365 6c66 2e5f 6669          self._fi
-00001740: 785f 6375 7273 6f72 5f63 6f6c 756d 6e5f  x_cursor_column_
-00001750: 6465 6669 6e69 7469 6f6e 2863 6f6c 756d  definition(colum
-00001760: 6e29 0a20 2020 2020 2020 2020 2020 2063  n).            c
-00001770: 6f6c 756d 6e2e 5f63 6865 636b 2829 0a20  olumn._check(). 
-00001780: 2020 2020 2020 2020 2020 2063 6f6c 756d             colum
-00001790: 6e73 2e61 7070 656e 6428 636f 6c75 6d6e  ns.append(column
-000017a0: 290a 2020 2020 2020 2020 0a20 2020 2020  ).        .     
-000017b0: 2020 2072 6574 7572 6e20 636f 6c75 6d6e     return column
-000017c0: 730a 0a0a 2020 2020 6465 6620 5f66 6978  s...    def _fix
-000017d0: 5f63 7572 736f 725f 636f 6c75 6d6e 5f64  _cursor_column_d
-000017e0: 6566 696e 6974 696f 6e28 7365 6c66 2c20  efinition(self, 
-000017f0: 636f 6c75 6d6e 3a20 436f 6c75 6d6e 293a  column: Column):
-00001800: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
-00001810: 2020 2020 2046 6978 2061 2063 6f6c 756d       Fix a colum
-00001820: 6e20 6465 6669 6e69 7469 6f6e 2c20 6166  n definition, af
-00001830: 7465 7220 696e 7374 616e 6369 6174 696f  ter instanciatio
-00001840: 6e20 696e 2060 6765 745f 636f 6c75 6d6e  n in `get_column
-00001850: 735f 6672 6f6d 5f63 7572 736f 7260 2c20  s_from_cursor`, 
-00001860: 666f 7220 6120 7370 6563 6966 6963 2064  for a specific d
-00001870: 6174 6162 6173 6520 656e 6769 6e65 2e0a  atabase engine..
-00001880: 2020 2020 2020 2020 2222 220a 2020 2020          """.    
-00001890: 2020 2020 7061 7373 0a20 2020 2020 2020      pass.       
-000018a0: 200a 0a20 2020 2064 6566 2067 6574 5f63   ..    def get_c
-000018b0: 7572 736f 725f 636f 6c75 6d6e 5f6e 616d  ursor_column_nam
-000018c0: 6573 2873 656c 662c 2063 7572 736f 723a  es(self, cursor:
-000018d0: 2054 5f43 7572 736f 7229 202d 3e20 6c69   T_Cursor) -> li
-000018e0: 7374 5b73 7472 5d3a 0a20 2020 2020 2020  st[str]:.       
-000018f0: 2072 6574 7572 6e20 5b69 6e66 6f5b 305d   return [info[0]
-00001900: 2066 6f72 2069 6e66 6f20 696e 2063 7572   for info in cur
-00001910: 736f 722e 6465 7363 7269 7074 696f 6e5d  sor.description]
-00001920: 0a20 2020 2020 2020 200a 0a20 2020 2064  .        ..    d
-00001930: 6566 2067 6574 5f74 6162 6c65 5f63 6f6c  ef get_table_col
-00001940: 756d 6e73 2873 656c 662c 2074 6162 6c65  umns(self, table
-00001950: 3a20 7374 727c 7475 706c 6529 202d 3e20  : str|tuple) -> 
-00001960: 6c69 7374 5b43 6f6c 756d 6e5d 3a0a 2020  list[Column]:.  
-00001970: 2020 2020 2020 7175 6572 7920 3d20 7365        query = se
-00001980: 6c66 2e67 6574 5f73 656c 6563 745f 7461  lf.get_select_ta
-00001990: 626c 655f 7175 6572 7928 7461 626c 652c  ble_query(table,
-000019a0: 2073 6368 656d 615f 6f6e 6c79 3d54 7275   schema_only=Tru
-000019b0: 6529 0a20 2020 2020 2020 2077 6974 6820  e).        with 
-000019c0: 7365 6c66 2e65 7865 6375 7465 5f67 6574  self.execute_get
-000019d0: 5f63 7572 736f 7228 7175 6572 7929 2061  _cursor(query) a
-000019e0: 7320 6375 7273 6f72 3a0a 2020 2020 2020  s cursor:.      
-000019f0: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
-00001a00: 662e 6765 745f 6375 7273 6f72 5f63 6f6c  f.get_cursor_col
-00001a10: 756d 6e73 2863 7572 736f 7229 0a0a 0a20  umns(cursor)... 
-00001a20: 2020 2064 6566 2067 6574 5f74 6162 6c65     def get_table
-00001a30: 5f63 6f6c 756d 6e5f 6e61 6d65 7328 7365  _column_names(se
-00001a40: 6c66 2c20 7461 626c 653a 2073 7472 7c74  lf, table: str|t
-00001a50: 7570 6c65 2920 2d3e 206c 6973 745b 7374  uple) -> list[st
-00001a60: 725d 3a0a 2020 2020 2020 2020 636f 6c75  r]:.        colu
-00001a70: 6d6e 5f6e 616d 6573 203d 205b 5d0a 2020  mn_names = [].  
-00001a80: 2020 2020 2020 666f 7220 636f 6c75 6d6e        for column
-00001a90: 2069 6e20 7365 6c66 2e67 6574 5f74 6162   in self.get_tab
-00001aa0: 6c65 5f63 6f6c 756d 6e73 2874 6162 6c65  le_columns(table
-00001ab0: 293a 0a20 2020 2020 2020 2020 2020 2063  ):.            c
-00001ac0: 6f6c 756d 6e5f 6e61 6d65 732e 6170 7065  olumn_names.appe
-00001ad0: 6e64 2863 6f6c 756d 6e2e 6e61 6d65 290a  nd(column.name).
-00001ae0: 2020 2020 2020 2020 7265 7475 726e 2063          return c
-00001af0: 6f6c 756d 6e5f 6e61 6d65 730a 0a0a 2020  olumn_names...  
-00001b00: 2020 4063 6c61 7373 6d65 7468 6f64 0a20    @classmethod. 
-00001b10: 2020 2064 6566 2073 706c 6974 5f6e 616d     def split_nam
-00001b20: 6528 636c 732c 2066 756c 6c5f 6e61 6d65  e(cls, full_name
-00001b30: 3a20 7374 727c 7475 706c 6529 202d 3e20  : str|tuple) -> 
-00001b40: 7475 706c 655b 7374 722c 7374 725d 3a0a  tuple[str,str]:.
-00001b50: 2020 2020 2020 2020 6966 2069 7369 6e73          if isins
-00001b60: 7461 6e63 6528 6675 6c6c 5f6e 616d 652c  tance(full_name,
-00001b70: 2074 7570 6c65 293a 0a20 2020 2020 2020   tuple):.       
-00001b80: 2020 2020 2072 6574 7572 6e20 6675 6c6c       return full
-00001b90: 5f6e 616d 650a 2020 2020 2020 2020 0a20  _name.        . 
-00001ba0: 2020 2020 2020 2074 7279 3a0a 2020 2020         try:.    
-00001bb0: 2020 2020 2020 2020 706f 7320 3d20 6675          pos = fu
-00001bc0: 6c6c 5f6e 616d 652e 696e 6465 7828 272e  ll_name.index('.
-00001bd0: 2729 0a20 2020 2020 2020 2020 2020 2073  ').            s
-00001be0: 6368 656d 615f 6e61 6d65 203d 2066 756c  chema_name = ful
-00001bf0: 6c5f 6e61 6d65 5b30 3a70 6f73 5d0a 2020  l_name[0:pos].  
-00001c00: 2020 2020 2020 2020 2020 6e61 6d65 203d            name =
-00001c10: 2066 756c 6c5f 6e61 6d65 5b70 6f73 2b31   full_name[pos+1
-00001c20: 3a5d 0a20 2020 2020 2020 2065 7863 6570  :].        excep
-00001c30: 7420 5661 6c75 6545 7272 6f72 3a0a 2020  t ValueError:.  
-00001c40: 2020 2020 2020 2020 2020 7363 6865 6d61            schema
-00001c50: 5f6e 616d 6520 3d20 636c 732e 6465 6661  _name = cls.defa
-00001c60: 756c 745f 7363 6865 6d61 5f6e 616d 650a  ult_schema_name.
-00001c70: 2020 2020 2020 2020 2020 2020 6e61 6d65              name
-00001c80: 203d 2066 756c 6c5f 6e61 6d65 0a0a 2020   = full_name..  
-00001c90: 2020 2020 2020 7265 7475 726e 2028 7363        return (sc
-00001ca0: 6865 6d61 5f6e 616d 652c 206e 616d 6529  hema_name, name)
-00001cb0: 0a20 2020 200a 0a20 2020 2064 6566 2067  .    ..    def g
-00001cc0: 6574 5f73 656c 6563 745f 7461 626c 655f  et_select_table_
-00001cd0: 7175 6572 7928 7365 6c66 2c20 7461 626c  query(self, tabl
-00001ce0: 653a 2073 7472 7c74 7570 6c65 2c20 7363  e: str|tuple, sc
-00001cf0: 6865 6d61 5f6f 6e6c 7920 3d20 4661 6c73  hema_only = Fals
-00001d00: 6529 3a0a 2020 2020 2020 2020 2222 220a  e):.        """.
-00001d10: 2020 2020 2020 2020 4275 696c 6420 6120          Build a 
-00001d20: 7175 6572 7920 6f6e 2074 6865 2067 6976  query on the giv
-00001d30: 656e 2074 6162 6c65 2e0a 0a20 2020 2020  en table...     
-00001d40: 2020 2049 6620 6073 6368 656d 615f 6f6e     If `schema_on
-00001d50: 6c79 6020 6973 2067 6976 656e 2c20 6e6f  ly` is given, no
-00001d60: 2072 6f77 2077 696c 6c20 6265 2072 6574   row will be ret
-00001d70: 7572 6e65 6420 2874 6869 7320 6973 2075  urned (this is u
-00001d80: 7365 6420 746f 2067 6574 2069 6e66 6f72  sed to get infor
-00001d90: 6d61 7469 6f6e 206f 6e20 7468 6520 7461  mation on the ta
-00001da0: 626c 6529 2e0a 2020 2020 2020 2020 4f74  ble)..        Ot
-00001db0: 6865 7277 6973 652c 2061 6c6c 2072 6f77  herwise, all row
-00001dc0: 7320 7769 6c6c 2062 6520 7265 7475 726e  s will be return
-00001dd0: 6564 2e0a 0a20 2020 2020 2020 2054 6865  ed...        The
-00001de0: 2072 6574 7572 6e20 7479 7065 206f 6620   return type of 
-00001df0: 7468 6973 2066 756e 6374 696f 6e20 6465  this function de
-00001e00: 7065 6e64 7320 6f6e 2074 6865 2064 6174  pends on the dat
-00001e10: 6162 6173 6520 656e 6769 6e65 2e0a 2020  abase engine..  
-00001e20: 2020 2020 2020 4974 2069 7320 7061 7373        It is pass
-00001e30: 6564 2064 6972 6563 746c 7920 746f 2074  ed directly to t
-00001e40: 6865 2063 7572 736f 7227 7320 6578 6563  he cursor's exec
-00001e50: 7574 6520 6675 6e63 7469 6f6e 2066 6f72  ute function for
-00001e60: 2074 6869 7320 656e 6769 6e65 2e0a 2020   this engine..  
-00001e70: 2020 2020 2020 2222 220a 2020 2020 2020        """.      
-00001e80: 2020 7261 6973 6520 4e6f 7449 6d70 6c65    raise NotImple
-00001e90: 6d65 6e74 6564 4572 726f 7228 290a 2020  mentedError().  
-00001ea0: 2020 0a0a 2020 2020 6465 6620 6578 6563    ..    def exec
-00001eb0: 7574 655f 6765 745f 6375 7273 6f72 2873  ute_get_cursor(s
-00001ec0: 656c 662c 2071 7565 7279 3a20 7374 722c  elf, query: str,
-00001ed0: 2070 6172 616d 733a 206c 6973 747c 7475   params: list|tu
-00001ee0: 706c 657c 6469 6374 203d 204e 6f6e 652c  ple|dict = None,
-00001ef0: 206c 696d 6974 3a20 696e 7420 3d20 4e6f   limit: int = No
-00001f00: 6e65 2920 2d3e 2054 5f43 7572 736f 723a  ne) -> T_Cursor:
-00001f10: 0a20 2020 2020 2020 2022 2222 0a20 2020  .        """.   
-00001f20: 2020 2020 204d 7573 7420 6265 2063 6c6f       Must be clo
-00001f30: 7365 642e 0a20 2020 2020 2020 2022 2222  sed..        """
-00001f40: 0a20 2020 2020 2020 2072 6169 7365 204e  .        raise N
-00001f50: 6f74 496d 706c 656d 656e 7465 6445 7272  otImplementedErr
-00001f60: 6f72 2829 0a20 2020 200a 0a20 2020 2064  or().    ..    d
-00001f70: 6566 2065 7865 6375 7465 5f67 6574 5f73  ef execute_get_s
-00001f80: 6361 6c61 7228 7365 6c66 2c20 7175 6572  calar(self, quer
-00001f90: 793a 2073 7472 2c20 7061 7261 6d73 3a20  y: str, params: 
-00001fa0: 6c69 7374 7c74 7570 6c65 7c64 6963 7420  list|tuple|dict 
-00001fb0: 3d20 4e6f 6e65 2c20 6c69 6d69 743a 2069  = None, limit: i
-00001fc0: 6e74 203d 204e 6f6e 6529 3a0a 2020 2020  nt = None):.    
-00001fd0: 2020 2020 7769 7468 2073 656c 662e 6578      with self.ex
-00001fe0: 6563 7574 655f 6765 745f 6375 7273 6f72  ecute_get_cursor
-00001ff0: 2871 7565 7279 2c20 7061 7261 6d73 2c20  (query, params, 
-00002000: 6c69 6d69 743d 6c69 6d69 7429 2061 7320  limit=limit) as 
-00002010: 6375 7273 6f72 3a0a 2020 2020 2020 2020  cursor:.        
-00002020: 2020 2020 7265 7375 6c74 203d 206e 6578      result = nex
-00002030: 7428 6375 7273 6f72 290a 2020 2020 2020  t(cursor).      
-00002040: 2020 2020 2020 0a20 2020 2020 2020 2020        .         
-00002050: 2020 2023 2043 6865 636b 206f 6e6c 7920     # Check only 
-00002060: 6f6e 6520 726f 770a 2020 2020 2020 2020  one row.        
-00002070: 2020 2020 7472 793a 0a20 2020 2020 2020      try:.       
-00002080: 2020 2020 2020 2020 206e 6578 7428 6375           next(cu
-00002090: 7273 6f72 290a 2020 2020 2020 2020 2020  rsor).          
-000020a0: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
-000020b0: 6545 7272 6f72 2866 2273 6576 6572 616c  eError(f"several
-000020c0: 2072 6f77 7320 7265 7475 726e 6564 2062   rows returned b
-000020d0: 7920 7175 6572 7922 290a 2020 2020 2020  y query").      
-000020e0: 2020 2020 2020 6578 6365 7074 2053 746f        except Sto
-000020f0: 7049 7465 7261 7469 6f6e 3a0a 2020 2020  pIteration:.    
-00002100: 2020 2020 2020 2020 2020 2020 7061 7373              pass
-00002110: 0a0a 2020 2020 2020 2020 2020 2020 2320  ..            # 
-00002120: 4368 6563 6b20 6f6e 6c79 206f 6e65 2076  Check only one v
-00002130: 616c 7565 0a20 2020 2020 2020 2020 2020  alue.           
-00002140: 2069 6620 6c65 6e28 7265 7375 6c74 2920   if len(result) 
-00002150: 3e20 313a 0a20 2020 2020 2020 2020 2020  > 1:.           
-00002160: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
-00002170: 4572 726f 7228 2273 6576 6572 616c 2076  Error("several v
-00002180: 616c 7565 7320 7265 7475 726e 6564 2062  alues returned b
-00002190: 7920 7175 6572 7922 290a 0a20 2020 2020  y query")..     
-000021a0: 2020 2020 2020 2072 6574 7572 6e20 7265         return re
-000021b0: 7375 6c74 5b30 5d0a 0a0a 2020 2020 6465  sult[0]...    de
-000021c0: 6620 6578 6563 7574 6528 7365 6c66 2c20  f execute(self, 
-000021d0: 7175 6572 793a 2073 7472 2c20 7061 7261  query: str, para
-000021e0: 6d73 3a20 6c69 7374 7c74 7570 6c65 7c64  ms: list|tuple|d
-000021f0: 6963 7420 3d20 4e6f 6e65 2c20 6c69 6d69  ict = None, limi
-00002200: 743a 2069 6e74 203d 204e 6f6e 6529 3a0a  t: int = None):.
-00002210: 2020 2020 2020 2020 7769 7468 2073 656c          with sel
-00002220: 662e 6578 6563 7574 655f 6765 745f 6375  f.execute_get_cu
-00002230: 7273 6f72 2871 7565 7279 2c20 7061 7261  rsor(query, para
-00002240: 6d73 2c20 6c69 6d69 743d 6c69 6d69 7429  ms, limit=limit)
-00002250: 3a0a 2020 2020 2020 2020 2020 2020 7061  :.            pa
-00002260: 7373 0a0a 0a20 2020 2064 6566 2065 7865  ss...    def exe
-00002270: 6375 7465 5f66 696c 6528 7365 6c66 2c20  cute_file(self, 
-00002280: 7061 7468 3a20 7374 727c 5061 7468 2c20  path: str|Path, 
-00002290: 7061 7261 6d73 3a20 6c69 7374 7c74 7570  params: list|tup
-000022a0: 6c65 7c64 6963 7420 3d20 4e6f 6e65 2c20  le|dict = None, 
-000022b0: 6c69 6d69 743a 2069 6e74 203d 204e 6f6e  limit: int = Non
-000022c0: 6529 3a0a 2020 2020 2020 2020 6966 206e  e):.        if n
-000022d0: 6f74 2069 7369 6e73 7461 6e63 6528 7061  ot isinstance(pa
-000022e0: 7468 2c20 5061 7468 293a 0a20 2020 2020  th, Path):.     
-000022f0: 2020 2020 2020 2070 6174 6820 3d20 5061         path = Pa
-00002300: 7468 2870 6174 6829 0a20 2020 2020 2020  th(path).       
-00002310: 2020 2020 200a 2020 2020 2020 2020 7175       .        qu
-00002320: 6572 7920 3d20 7061 7468 2e72 6561 645f  ery = path.read_
-00002330: 7465 7874 2865 6e63 6f64 696e 673d 2775  text(encoding='u
-00002340: 7466 2d38 2729 0a20 2020 2020 2020 2073  tf-8').        s
-00002350: 656c 662e 6578 6563 7574 6528 7175 6572  elf.execute(quer
-00002360: 792c 2070 6172 616d 732c 206c 696d 6974  y, params, limit
-00002370: 3d6c 696d 6974 290a 0a0a 2020 2020 6465  =limit)...    de
-00002380: 6620 6361 6c6c 5f70 726f 6365 6475 7265  f call_procedure
-00002390: 2873 656c 662c 206e 616d 653a 2073 7472  (self, name: str
-000023a0: 7c74 7570 6c65 2c20 2a61 7267 7329 3a0a  |tuple, *args):.
-000023b0: 2020 2020 2020 2020 7261 6973 6520 4e6f          raise No
-000023c0: 7449 6d70 6c65 6d65 6e74 6564 4572 726f  tImplementedErro
-000023d0: 7228 290a 0a0a 2020 2020 6465 6620 7461  r()...    def ta
-000023e0: 626c 655f 6578 6973 7473 2873 656c 662c  ble_exists(self,
-000023f0: 2074 6162 6c65 3a20 7374 727c 7475 706c   table: str|tupl
-00002400: 6529 202d 3e20 626f 6f6c 3a0a 2020 2020  e) -> bool:.    
-00002410: 2020 2020 7261 6973 6520 4e6f 7449 6d70      raise NotImp
-00002420: 6c65 6d65 6e74 6564 4572 726f 7228 290a  lementedError().
-00002430: 0a0a 2020 2020 6465 6620 7472 756e 6361  ..    def trunca
-00002440: 7465 5f74 6162 6c65 2873 656c 662c 2074  te_table(self, t
-00002450: 6162 6c65 3a20 7374 727c 7475 706c 6529  able: str|tuple)
-00002460: 3a0a 2020 2020 2020 2020 7261 6973 6520  :.        raise 
-00002470: 4e6f 7449 6d70 6c65 6d65 6e74 6564 4572  NotImplementedEr
-00002480: 726f 7228 290a 0a0a 2020 2020 6465 6620  ror()...    def 
-00002490: 636f 7079 5f66 726f 6d5f 6373 7628 7365  copy_from_csv(se
-000024a0: 6c66 2c20 6670 3a20 494f 4261 7365 2c20  lf, fp: IOBase, 
-000024b0: 7461 626c 653a 2073 7472 7c74 7570 6c65  table: str|tuple
-000024c0: 2c20 636f 6c75 6d6e 733a 206c 6973 745b  , columns: list[
-000024d0: 7374 725d 203d 204e 6f6e 652c 2064 656c  str] = None, del
-000024e0: 696d 6974 6572 3a20 7374 7220 3d20 4e6f  imiter: str = No
-000024f0: 6e65 2c20 7175 6f74 6563 6861 723a 2073  ne, quotechar: s
-00002500: 7472 203d 2027 2227 2c20 6e75 6c6c 6368  tr = '"', nullch
-00002510: 6172 3a20 7374 7220 3d20 2727 2c20 6e6f  ar: str = '', no
-00002520: 6865 6164 6572 3a20 626f 6f6c 203d 2046  header: bool = F
-00002530: 616c 7365 293a 0a20 2020 2020 2020 2072  alse):.        r
-00002540: 6169 7365 204e 6f74 496d 706c 656d 656e  aise NotImplemen
-00002550: 7465 6445 7272 6f72 2829 0a20 2020 200a  tedError().    .
-00002560: 0a20 2020 2064 6566 2064 6570 6c6f 795f  .    def deploy_
-00002570: 7371 6c28 7365 6c66 2c20 2a70 6174 6873  sql(self, *paths
-00002580: 3a20 5061 7468 7c73 7472 2c20 656e 636f  : Path|str, enco
-00002590: 6469 6e67 203d 2022 7574 662d 3822 293a  ding = "utf-8"):
-000025a0: 0a20 2020 2020 2020 2061 6374 7561 6c5f  .        actual_
-000025b0: 7061 7468 733a 206c 6973 745b 5061 7468  paths: list[Path
-000025c0: 5d20 3d20 5b5d 0a20 2020 2020 2020 2066  ] = [].        f
-000025d0: 6f72 2070 6174 6820 696e 2070 6174 6873  or path in paths
-000025e0: 3a0a 2020 2020 2020 2020 2020 2020 6966  :.            if
-000025f0: 2069 7369 6e73 7461 6e63 6528 7061 7468   isinstance(path
-00002600: 2c20 7374 7229 3a0a 2020 2020 2020 2020  , str):.        
-00002610: 2020 2020 2020 2020 7061 7468 203d 2050          path = P
-00002620: 6174 6828 7061 7468 290a 2020 2020 2020  ath(path).      
-00002630: 2020 2020 2020 6163 7475 616c 5f70 6174        actual_pat
-00002640: 6873 2e61 7070 656e 6428 7061 7468 290a  hs.append(path).
-00002650: 0a20 2020 2020 2020 2061 6374 7561 6c5f  .        actual_
-00002660: 7061 7468 732e 736f 7274 2829 0a0a 2020  paths.sort()..  
-00002670: 2020 2020 2020 666f 7220 7061 7468 2069        for path i
-00002680: 6e20 6163 7475 616c 5f70 6174 6873 3a0a  n actual_paths:.
-00002690: 2020 2020 2020 2020 2020 2020 6966 2070              if p
-000026a0: 6174 682e 6973 5f64 6972 2829 3a0a 2020  ath.is_dir():.  
-000026b0: 2020 2020 2020 2020 2020 2020 2020 7375                su
-000026c0: 6270 6174 6873 203d 2073 6f72 7465 6428  bpaths = sorted(
-000026d0: 7061 7468 2e69 7465 7264 6972 2829 290a  path.iterdir()).
-000026e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000026f0: 7365 6c66 2e64 6570 6c6f 795f 7371 6c28  self.deploy_sql(
-00002700: 2a73 7562 7061 7468 732c 2065 6e63 6f64  *subpaths, encod
-00002710: 696e 673d 656e 636f 6469 6e67 290a 0a20  ing=encoding).. 
-00002720: 2020 2020 2020 2020 2020 2065 6c69 6620             elif 
-00002730: 6e6f 7420 7061 7468 2e6e 616d 652e 656e  not path.name.en
-00002740: 6473 7769 7468 2822 2e73 716c 2229 3a0a  dswith(".sql"):.
-00002750: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00002760: 636f 6e74 696e 7565 2023 2069 676e 6f72  continue # ignor
-00002770: 650a 0a20 2020 2020 2020 2020 2020 2065  e..            e
-00002780: 6c69 6620 7061 7468 2e6e 616d 652e 656e  lif path.name.en
-00002790: 6473 7769 7468 2822 5f72 6576 6572 742e  dswith("_revert.
-000027a0: 7371 6c22 293a 0a20 2020 2020 2020 2020  sql"):.         
-000027b0: 2020 2020 2020 2063 6f6e 7469 6e75 6520         continue 
-000027c0: 2320 6967 6e6f 7265 0a0a 2020 2020 2020  # ignore..      
-000027d0: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    
-000027e0: 2020 2020 2020 2020 2020 2020 6c6f 6767              logg
-000027f0: 6572 2e69 6e66 6f28 2265 7865 6375 7465  er.info("execute
-00002800: 2025 7322 2c20 7061 7468 290a 2020 2020   %s", path).    
-00002810: 2020 2020 2020 2020 2020 2020 7175 6572              quer
-00002820: 7920 3d20 7061 7468 2e72 6561 645f 7465  y = path.read_te
-00002830: 7874 2865 6e63 6f64 696e 673d 656e 636f  xt(encoding=enco
-00002840: 6469 6e67 290a 2020 2020 2020 2020 2020  ding).          
-00002850: 2020 2020 2020 7365 6c66 2e65 7865 6375        self.execu
-00002860: 7465 2871 7565 7279 290a 0a0a 2020 2020  te(query)...    
-00002870: 6465 6620 7265 7665 7274 5f73 716c 2873  def revert_sql(s
-00002880: 656c 662c 202a 7061 7468 733a 2050 6174  elf, *paths: Pat
-00002890: 687c 7374 722c 2065 6e63 6f64 696e 6720  h|str, encoding 
-000028a0: 3d20 2275 7466 2d38 2229 3a0a 2020 2020  = "utf-8"):.    
-000028b0: 2020 2020 6163 7475 616c 5f70 6174 6873      actual_paths
-000028c0: 3a20 6c69 7374 5b50 6174 685d 203d 205b  : list[Path] = [
-000028d0: 5d0a 2020 2020 2020 2020 666f 7220 7061  ].        for pa
-000028e0: 7468 2069 6e20 7061 7468 733a 0a20 2020  th in paths:.   
-000028f0: 2020 2020 2020 2020 2069 6620 6973 696e           if isin
-00002900: 7374 616e 6365 2870 6174 682c 2073 7472  stance(path, str
-00002910: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             
-00002920: 2020 2070 6174 6820 3d20 5061 7468 2870     path = Path(p
-00002930: 6174 6829 0a20 2020 2020 2020 2020 2020  ath).           
-00002940: 2061 6374 7561 6c5f 7061 7468 732e 6170   actual_paths.ap
-00002950: 7065 6e64 2870 6174 6829 0a0a 2020 2020  pend(path)..    
-00002960: 2020 2020 6163 7475 616c 5f70 6174 6873      actual_paths
-00002970: 2e73 6f72 7428 7265 7665 7273 653d 5472  .sort(reverse=Tr
-00002980: 7565 290a 0a20 2020 2020 2020 2066 6f72  ue)..        for
-00002990: 2070 6174 6820 696e 2061 6374 7561 6c5f   path in actual_
-000029a0: 7061 7468 733a 0a20 2020 2020 2020 2020  paths:.         
-000029b0: 2020 2069 6620 7061 7468 2e69 735f 6469     if path.is_di
-000029c0: 7228 293a 0a20 2020 2020 2020 2020 2020  r():.           
-000029d0: 2020 2020 2073 7562 7061 7468 7320 3d20       subpaths = 
-000029e0: 736f 7274 6564 2870 6174 682e 6974 6572  sorted(path.iter
-000029f0: 6469 7228 2929 0a20 2020 2020 2020 2020  dir()).         
-00002a00: 2020 2020 2020 2073 656c 662e 7265 7665         self.reve
-00002a10: 7274 5f73 716c 282a 7375 6270 6174 6873  rt_sql(*subpaths
-00002a20: 2c20 656e 636f 6469 6e67 3d65 6e63 6f64  , encoding=encod
-00002a30: 696e 6729 0a0a 2020 2020 2020 2020 2020  ing)..          
-00002a40: 2020 656c 6966 206e 6f74 2070 6174 682e    elif not path.
-00002a50: 6e61 6d65 2e65 6e64 7377 6974 6828 225f  name.endswith("_
-00002a60: 7265 7665 7274 2e73 716c 2229 3a0a 2020  revert.sql"):.  
-00002a70: 2020 2020 2020 2020 2020 2020 2020 636f                co
-00002a80: 6e74 696e 7565 2023 2069 676e 6f72 650a  ntinue # ignore.
-00002a90: 0a20 2020 2020 2020 2020 2020 2065 6c73  .            els
-00002aa0: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             
-00002ab0: 2020 206c 6f67 6765 722e 696e 666f 2822     logger.info("
-00002ac0: 6578 6563 7574 6520 2573 222c 2070 6174  execute %s", pat
-00002ad0: 6829 0a20 2020 2020 2020 2020 2020 2020  h).             
-00002ae0: 2020 2071 7565 7279 203d 2070 6174 682e     query = path.
-00002af0: 7265 6164 5f74 6578 7428 656e 636f 6469  read_text(encodi
-00002b00: 6e67 3d65 6e63 6f64 696e 6729 0a20 2020  ng=encoding).   
-00002b10: 2020 2020 2020 2020 2020 2020 2073 656c               sel
-00002b20: 662e 6578 6563 7574 6528 7175 6572 7929  f.execute(query)
-00002b30: 0a0a 0a63 6c61 7373 2043 6f6c 756d 6e3a  ...class Column:
-00002b40: 0a20 2020 2064 6566 205f 5f69 6e69 745f  .    def __init_
-00002b50: 5f28 7365 6c66 2c20 6e61 6d65 3a20 7374  _(self, name: st
-00002b60: 722c 202a 2c20 706f 7369 7469 6f6e 3a20  r, *, position: 
-00002b70: 696e 7420 3d20 4e6f 6e65 2c20 7079 7468  int = None, pyth
-00002b80: 6f6e 5f74 7970 653a 2074 7970 6520 3d20  on_type: type = 
-00002b90: 4e6f 6e65 2c20 6f72 6967 696e 616c 5f74  None, original_t
-00002ba0: 7970 653a 2073 7472 203d 204e 6f6e 652c  ype: str = None,
-00002bb0: 2070 7265 6369 7369 6f6e 3a20 696e 7420   precision: int 
-00002bc0: 3d20 4e6f 6e65 2c20 7363 616c 653a 2069  = None, scale: i
-00002bd0: 6e74 203d 204e 6f6e 652c 206e 756c 6c5f  nt = None, null_
-00002be0: 6f6b 3a20 626f 6f6c 203d 204e 6f6e 652c  ok: bool = None,
-00002bf0: 2064 6973 706c 6179 5f73 697a 653a 2069   display_size: i
-00002c00: 6e74 203d 204e 6f6e 652c 2069 6e74 6572  nt = None, inter
-00002c10: 6e61 6c5f 7369 7a65 3a20 696e 7420 3d20  nal_size: int = 
-00002c20: 4e6f 6e65 2c20 7072 696d 6172 795f 6b65  None, primary_ke
-00002c30: 793a 2062 6f6f 6c20 3d20 4661 6c73 652c  y: bool = False,
-00002c40: 2064 6566 6175 6c74 3a20 7374 727c 4c69   default: str|Li
-00002c50: 7465 7261 6c28 275f 5f6e 6f77 5f5f 2729  teral('__now__')
-00002c60: 203d 204e 6f6e 6529 3a0a 2020 2020 2020   = None):.      
-00002c70: 2020 7365 6c66 2e6e 616d 653a 2073 7472    self.name: str
-00002c80: 7c4e 6f6e 6520 3d20 6e61 6d65 0a20 2020  |None = name.   
-00002c90: 2020 2020 2022 2222 204e 6f6d 2064 6520       """ Nom de 
-00002ca0: 6c61 2063 6f6c 6f6e 6e65 2e20 5065 7574  la colonne. Peut
-00002cb0: 20c3 aa74 7265 2076 6964 6520 2870 6172   ..tre vide (par
-00002cc0: 2065 7865 6d70 6c65 2070 6f75 7220 6c65   exemple pour le
-00002cd0: 7320 636f 6c6f 6e6e 6573 2063 616c 6375  s colonnes calcu
-00002ce0: 6cc3 a965 7320 7361 6e73 2061 6c69 6173  l..es sans alias
-00002cf0: 2064 c3a9 6669 6e69 292e 2022 2222 0a0a   d..fini). """..
-00002d00: 2020 2020 2020 2020 7365 6c66 2e70 6f73          self.pos
-00002d10: 6974 696f 6e3a 2069 6e74 7c4e 6f6e 6520  ition: int|None 
-00002d20: 3d20 706f 7369 7469 6f6e 0a20 2020 2020  = position.     
-00002d30: 2020 2022 2222 204f 7264 696e 616c 2070     """ Ordinal p
-00002d40: 6f73 6974 696f 6e2c 2073 7461 7274 696e  osition, startin
-00002d50: 6720 6672 6f6d 2031 2e20 2222 220a 0a20  g from 1. """.. 
-00002d60: 2020 2020 2020 2073 656c 662e 7079 7468         self.pyth
-00002d70: 6f6e 5f74 7970 653a 2074 7970 657c 4e6f  on_type: type|No
-00002d80: 6e65 203d 2070 7974 686f 6e5f 7479 7065  ne = python_type
-00002d90: 0a20 2020 2020 2020 2022 2222 2054 7970  .        """ Typ
-00002da0: 6520 5079 7468 6f6e 2075 7469 6c69 73c3  e Python utilis.
-00002db0: a920 7061 7220 6c65 2064 7269 7665 7220  . par le driver 
-00002dc0: 706f 7572 2072 6570 72c3 a973 656e 7465  pour repr..sente
-00002dd0: 7220 6c65 7320 7661 6c65 7572 732e 0a20  r les valeurs.. 
-00002de0: 2020 2020 2020 2041 7474 656e 7469 6f6e         Attention
-00002df0: 203a 206c 6573 2076 616c 6575 7273 2064   : les valeurs d
-00002e00: 6520 7479 7065 2053 514c 2044 4154 4520  e type SQL DATE 
-00002e10: 6f75 2044 4154 4554 494d 4520 7065 7576  ou DATETIME peuv
-00002e20: 656e 7420 c3aa 7472 6520 6d61 7070 c3a9  ent ..tre mapp..
-00002e30: 6573 2061 7520 7479 7065 2050 7974 686f  es au type Pytho
-00002e40: 6e20 6073 7472 6020 7375 6976 616e 7420  n `str` suivant 
-00002e50: 6c65 2064 7269 7665 7220 7574 696c 6973  le driver utilis
-00002e60: c3a9 2e20 2222 220a 2020 2020 2020 2020  ... """.        
-00002e70: 2320 4366 2e20 6874 7470 733a 2f2f 7374  # Cf. https://st
-00002e80: 6163 6b6f 7665 7266 6c6f 772e 636f 6d2f  ackoverflow.com/
-00002e90: 7175 6573 7469 6f6e 732f 3731 3732 3534  questions/717254
-00002ea0: 302f 7079 6f64 6263 2d72 6574 7572 6e73  0/pyodbc-returns
-00002eb0: 2d73 716c 2d73 6572 7665 722d 6461 7465  -sql-server-date
-00002ec0: 2d66 6965 6c64 732d 6173 2d73 7472 696e  -fields-as-strin
-00002ed0: 6773 0a0a 2020 2020 2020 2020 7365 6c66  gs..        self
-00002ee0: 2e6f 7269 6769 6e61 6c5f 7479 7065 3a20  .original_type: 
-00002ef0: 7374 727c 4e6f 6e65 203d 206f 7269 6769  str|None = origi
-00002f00: 6e61 6c5f 7479 7065 0a0a 2020 2020 2020  nal_type..      
-00002f10: 2020 7365 6c66 2e70 7265 6369 7369 6f6e    self.precision
-00002f20: 3a20 696e 747c 4e6f 6e65 203d 2070 7265  : int|None = pre
-00002f30: 6369 7369 6f6e 0a20 2020 2020 2020 2022  cision.        "
-00002f40: 2222 2054 6f74 616c 206e 756d 6265 7220  "" Total number 
-00002f50: 6f66 2073 6967 6e69 6669 6361 6e74 2064  of significant d
-00002f60: 6967 6974 732e 0a20 2020 2020 2020 202d  igits..        -
-00002f70: 206d 7373 716c 3a20 616c 7761 7973 2073   mssql: always s
-00002f80: 6574 2e20 2222 220a 2020 2020 2020 2020  et. """.        
-00002f90: 0a20 2020 2020 2020 2073 656c 662e 7363  .        self.sc
-00002fa0: 616c 653a 2069 6e74 7c4e 6f6e 6520 3d20  ale: int|None = 
-00002fb0: 7363 616c 650a 2020 2020 2020 2020 2222  scale.        ""
-00002fc0: 2220 436f 756e 7420 6f66 2064 6563 696d  " Count of decim
-00002fd0: 616c 2064 6967 6974 7320 696e 2074 6865  al digits in the
-00002fe0: 2066 7261 6374 696f 6e61 6c20 7061 7274   fractional part
-00002ff0: 206f 6620 7468 6520 7479 7065 2e0a 2020   of the type..  
-00003000: 2020 2020 2020 2d20 6d73 7371 6c3a 2061        - mssql: a
-00003010: 6c77 6179 7320 7365 742c 2075 7369 6e67  lways set, using
-00003020: 2030 2069 6620 6e6f 6e65 2e20 2222 220a   0 if none. """.
-00003030: 0a20 2020 2020 2020 2073 656c 662e 6e75  .        self.nu
-00003040: 6c6c 5f6f 6b3a 2062 6f6f 6c7c 4e6f 6e65  ll_ok: bool|None
-00003050: 203d 206e 756c 6c5f 6f6b 0a20 2020 2020   = null_ok.     
-00003060: 2020 2022 2222 2049 6e64 6963 6174 6520     """ Indicate 
-00003070: 6966 2074 6865 2063 6f6c 756d 6e20 6163  if the column ac
-00003080: 6365 7074 7320 6e75 6c6c 2076 616c 7565  cepts null value
-00003090: 2e0a 2020 2020 2020 2020 2d20 7067 3a20  ..        - pg: 
-000030a0: 616c 7761 7973 204e 6f6e 6520 6173 206e  always None as n
-000030b0: 6f74 2065 6173 7920 746f 2072 6574 7269  ot easy to retri
-000030c0: 6576 6520 6672 6f6d 2074 6865 206c 6962  eve from the lib
-000030d0: 7071 2e0a 2020 2020 2020 2020 2222 220a  pq..        """.
-000030e0: 0a20 2020 2020 2020 2073 656c 662e 6469  .        self.di
-000030f0: 7370 6c61 795f 7369 7a65 3a20 696e 747c  splay_size: int|
-00003100: 4e6f 6e65 203d 2064 6973 706c 6179 5f73  None = display_s
-00003110: 697a 650a 2020 2020 2020 2020 2222 2220  ize.        """ 
-00003120: 5468 6520 6163 7475 616c 206c 656e 6774  The actual lengt
-00003130: 6820 6f66 2074 6865 2063 6f6c 756d 6e20  h of the column 
-00003140: 696e 2062 7974 6573 2e0a 2020 2020 2020  in bytes..      
-00003150: 2020 2d20 6d73 7371 6c3a 2061 6c77 6179    - mssql: alway
-00003160: 7320 4e6f 6e65 2028 6f72 6967 696e 616c  s None (original
-00003170: 6c79 2061 6c77 6179 7320 7a65 726f 292e  ly always zero).
-00003180: 2022 2222 0a0a 2020 2020 2020 2020 7365   """..        se
-00003190: 6c66 2e69 6e74 6572 6e61 6c5f 7369 7a65  lf.internal_size
-000031a0: 3a20 696e 747c 4e6f 6e65 203d 2069 6e74  : int|None = int
-000031b0: 6572 6e61 6c5f 7369 7a65 0a20 2020 2020  ernal_size.     
-000031c0: 2020 2022 2222 2054 6865 2073 697a 6520     """ The size 
-000031d0: 696e 2062 7974 6573 206f 6620 7468 6520  in bytes of the 
-000031e0: 636f 6c75 6d6e 2061 7373 6f63 6961 7465  column associate
-000031f0: 6420 746f 2074 6869 7320 636f 6c75 6d6e  d to this column
-00003200: 206f 6e20 7468 6520 7365 7276 6572 2e0a   on the server..
-00003210: 2020 2020 2020 2020 2d20 6d73 7371 6c3a          - mssql:
-00003220: 2061 6c77 6179 7320 4e6f 6e65 2028 6f72   always None (or
-00003230: 6967 696e 616c 6c79 2061 6c77 6179 7320  iginally always 
-00003240: 7361 6d65 2061 7320 7072 6563 6973 696f  same as precisio
-00003250: 6e29 2e20 2222 220a 0a20 2020 2020 2020  n). """..       
-00003260: 2073 656c 662e 7072 696d 6172 795f 6b65   self.primary_ke
-00003270: 793a 2062 6f6f 6c20 3d20 7072 696d 6172  y: bool = primar
-00003280: 795f 6b65 790a 0a20 2020 2020 2020 2073  y_key..        s
-00003290: 656c 662e 6465 6661 756c 743a 2073 7472  elf.default: str
-000032a0: 7c4c 6974 6572 616c 2827 5f5f 6e6f 775f  |Literal('__now_
-000032b0: 5f27 297c 4e6f 6e65 203d 2064 6566 6175  _')|None = defau
-000032c0: 6c74 0a0a 0a20 2020 2064 6566 205f 6368  lt...    def _ch
-000032d0: 6563 6b28 7365 6c66 293a 0a20 2020 2020  eck(self):.     
-000032e0: 2020 2022 2222 0a20 2020 2020 2020 2045     """.        E
-000032f0: 6e73 7572 6520 636f 6e73 6973 7465 6e63  nsure consistenc
-00003300: 7920 6f66 2063 6f6c 756d 6e20 6465 6669  y of column defi
-00003310: 6e69 7469 6f6e 2e0a 0a20 2020 2020 2020  nition...       
-00003320: 2043 616c 6c65 6420 6166 7465 7220 6372   Called after cr
-00003330: 6561 7469 6f6e 206f 6620 436f 6c75 6d6e  eation of Column
-00003340: 2069 6e73 7461 6e63 6520 616e 6420 706f   instance and po
-00003350: 7465 6e74 6961 6c20 6669 7820 6164 6170  tential fix adap
-00003360: 7465 6420 746f 2073 7065 6369 6669 6320  ted to specific 
-00003370: 4461 7461 6261 7365 2065 6e67 696e 6573  Database engines
-00003380: 2e0a 2020 2020 2020 2020 2222 220a 2020  ..        """.  
-00003390: 2020 2020 2020 6966 2073 656c 662e 7079        if self.py
-000033a0: 7468 6f6e 5f74 7970 6520 6973 206e 6f74  thon_type is not
-000033b0: 204e 6f6e 6520 616e 6420 6e6f 7420 6973   None and not is
-000033c0: 696e 7374 616e 6365 2873 656c 662e 7079  instance(self.py
-000033d0: 7468 6f6e 5f74 7970 652c 2074 7970 6529  thon_type, type)
-000033e0: 3a0a 2020 2020 2020 2020 2020 2020 7261  :.            ra
-000033f0: 6973 6520 5661 6c75 6545 7272 6f72 2866  ise ValueError(f
-00003400: 2269 6e76 616c 6964 2070 7974 686f 6e5f  "invalid python_
-00003410: 7479 7065 205c 227b 7365 6c66 2e70 7974  type \"{self.pyt
-00003420: 686f 6e5f 7479 7065 7d5c 2220 287b 7479  hon_type}\" ({ty
-00003430: 7065 2873 656c 662e 7079 7468 6f6e 5f74  pe(self.python_t
-00003440: 7970 6529 2e5f 5f6e 616d 655f 5f7d 293a  ype).__name__}):
-00003450: 2065 7870 6563 7465 6420 7479 7065 2069   expected type i
-00003460: 6e73 7461 6e63 6522 290a 0a0a 2020 2020  nstance")...    
-00003470: 6465 6620 5f5f 7374 725f 5f28 7365 6c66  def __str__(self
-00003480: 2920 2d3e 2073 7472 3a0a 2020 2020 2020  ) -> str:.      
-00003490: 2020 7265 7475 726e 2073 656c 662e 6e61    return self.na
-000034a0: 6d65 0a                                  me.
+00000000: 0d0a 6672 6f6d 205f 5f66 7574 7572 655f  ..from __future_
+00000010: 5f20 696d 706f 7274 2061 6e6e 6f74 6174  _ import annotat
+00000020: 696f 6e73 0d0a 6672 6f6d 2064 6174 6574  ions..from datet
+00000030: 696d 6520 696d 706f 7274 2074 696d 657a  ime import timez
+00000040: 6f6e 650d 0a66 726f 6d20 696f 2069 6d70  one..from io imp
+00000050: 6f72 7420 494f 4261 7365 0d0a 696d 706f  ort IOBase..impo
+00000060: 7274 206c 6f67 6769 6e67 0d0a 6672 6f6d  rt logging..from
+00000070: 2070 6174 686c 6962 2069 6d70 6f72 7420   pathlib import 
+00000080: 5061 7468 0d0a 696d 706f 7274 2072 650d  Path..import re.
+00000090: 0a66 726f 6d20 7479 7069 6e67 2069 6d70  .from typing imp
+000000a0: 6f72 7420 416e 792c 2047 656e 6572 6963  ort Any, Generic
+000000b0: 2c20 5479 7065 5661 720d 0a66 726f 6d20  , TypeVar..from 
+000000c0: 7572 6c6c 6962 2e70 6172 7365 2069 6d70  urllib.parse imp
+000000d0: 6f72 7420 7175 6f74 650d 0a66 726f 6d20  ort quote..from 
+000000e0: 2e2e 6d69 7363 2069 6d70 6f72 7420 4c69  ..misc import Li
+000000f0: 7465 7261 6c0d 0a0d 0a6c 6f67 6765 7220  teral....logger 
+00000100: 3d20 6c6f 6767 696e 672e 6765 744c 6f67  = logging.getLog
+00000110: 6765 7228 5f5f 6e61 6d65 5f5f 290d 0a0d  ger(__name__)...
+00000120: 0a54 5f43 6f6e 6e65 6374 696f 6e20 3d20  .T_Connection = 
+00000130: 5479 7065 5661 7228 2754 5f43 6f6e 6e65  TypeVar('T_Conne
+00000140: 6374 696f 6e27 290d 0a54 5f43 7572 736f  ction')..T_Curso
+00000150: 7220 3d20 5479 7065 5661 7228 2754 5f43  r = TypeVar('T_C
+00000160: 7572 736f 7227 290d 0a0d 0a63 6c61 7373  ursor')....class
+00000170: 2044 6257 7261 7070 6572 2847 656e 6572   DbWrapper(Gener
+00000180: 6963 5b54 5f43 6f6e 6e65 6374 696f 6e2c  ic[T_Connection,
+00000190: 2054 5f43 7572 736f 725d 293a 0d0a 2020   T_Cursor]):..  
+000001a0: 2020 4063 6c61 7373 6d65 7468 6f64 0d0a    @classmethod..
+000001b0: 2020 2020 6465 6620 6973 5f61 7661 696c      def is_avail
+000001c0: 6162 6c65 2863 6c73 293a 0d0a 2020 2020  able(cls):..    
+000001d0: 2020 2020 7261 6973 6520 4e6f 7449 6d70      raise NotImp
+000001e0: 6c65 6d65 6e74 6564 4572 726f 7228 290d  lementedError().
+000001f0: 0a20 2020 200d 0a20 2020 2073 6368 656d  .    ..    schem
+00000200: 653a 2073 7472 0d0a 2020 2020 6465 6661  e: str..    defa
+00000210: 756c 745f 7363 6865 6d61 5f6e 616d 653a  ult_schema_name:
+00000220: 2073 7472 0d0a 2020 2020 636f 6d70 6174   str..    compat
+00000230: 6962 6c65 5f64 6a61 6e67 6f5f 656e 6769  ible_django_engi
+00000240: 6e65 733a 206c 6973 745b 7374 725d 0d0a  nes: list[str]..
+00000250: 0d0a 2020 2020 6465 6620 5f5f 696e 6974  ..    def __init
+00000260: 5f5f 2873 656c 662c 202a 2c20 6e61 6d65  __(self, *, name
+00000270: 3a20 7374 7220 3d20 4e6f 6e65 2c20 686f  : str = None, ho
+00000280: 7374 3a20 7374 7220 3d20 4e6f 6e65 2c20  st: str = None, 
+00000290: 706f 7274 3a20 696e 7420 3d20 4e6f 6e65  port: int = None
+000002a0: 2c20 7573 6572 3a20 7374 7220 3d20 4e6f  , user: str = No
+000002b0: 6e65 2c20 7061 7373 776f 7264 3a20 7374  ne, password: st
+000002c0: 7220 3d20 4e6f 6e65 2c20 646a 616e 676f  r = None, django
+000002d0: 5f61 6c69 6173 3a20 7374 7220 3d20 4e6f  _alias: str = No
+000002e0: 6e65 2c20 636f 6e6e 6563 7469 6f6e 3a20  ne, connection: 
+000002f0: 545f 436f 6e6e 6563 7469 6f6e 203d 204e  T_Connection = N
+00000300: 6f6e 652c 206e 6169 7665 5f74 7a3a 204c  one, naive_tz: L
+00000310: 6974 6572 616c 5b27 6c6f 6361 6c27 5d7c  iteral['local']|
+00000320: 7469 6d65 7a6f 6e65 203d 2027 6c6f 6361  timezone = 'loca
+00000330: 6c27 293a 2020 2020 0d0a 2020 2020 2020  l'):    ..      
+00000340: 2020 6966 206e 6f74 2073 656c 662e 6973    if not self.is
+00000350: 5f61 7661 696c 6162 6c65 2829 3a0d 0a20  _available():.. 
+00000360: 2020 2020 2020 2020 2020 2072 6169 7365             raise
+00000370: 2056 616c 7565 4572 726f 7228 6622 6361   ValueError(f"ca
+00000380: 6e6e 6f74 2075 7365 207b 7479 7065 2873  nnot use {type(s
+00000390: 656c 6629 2e5f 5f6e 616d 655f 5f7d 2028  elf).__name__} (
+000003a0: 6e6f 7420 6176 6169 6c61 626c 6529 2229  not available)")
+000003b0: 0d0a 2020 2020 2020 2020 0d0a 2020 2020  ..        ..    
+000003c0: 2020 2020 6966 2064 6a61 6e67 6f5f 616c      if django_al
+000003d0: 6961 7320 6973 206e 6f74 204e 6f6e 653a  ias is not None:
+000003e0: 0d0a 2020 2020 2020 2020 2020 2020 6672  ..            fr
+000003f0: 6f6d 2064 6a61 6e67 6f2e 636f 6e66 2069  om django.conf i
+00000400: 6d70 6f72 7420 7365 7474 696e 6773 0d0a  mport settings..
+00000410: 2020 2020 2020 2020 2020 2020 4441 5441              DATA
+00000420: 4241 5345 5320 3d20 7365 7474 696e 6773  BASES = settings
+00000430: 2e44 4154 4142 4153 4553 0d0a 2020 2020  .DATABASES..    
+00000440: 2020 2020 2020 2020 6966 206e 6f74 2064          if not d
+00000450: 6a61 6e67 6f5f 616c 6961 7320 696e 2044  jango_alias in D
+00000460: 4154 4142 4153 4553 3a0d 0a20 2020 2020  ATABASES:..     
+00000470: 2020 2020 2020 2020 2020 2072 6169 7365             raise
+00000480: 2056 616c 7565 4572 726f 7228 6622 6b65   ValueError(f"ke
+00000490: 7920 5c22 7b64 6a61 6e67 6f5f 616c 6961  y \"{django_alia
+000004a0: 737d 5c22 206e 6f74 2066 6f75 6e64 2069  s}\" not found i
+000004b0: 6e20 646a 616e 676f 2044 4154 4142 4153  n django DATABAS
+000004c0: 4553 2073 6574 7469 6e67 7322 290d 0a20  ES settings").. 
+000004d0: 2020 2020 2020 2020 2020 2064 6174 6162             datab
+000004e0: 6173 655f 7365 7474 696e 6773 3a20 6469  ase_settings: di
+000004f0: 6374 5b73 7472 2c41 6e79 5d20 3d20 4441  ct[str,Any] = DA
+00000500: 5441 4241 5345 535b 646a 616e 676f 5f61  TABASES[django_a
+00000510: 6c69 6173 5d0d 0a20 2020 2020 2020 2020  lias]..         
+00000520: 2020 2065 6e67 696e 6520 3d20 6461 7461     engine = data
+00000530: 6261 7365 5f73 6574 7469 6e67 735b 2745  base_settings['E
+00000540: 4e47 494e 4527 5d0d 0a20 2020 2020 2020  NGINE']..       
+00000550: 2020 2020 2069 6620 6e6f 7420 656e 6769       if not engi
+00000560: 6e65 2069 6e20 7365 6c66 2e63 6f6d 7061  ne in self.compa
+00000570: 7469 626c 655f 646a 616e 676f 5f65 6e67  tible_django_eng
+00000580: 696e 6573 3a0d 0a20 2020 2020 2020 2020  ines:..         
+00000590: 2020 2020 2020 2072 6169 7365 2056 616c         raise Val
+000005a0: 7565 4572 726f 7228 6622 646a 616e 676f  ueError(f"django
+000005b0: 2064 6174 6162 6173 6520 656e 6769 6e65   database engine
+000005c0: 205c 227b 656e 6769 6e65 7d5c 2220 6973   \"{engine}\" is
+000005d0: 206e 6f74 2063 6f6d 7061 7469 626c 6520   not compatible 
+000005e0: 7769 7468 207b 7479 7065 2873 656c 6629  with {type(self)
+000005f0: 2e5f 5f6e 616d 655f 5f7d 2229 0d0a 2020  .__name__}")..  
+00000600: 2020 2020 2020 656c 7365 3a0d 0a20 2020        else:..   
+00000610: 2020 2020 2020 2020 2064 6174 6162 6173           databas
+00000620: 655f 7365 7474 696e 6773 3a20 6469 6374  e_settings: dict
+00000630: 5b73 7472 2c41 6e79 5d20 3d20 7b7d 0d0a  [str,Any] = {}..
+00000640: 0d0a 2020 2020 2020 2020 7365 6c66 2e5f  ..        self._
+00000650: 6e61 6d65 3a20 7374 7220 3d20 6e61 6d65  name: str = name
+00000660: 2069 6620 6e61 6d65 2069 7320 6e6f 7420   if name is not 
+00000670: 4e6f 6e65 2065 6c73 6520 6461 7461 6261  None else databa
+00000680: 7365 5f73 6574 7469 6e67 732e 6765 7428  se_settings.get(
+00000690: 274e 414d 4527 2c20 4e6f 6e65 290d 0a20  'NAME', None).. 
+000006a0: 2020 2020 2020 2073 656c 662e 5f68 6f73         self._hos
+000006b0: 743a 2073 7472 203d 2068 6f73 7420 6966  t: str = host if
+000006c0: 2068 6f73 7420 6973 206e 6f74 204e 6f6e   host is not Non
+000006d0: 6520 656c 7365 2064 6174 6162 6173 655f  e else database_
+000006e0: 7365 7474 696e 6773 2e67 6574 2827 484f  settings.get('HO
+000006f0: 5354 272c 204e 6f6e 6529 0d0a 2020 2020  ST', None)..    
+00000700: 2020 2020 7365 6c66 2e5f 7573 6572 3a20      self._user: 
+00000710: 7374 7220 3d20 7573 6572 2069 6620 7573  str = user if us
+00000720: 6572 2069 7320 6e6f 7420 4e6f 6e65 2065  er is not None e
+00000730: 6c73 6520 6461 7461 6261 7365 5f73 6574  lse database_set
+00000740: 7469 6e67 732e 6765 7428 2755 5345 5227  tings.get('USER'
+00000750: 2c20 4e6f 6e65 290d 0a20 2020 2020 2020  , None)..       
+00000760: 2073 656c 662e 5f70 6173 7377 6f72 643a   self._password:
+00000770: 2073 7472 203d 2070 6173 7377 6f72 6420   str = password 
+00000780: 6966 2070 6173 7377 6f72 6420 6973 206e  if password is n
+00000790: 6f74 204e 6f6e 6520 656c 7365 2064 6174  ot None else dat
+000007a0: 6162 6173 655f 7365 7474 696e 6773 2e67  abase_settings.g
+000007b0: 6574 2827 5041 5353 574f 5244 272c 204e  et('PASSWORD', N
+000007c0: 6f6e 6529 0d0a 2020 2020 2020 2020 7365  one)..        se
+000007d0: 6c66 2e5f 706f 7274 3a20 7374 7220 3d20  lf._port: str = 
+000007e0: 706f 7274 2069 6620 706f 7274 2069 7320  port if port is 
+000007f0: 6e6f 7420 4e6f 6e65 2065 6c73 6520 6461  not None else da
+00000800: 7461 6261 7365 5f73 6574 7469 6e67 732e  tabase_settings.
+00000810: 6765 7428 2750 4f52 5427 2c20 4e6f 6e65  get('PORT', None
+00000820: 290d 0a0d 0a20 2020 2020 2020 2069 6620  )....        if 
+00000830: 6973 696e 7374 616e 6365 2873 656c 662e  isinstance(self.
+00000840: 5f70 6f72 742c 2073 7472 293a 0d0a 2020  _port, str):..  
+00000850: 2020 2020 2020 2020 2020 7365 6c66 2e5f            self._
+00000860: 706f 7274 203d 2069 6e74 2873 656c 662e  port = int(self.
+00000870: 5f70 6f72 7429 0d0a 2020 2020 2020 2020  _port)..        
+00000880: 0d0a 2020 2020 2020 2020 7365 6c66 2e5f  ..        self._
+00000890: 6e61 6976 655f 747a 203d 206e 6169 7665  naive_tz = naive
+000008a0: 5f74 7a0d 0a20 2020 2020 2020 2022 2222  _tz..        """
+000008b0: 2049 6620 6e6f 7420 4e6f 6e65 2c20 696e   If not None, in
+000008c0: 6469 6361 7465 2077 6869 6368 2074 696d  dicate which tim
+000008d0: 657a 6f6e 6520 7769 6c6c 2062 6520 7573  ezone will be us
+000008e0: 6564 2066 6f72 206e 6169 7665 2064 6174  ed for naive dat
+000008f0: 6574 696d 6573 2e20 2222 220d 0a0d 0a20  etimes. """.... 
+00000900: 2020 2020 2020 2073 656c 662e 5f63 6f6e         self._con
+00000910: 6e65 6374 696f 6e20 3d20 636f 6e6e 6563  nection = connec
+00000920: 7469 6f6e 0d0a 2020 2020 2020 2020 7365  tion..        se
+00000930: 6c66 2e5f 6d75 7374 5f63 6c6f 7365 5f63  lf._must_close_c
+00000940: 6f6e 6e65 6374 696f 6e20 3d20 636f 6e6e  onnection = conn
+00000950: 6563 7469 6f6e 2069 7320 4e6f 6e65 0d0a  ection is None..
+00000960: 0d0a 0d0a 2020 2020 6465 6620 5f5f 656e  ....    def __en
+00000970: 7465 725f 5f28 7365 6c66 293a 0d0a 2020  ter__(self):..  
+00000980: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
+00000990: 660d 0a0d 0a0d 0a20 2020 2064 6566 205f  f......    def _
+000009a0: 5f65 7869 745f 5f28 7365 6c66 2c20 6578  _exit__(self, ex
+000009b0: 635f 7479 7065 203d 204e 6f6e 652c 2065  c_type = None, e
+000009c0: 7863 5f76 616c 203d 204e 6f6e 652c 2065  xc_val = None, e
+000009d0: 7863 5f74 6220 3d20 4e6f 6e65 293a 0d0a  xc_tb = None):..
+000009e0: 2020 2020 2020 2020 6966 2073 656c 662e          if self.
+000009f0: 5f63 6f6e 6e65 6374 696f 6e20 616e 6420  _connection and 
+00000a00: 7365 6c66 2e5f 6d75 7374 5f63 6c6f 7365  self._must_close
+00000a10: 5f63 6f6e 6e65 6374 696f 6e3a 0d0a 2020  _connection:..  
+00000a20: 2020 2020 2020 2020 2020 7365 6c66 2e5f            self._
+00000a30: 636f 6e6e 6563 7469 6f6e 2e63 6c6f 7365  connection.close
+00000a40: 2829 0d0a 0d0a 0d0a 2020 2020 6465 6620  ()......    def 
+00000a50: 6765 745f 7572 6928 7365 6c66 2c20 7461  get_uri(self, ta
+00000a60: 626c 653a 2073 7472 7c74 7570 6c65 2c20  ble: str|tuple, 
+00000a70: 2a2c 2077 6974 685f 7061 7373 776f 7264  *, with_password
+00000a80: 203d 2046 616c 7365 293a 0d0a 2020 2020   = False):..    
+00000a90: 2020 2020 7572 6920 3d20 6622 7b73 656c      uri = f"{sel
+00000aa0: 662e 7363 6865 6d65 7d3a 220d 0a0d 0a20  f.scheme}:".... 
+00000ab0: 2020 2020 2020 2069 6620 7365 6c66 2e5f         if self._
+00000ac0: 7573 6572 206f 7220 7365 6c66 2e5f 686f  user or self._ho
+00000ad0: 7374 3a0d 0a20 2020 2020 2020 2020 2020  st:..           
+00000ae0: 2075 7269 202b 3d20 272f 2f27 0d0a 2020   uri += '//'..  
+00000af0: 2020 2020 2020 2020 2020 6966 2073 656c            if sel
+00000b00: 662e 5f75 7365 723a 0d0a 2020 2020 2020  f._user:..      
+00000b10: 2020 2020 2020 2020 2020 7572 6920 2b3d            uri +=
+00000b20: 2071 756f 7465 2873 656c 662e 5f75 7365   quote(self._use
+00000b30: 7229 0d0a 2020 2020 2020 2020 2020 2020  r)..            
+00000b40: 2020 2020 6966 2073 656c 662e 5f70 6173      if self._pas
+00000b50: 7377 6f72 643a 0d0a 2020 2020 2020 2020  sword:..        
+00000b60: 2020 2020 2020 2020 2020 2020 7572 6920              uri 
+00000b70: 2b3d 2027 3a27 202b 2028 7175 6f74 6528  += ':' + (quote(
+00000b80: 7365 6c66 2e5f 7061 7373 776f 7264 2920  self._password) 
+00000b90: 6966 2077 6974 685f 7061 7373 776f 7264  if with_password
+00000ba0: 2065 6c73 6520 7265 2e73 7562 2872 272e   else re.sub(r'.
+00000bb0: 272c 2027 2a27 2c20 7365 6c66 2e5f 7061  ', '*', self._pa
+00000bc0: 7373 776f 7264 2929 0d0a 2020 2020 2020  ssword))..      
+00000bd0: 2020 2020 2020 2020 2020 7572 6920 2b3d            uri +=
+00000be0: 2027 4027 0d0a 0d0a 2020 2020 2020 2020   '@'....        
+00000bf0: 2020 2020 6966 2073 656c 662e 5f68 6f73      if self._hos
+00000c00: 743a 0d0a 2020 2020 2020 2020 2020 2020  t:..            
+00000c10: 2020 2020 7572 6920 2b3d 2071 756f 7465      uri += quote
+00000c20: 2873 656c 662e 5f68 6f73 7429 0d0a 2020  (self._host)..  
+00000c30: 2020 2020 2020 2020 2020 2020 2020 6966                if
+00000c40: 2073 656c 662e 5f70 6f72 743a 0d0a 2020   self._port:..  
+00000c50: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00000c60: 2020 7572 6920 2b3d 2066 273a 7b73 656c    uri += f':{sel
+00000c70: 662e 5f70 6f72 747d 270d 0a0d 0a20 2020  f._port}'....   
+00000c80: 2020 2020 2069 6620 7365 6c66 2e5f 6e61       if self._na
+00000c90: 6d65 3a0d 0a20 2020 2020 2020 2020 2020  me:..           
+00000ca0: 2075 7269 202b 3d20 6622 2f7b 7175 6f74   uri += f"/{quot
+00000cb0: 6528 7365 6c66 2e5f 6e61 6d65 297d 220d  e(self._name)}".
+00000cc0: 0a0d 0a20 2020 2020 2020 2073 6368 656d  ...        schem
+00000cd0: 612c 2074 6162 6c65 203d 2073 656c 662e  a, table = self.
+00000ce0: 7370 6c69 745f 6e61 6d65 2874 6162 6c65  split_name(table
+00000cf0: 290d 0a20 2020 2020 2020 2069 6620 7461  )..        if ta
+00000d00: 626c 653a 0d0a 2020 2020 2020 2020 2020  ble:..          
+00000d10: 2020 7572 6920 2b3d 2066 222f 220d 0a20    uri += f"/".. 
+00000d20: 2020 2020 2020 2020 2020 2069 6620 7363             if sc
+00000d30: 6865 6d61 3a0d 0a20 2020 2020 2020 2020  hema:..         
+00000d40: 2020 2020 2020 2075 7269 202b 3d20 7175         uri += qu
+00000d50: 6f74 6528 7363 6865 6d61 290d 0a20 2020  ote(schema)..   
+00000d60: 2020 2020 2020 2020 2020 2020 2075 7269               uri
+00000d70: 202b 3d20 272e 270d 0a20 2020 2020 2020   += '.'..       
+00000d80: 2020 2020 2075 7269 202b 3d20 7175 6f74       uri += quot
+00000d90: 6528 7461 626c 6529 0d0a 0d0a 2020 2020  e(table)....    
+00000da0: 2020 2020 7265 7475 726e 2075 7269 0d0a      return uri..
+00000db0: 0d0a 0d0a 2020 2020 4070 726f 7065 7274  ....    @propert
+00000dc0: 790d 0a20 2020 2064 6566 2063 6f6e 6e65  y..    def conne
+00000dd0: 6374 696f 6e28 7365 6c66 293a 0d0a 2020  ction(self):..  
+00000de0: 2020 2020 2020 6966 206e 6f74 2073 656c        if not sel
+00000df0: 662e 5f63 6f6e 6e65 6374 696f 6e3a 0d0a  f._connection:..
+00000e00: 2020 2020 2020 2020 2020 2020 7365 6c66              self
+00000e10: 2e5f 636f 6e6e 6563 7469 6f6e 203d 2073  ._connection = s
+00000e20: 656c 662e 5f63 7265 6174 655f 636f 6e6e  elf._create_conn
+00000e30: 6563 7469 6f6e 2829 0d0a 2020 2020 2020  ection()..      
+00000e40: 2020 7265 7475 726e 2073 656c 662e 5f63    return self._c
+00000e50: 6f6e 6e65 6374 696f 6e0d 0a0d 0a0d 0a20  onnection...... 
+00000e60: 2020 2064 6566 205f 6372 6561 7465 5f63     def _create_c
+00000e70: 6f6e 6e65 6374 696f 6e28 7365 6c66 2920  onnection(self) 
+00000e80: 2d3e 2054 5f43 6f6e 6e65 6374 696f 6e3a  -> T_Connection:
+00000e90: 0d0a 2020 2020 2020 2020 7261 6973 6520  ..        raise 
+00000ea0: 4e6f 7449 6d70 6c65 6d65 6e74 6564 4572  NotImplementedEr
+00000eb0: 726f 7228 290d 0a0d 0a0d 0a20 2020 2064  ror()......    d
+00000ec0: 6566 2063 7572 736f 7228 7365 6c66 2920  ef cursor(self) 
+00000ed0: 2d3e 2054 5f43 7572 736f 723a 0d0a 2020  -> T_Cursor:..  
+00000ee0: 2020 2020 2020 7265 7475 726e 2073 656c        return sel
+00000ef0: 662e 636f 6e6e 6563 7469 6f6e 2e63 7572  f.connection.cur
+00000f00: 736f 7228 290d 0a20 2020 200d 0a20 2020  sor()..    ..   
+00000f10: 200d 0a20 2020 2064 6566 206c 696d 6974   ..    def limit
+00000f20: 5f71 7565 7279 2873 656c 662c 2071 7565  _query(self, que
+00000f30: 7279 3a20 7374 722c 206c 696d 6974 3a20  ry: str, limit: 
+00000f40: 696e 7429 3a0d 0a20 2020 2020 2020 2069  int):..        i
+00000f50: 6620 6c69 6d69 7420 6973 204e 6f6e 653a  f limit is None:
+00000f60: 0d0a 2020 2020 2020 2020 2020 2020 7265  ..            re
+00000f70: 7475 726e 2071 7565 7279 0d0a 2020 2020  turn query..    
+00000f80: 0d0a 2020 2020 2020 2020 6966 206e 6f74  ..        if not
+00000f90: 2069 7369 6e73 7461 6e63 6528 6c69 6d69   isinstance(limi
+00000fa0: 742c 2069 6e74 293a 0d0a 2020 2020 2020  t, int):..      
+00000fb0: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
+00000fc0: 6545 7272 6f72 2866 2269 6e76 616c 6964  eError(f"invalid
+00000fd0: 2074 7970 6520 666f 7220 6c69 6d69 743a   type for limit:
+00000fe0: 207b 7479 7065 286c 696d 6974 292e 5f5f   {type(limit).__
+00000ff0: 6e61 6d65 5f5f 7d20 2865 7870 6563 7465  name__} (expecte
+00001000: 6420 696e 7429 2229 0d0a 2020 2020 2020  d int)")..      
+00001010: 2020 0d0a 2020 2020 2020 2020 696d 706f    ..        impo
+00001020: 7274 2073 716c 7061 7273 6520 2320 6e6f  rt sqlparse # no
+00001030: 7420 6174 2074 6865 2074 6f70 2062 6563  t at the top bec
+00001040: 6175 7365 2074 6865 2065 6e64 7573 6572  ause the enduser
+00001050: 206d 6967 6874 206e 6f74 206e 6565 6420   might not need 
+00001060: 7468 6973 2066 6561 7475 7265 0d0a 0d0a  this feature....
+00001070: 2020 2020 2020 2020 2320 5061 7273 6520          # Parse 
+00001080: 5351 4c20 746f 2072 656d 6f76 6520 746f  SQL to remove to
+00001090: 6b65 6e20 6265 666f 7265 2074 6865 2053  ken before the S
+000010a0: 454c 4543 5420 6b65 7977 6f72 640d 0a20  ELECT keyword.. 
+000010b0: 2020 2020 2020 2023 2065 7861 6d70 6c65         # example
+000010c0: 3a20 5749 5448 2028 4354 4529 2074 6f6b  : WITH (CTE) tok
+000010d0: 656e 730d 0a20 2020 2020 2020 2073 7461  ens..        sta
+000010e0: 7465 6d65 6e74 7320 3d20 7371 6c70 6172  tements = sqlpar
+000010f0: 7365 2e70 6172 7365 2871 7565 7279 290d  se.parse(query).
+00001100: 0a20 2020 2020 2020 2069 6620 6c65 6e28  .        if len(
+00001110: 7374 6174 656d 656e 7473 2920 213d 2031  statements) != 1
+00001120: 3a0d 0a20 2020 2020 2020 2020 2020 2072  :..            r
+00001130: 6169 7365 2056 616c 7565 4572 726f 7228  aise ValueError(
+00001140: 6622 7175 6572 7920 636f 6e74 6169 6e73  f"query contains
+00001150: 207b 6c65 6e28 7374 6174 656d 656e 7473   {len(statements
+00001160: 297d 2073 7461 7465 6d65 6e74 7322 290d  )} statements").
+00001170: 0a0d 0a20 2020 2020 2020 2023 2047 6574  ...        # Get
+00001180: 2066 6972 7374 2044 4d4c 206b 6579 776f   first DML keywo
+00001190: 7264 0d0a 2020 2020 2020 2020 646d 6c5f  rd..        dml_
+000011a0: 6b65 7977 6f72 6420 3d20 4e6f 6e65 0d0a  keyword = None..
+000011b0: 2020 2020 2020 2020 646d 6c5f 6b65 7977          dml_keyw
+000011c0: 6f72 645f 696e 6465 7820 3d20 4e6f 6e65  ord_index = None
+000011d0: 0d0a 2020 2020 2020 2020 6f72 6465 725f  ..        order_
+000011e0: 6279 5f69 6e64 6578 203d 204e 6f6e 650d  by_index = None.
+000011f0: 0a20 2020 2020 2020 2066 6f72 2069 2c20  .        for i, 
+00001200: 746f 6b65 6e20 696e 2065 6e75 6d65 7261  token in enumera
+00001210: 7465 2873 7461 7465 6d65 6e74 735b 305d  te(statements[0]
+00001220: 2e74 6f6b 656e 7329 3a0d 0a20 2020 2020  .tokens):..     
+00001230: 2020 2020 2020 2069 6620 746f 6b65 6e2e         if token.
+00001240: 7474 7970 6520 3d3d 2073 716c 7061 7273  ttype == sqlpars
+00001250: 652e 746f 6b65 6e73 2e44 4d4c 3a0d 0a20  e.tokens.DML:.. 
+00001260: 2020 2020 2020 2020 2020 2020 2020 2069                 i
+00001270: 6620 646d 6c5f 6b65 7977 6f72 6420 6973  f dml_keyword is
+00001280: 204e 6f6e 653a 0d0a 2020 2020 2020 2020   None:..        
+00001290: 2020 2020 2020 2020 2020 2020 646d 6c5f              dml_
+000012a0: 6b65 7977 6f72 6420 3d20 7374 7228 746f  keyword = str(to
+000012b0: 6b65 6e29 2e75 7070 6572 2829 0d0a 2020  ken).upper()..  
+000012c0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000012d0: 2020 646d 6c5f 6b65 7977 6f72 645f 696e    dml_keyword_in
+000012e0: 6465 7820 3d20 690d 0a20 2020 2020 2020  dex = i..       
+000012f0: 2020 2020 2065 6c69 6620 746f 6b65 6e2e       elif token.
+00001300: 7474 7970 6520 3d3d 2073 716c 7061 7273  ttype == sqlpars
+00001310: 652e 746f 6b65 6e73 2e4b 6579 776f 7264  e.tokens.Keyword
+00001320: 3a0d 0a20 2020 2020 2020 2020 2020 2020  :..             
+00001330: 2020 2069 6620 6f72 6465 725f 6279 5f69     if order_by_i
+00001340: 6e64 6578 2069 7320 4e6f 6e65 3a0d 0a20  ndex is None:.. 
+00001350: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00001360: 2020 206b 6579 776f 7264 203d 2073 7472     keyword = str
+00001370: 2874 6f6b 656e 292e 7570 7065 7228 290d  (token).upper().
+00001380: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00001390: 2020 2020 2069 6620 6b65 7977 6f72 6420       if keyword 
+000013a0: 3d3d 2022 4f52 4445 5220 4259 223a 0d0a  == "ORDER BY":..
+000013b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000013c0: 2020 2020 2020 2020 6f72 6465 725f 6279          order_by
+000013d0: 5f69 6e64 6578 203d 2069 0d0a 0d0a 2020  _index = i....  
+000013e0: 2020 2020 2020 2320 4368 6563 6b20 6966        # Check if
+000013f0: 2074 6865 2044 4d4c 206b 6579 776f 7264   the DML keyword
+00001400: 2069 7320 5345 4c45 4354 0d0a 2020 2020   is SELECT..    
+00001410: 2020 2020 6966 206e 6f74 2064 6d6c 5f6b      if not dml_k
+00001420: 6579 776f 7264 3a0d 0a20 2020 2020 2020  eyword:..       
+00001430: 2020 2020 2072 6169 7365 2056 616c 7565       raise Value
+00001440: 4572 726f 7228 6622 6e6f 2053 454c 4543  Error(f"no SELEC
+00001450: 5420 666f 756e 6420 2871 7565 7279 2064  T found (query d
+00001460: 6f65 7320 6e6f 7420 636f 6e74 6169 6e20  oes not contain 
+00001470: 444d 4c20 6b65 7977 6f72 6429 2229 0d0a  DML keyword)")..
+00001480: 2020 2020 2020 2020 6966 2064 6d6c 5f6b          if dml_k
+00001490: 6579 776f 7264 2021 3d20 2753 454c 4543  eyword != 'SELEC
+000014a0: 5427 3a0d 0a20 2020 2020 2020 2020 2020  T':..           
+000014b0: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
+000014c0: 7228 6622 6669 7273 7420 444d 4c20 6b65  r(f"first DML ke
+000014d0: 7977 6f72 6420 6973 207b 646d 6c5f 6b65  yword is {dml_ke
+000014e0: 7977 6f72 647d 2c20 6578 7065 6374 6564  yword}, expected
+000014f0: 2053 454c 4543 5422 290d 0a0d 0a20 2020   SELECT")....   
+00001500: 2020 2020 2023 2047 6574 2070 6172 7420       # Get part 
+00001510: 6265 666f 7265 2053 454c 4543 5420 2865  before SELECT (e
+00001520: 7861 6d70 6c65 3a20 5749 5448 290d 0a20  xample: WITH).. 
+00001530: 2020 2020 2020 2069 6620 646d 6c5f 6b65         if dml_ke
+00001540: 7977 6f72 645f 696e 6465 7820 3e20 303a  yword_index > 0:
+00001550: 0d0a 2020 2020 2020 2020 2020 2020 746f  ..            to
+00001560: 6b65 6e73 203d 2073 7461 7465 6d65 6e74  kens = statement
+00001570: 735b 305d 2e74 6f6b 656e 735b 3a64 6d6c  s[0].tokens[:dml
+00001580: 5f6b 6579 776f 7264 5f69 6e64 6578 5d0d  _keyword_index].
+00001590: 0a20 2020 2020 2020 2020 2020 206c 696d  .            lim
+000015a0: 6974 6564 5f71 7565 7279 203d 2027 272e  ited_query = ''.
+000015b0: 6a6f 696e 2873 7472 2874 6f6b 656e 2920  join(str(token) 
+000015c0: 666f 7220 746f 6b65 6e20 696e 2074 6f6b  for token in tok
+000015d0: 656e 7329 0d0a 2020 2020 2020 2020 656c  ens)..        el
+000015e0: 7365 3a0d 0a20 2020 2020 2020 2020 2020  se:..           
+000015f0: 206c 696d 6974 6564 5f71 7565 7279 203d   limited_query =
+00001600: 2027 270d 0a20 2020 200d 0a20 2020 2020   ''..    ..     
+00001610: 2020 2023 2041 7070 656e 6420 5345 4c45     # Append SELE
+00001620: 4354 2062 6566 6f72 6520 4f52 4445 5220  CT before ORDER 
+00001630: 4259 0d0a 2020 2020 2020 2020 6966 206f  BY..        if o
+00001640: 7264 6572 5f62 795f 696e 6465 7820 6973  rder_by_index is
+00001650: 206e 6f74 204e 6f6e 653a 0d0a 2020 2020   not None:..    
+00001660: 2020 2020 2020 2020 746f 6b65 6e73 203d          tokens =
+00001670: 2073 7461 7465 6d65 6e74 735b 305d 2e74   statements[0].t
+00001680: 6f6b 656e 735b 646d 6c5f 6b65 7977 6f72  okens[dml_keywor
+00001690: 645f 696e 6465 783a 6f72 6465 725f 6279  d_index:order_by
+000016a0: 5f69 6e64 6578 5d0d 0a20 2020 2020 2020  _index]..       
+000016b0: 2065 6c73 653a 0d0a 2020 2020 2020 2020   else:..        
+000016c0: 2020 2020 746f 6b65 6e73 203d 2073 7461      tokens = sta
+000016d0: 7465 6d65 6e74 735b 305d 2e74 6f6b 656e  tements[0].token
+000016e0: 735b 646d 6c5f 6b65 7977 6f72 645f 696e  s[dml_keyword_in
+000016f0: 6465 783a 5d0d 0a0d 0a20 2020 2020 2020  dex:]....       
+00001700: 206c 696d 6974 6564 5f71 7565 7279 202b   limited_query +
+00001710: 3d20 7365 6c66 2e5f 6c69 6d69 745f 7061  = self._limit_pa
+00001720: 7273 6564 5f71 7565 7279 2827 272e 6a6f  rsed_query(''.jo
+00001730: 696e 2873 7472 2874 6f6b 656e 2920 666f  in(str(token) fo
+00001740: 7220 746f 6b65 6e20 696e 2074 6f6b 656e  r token in token
+00001750: 7329 2c20 6c69 6d69 743d 6c69 6d69 7429  s), limit=limit)
+00001760: 0d0a 0d0a 2020 2020 2020 2020 2320 4170  ....        # Ap
+00001770: 7065 6e64 204f 5244 4552 2042 590d 0a20  pend ORDER BY.. 
+00001780: 2020 2020 2020 2069 6620 6f72 6465 725f         if order_
+00001790: 6279 5f69 6e64 6578 2069 7320 6e6f 7420  by_index is not 
+000017a0: 4e6f 6e65 3a0d 0a20 2020 2020 2020 2020  None:..         
+000017b0: 2020 2074 6f6b 656e 7320 3d20 7374 6174     tokens = stat
+000017c0: 656d 656e 7473 5b30 5d2e 746f 6b65 6e73  ements[0].tokens
+000017d0: 5b6f 7264 6572 5f62 795f 696e 6465 783a  [order_by_index:
+000017e0: 5d0d 0a20 2020 2020 2020 2020 2020 206c  ]..            l
+000017f0: 696d 6974 6564 5f71 7565 7279 202b 3d20  imited_query += 
+00001800: 275c 6e27 202b 2027 272e 6a6f 696e 2873  '\n' + ''.join(s
+00001810: 7472 2874 6f6b 656e 2920 666f 7220 746f  tr(token) for to
+00001820: 6b65 6e20 696e 2074 6f6b 656e 7329 0d0a  ken in tokens)..
+00001830: 0d0a 2020 2020 2020 2020 7265 7475 726e  ..        return
+00001840: 206c 696d 6974 6564 5f71 7565 7279 0d0a   limited_query..
+00001850: 2020 2020 0d0a 0d0a 2020 2020 6465 6620      ....    def 
+00001860: 5f6c 696d 6974 5f70 6172 7365 645f 7175  _limit_parsed_qu
+00001870: 6572 7928 7365 6c66 2c20 7175 6572 793a  ery(self, query:
+00001880: 2073 7472 2c20 6c69 6d69 743a 2069 6e74   str, limit: int
+00001890: 2920 2d3e 2073 7472 3a0d 0a20 2020 2020  ) -> str:..     
+000018a0: 2020 2072 6169 7365 204e 6f74 496d 706c     raise NotImpl
+000018b0: 656d 656e 7465 6445 7272 6f72 2829 0d0a  ementedError()..
+000018c0: 0d0a 0d0a 2020 2020 6465 6620 6275 696c  ....    def buil
+000018d0: 645f 7175 6572 795f 7769 7468 5f70 6f73  d_query_with_pos
+000018e0: 6974 696f 6e61 6c5f 7061 7261 6d73 2873  itional_params(s
+000018f0: 656c 662c 2071 7565 7279 3a20 7374 722c  elf, query: str,
+00001900: 2070 6172 616d 733a 206c 6973 747c 7475   params: list|tu
+00001910: 706c 657c 6469 6374 293a 0d0a 2020 2020  ple|dict):..    
+00001920: 2020 2020 6966 2069 7369 6e73 7461 6e63      if isinstanc
+00001930: 6528 7061 7261 6d73 2c20 6469 6374 293a  e(params, dict):
+00001940: 0d0a 2020 2020 2020 2020 2020 2020 6672  ..            fr
+00001950: 6f6d 2073 716c 7061 7261 6d73 2069 6d70  om sqlparams imp
+00001960: 6f72 7420 5351 4c50 6172 616d 7320 2320  ort SQLParams # 
+00001970: 6e6f 7420 6174 2074 6865 2074 6f70 2062  not at the top b
+00001980: 6563 6175 7365 2074 6865 2065 6e64 7573  ecause the endus
+00001990: 6572 206d 6967 6874 206e 6f74 206e 6565  er might not nee
+000019a0: 6420 7468 6973 2066 6561 7475 7265 0d0a  d this feature..
+000019b0: 0d0a 2020 2020 2020 2020 2020 2020 6966  ..            if
+000019c0: 206e 6f74 2068 6173 6174 7472 2873 656c   not hasattr(sel
+000019d0: 662e 5f5f 636c 6173 735f 5f2c 2027 5f70  f.__class__, '_p
+000019e0: 6172 616d 735f 666f 726d 6174 7465 7227  arams_formatter'
+000019f0: 293a 0d0a 2020 2020 2020 2020 2020 2020  ):..            
+00001a00: 2020 2020 7365 6c66 2e5f 5f63 6c61 7373      self.__class
+00001a10: 5f5f 2e5f 7061 7261 6d73 5f66 6f72 6d61  __._params_forma
+00001a20: 7474 6572 203d 2053 514c 5061 7261 6d73  tter = SQLParams
+00001a30: 2827 6e61 6d65 6427 2c20 2771 6d61 726b  ('named', 'qmark
+00001a40: 2729 0d0a 2020 2020 2020 2020 2020 2020  ')..            
+00001a50: 7175 6572 792c 2070 6172 616d 7320 3d20  query, params = 
+00001a60: 7365 6c66 2e5f 5f63 6c61 7373 5f5f 2e5f  self.__class__._
+00001a70: 7061 7261 6d73 5f66 6f72 6d61 7474 6572  params_formatter
+00001a80: 2e66 6f72 6d61 7428 7175 6572 792c 2070  .format(query, p
+00001a90: 6172 616d 7329 0d0a 0d0a 2020 2020 2020  arams)....      
+00001aa0: 2020 7265 7475 726e 2071 7565 7279 2c20    return query, 
+00001ab0: 7061 7261 6d73 0d0a 2020 2020 2020 2020  params..        
+00001ac0: 0d0a 0d0a 2020 2020 6465 6620 6765 745f  ....    def get_
+00001ad0: 6375 7273 6f72 5f63 6f6c 756d 6e73 2873  cursor_columns(s
+00001ae0: 656c 662c 2063 7572 736f 723a 2054 5f43  elf, cursor: T_C
+00001af0: 7572 736f 7229 202d 3e20 6c69 7374 5b43  ursor) -> list[C
+00001b00: 6f6c 756d 6e5d 3a0d 0a20 2020 2020 2020  olumn]:..       
+00001b10: 2063 6f6c 756d 6e73 203d 205b 5d0d 0a0d   columns = []...
+00001b20: 0a20 2020 2020 2020 2066 6f72 2069 2c20  .        for i, 
+00001b30: 696e 666f 2069 6e20 656e 756d 6572 6174  info in enumerat
+00001b40: 6528 6375 7273 6f72 2e64 6573 6372 6970  e(cursor.descrip
+00001b50: 7469 6f6e 293a 0d0a 2020 2020 2020 2020  tion):..        
+00001b60: 2020 2020 6e61 6d65 2c20 7079 7468 6f6e      name, python
+00001b70: 5f74 7970 652c 2064 6973 706c 6179 5f73  _type, display_s
+00001b80: 697a 652c 2069 6e74 6572 6e61 6c5f 7369  ize, internal_si
+00001b90: 7a65 2c20 7072 6563 6973 696f 6e2c 2073  ze, precision, s
+00001ba0: 6361 6c65 2c20 6e75 6c6c 5f6f 6b20 3d20  cale, null_ok = 
+00001bb0: 696e 666f 0d0a 2020 2020 2020 2020 2020  info..          
+00001bc0: 2020 636f 6c75 6d6e 203d 2043 6f6c 756d    column = Colum
+00001bd0: 6e28 6e61 6d65 2c20 706f 7369 7469 6f6e  n(name, position
+00001be0: 3d69 202b 2031 2c20 7079 7468 6f6e 5f74  =i + 1, python_t
+00001bf0: 7970 653d 7079 7468 6f6e 5f74 7970 652c  ype=python_type,
+00001c00: 2064 6973 706c 6179 5f73 697a 653d 6469   display_size=di
+00001c10: 7370 6c61 795f 7369 7a65 2c20 696e 7465  splay_size, inte
+00001c20: 726e 616c 5f73 697a 653d 696e 7465 726e  rnal_size=intern
+00001c30: 616c 5f73 697a 652c 2070 7265 6369 7369  al_size, precisi
+00001c40: 6f6e 3d70 7265 6369 7369 6f6e 2c20 7363  on=precision, sc
+00001c50: 616c 653d 7363 616c 652c 206e 756c 6c5f  ale=scale, null_
+00001c60: 6f6b 3d6e 756c 6c5f 6f6b 290d 0a20 2020  ok=null_ok)..   
+00001c70: 2020 2020 2020 2020 2073 656c 662e 5f66           self._f
+00001c80: 6978 5f63 7572 736f 725f 636f 6c75 6d6e  ix_cursor_column
+00001c90: 5f64 6566 696e 6974 696f 6e28 636f 6c75  _definition(colu
+00001ca0: 6d6e 290d 0a20 2020 2020 2020 2020 2020  mn)..           
+00001cb0: 2063 6f6c 756d 6e2e 5f63 6865 636b 2829   column._check()
+00001cc0: 0d0a 2020 2020 2020 2020 2020 2020 636f  ..            co
+00001cd0: 6c75 6d6e 732e 6170 7065 6e64 2863 6f6c  lumns.append(col
+00001ce0: 756d 6e29 0d0a 2020 2020 2020 2020 0d0a  umn)..        ..
+00001cf0: 2020 2020 2020 2020 7265 7475 726e 2063          return c
+00001d00: 6f6c 756d 6e73 0d0a 0d0a 0d0a 2020 2020  olumns......    
+00001d10: 6465 6620 5f66 6978 5f63 7572 736f 725f  def _fix_cursor_
+00001d20: 636f 6c75 6d6e 5f64 6566 696e 6974 696f  column_definitio
+00001d30: 6e28 7365 6c66 2c20 636f 6c75 6d6e 3a20  n(self, column: 
+00001d40: 436f 6c75 6d6e 293a 0d0a 2020 2020 2020  Column):..      
+00001d50: 2020 2222 220d 0a20 2020 2020 2020 2046    """..        F
+00001d60: 6978 2061 2063 6f6c 756d 6e20 6465 6669  ix a column defi
+00001d70: 6e69 7469 6f6e 2c20 6166 7465 7220 696e  nition, after in
+00001d80: 7374 616e 6369 6174 696f 6e20 696e 2060  stanciation in `
+00001d90: 6765 745f 636f 6c75 6d6e 735f 6672 6f6d  get_columns_from
+00001da0: 5f63 7572 736f 7260 2c20 666f 7220 6120  _cursor`, for a 
+00001db0: 7370 6563 6966 6963 2064 6174 6162 6173  specific databas
+00001dc0: 6520 656e 6769 6e65 2e0d 0a20 2020 2020  e engine...     
+00001dd0: 2020 2022 2222 0d0a 2020 2020 2020 2020     """..        
+00001de0: 7061 7373 0d0a 2020 2020 2020 2020 0d0a  pass..        ..
+00001df0: 0d0a 2020 2020 6465 6620 6765 745f 6375  ..    def get_cu
+00001e00: 7273 6f72 5f63 6f6c 756d 6e5f 6e61 6d65  rsor_column_name
+00001e10: 7328 7365 6c66 2c20 6375 7273 6f72 3a20  s(self, cursor: 
+00001e20: 545f 4375 7273 6f72 2920 2d3e 206c 6973  T_Cursor) -> lis
+00001e30: 745b 7374 725d 3a0d 0a20 2020 2020 2020  t[str]:..       
+00001e40: 2072 6574 7572 6e20 5b69 6e66 6f5b 305d   return [info[0]
+00001e50: 2066 6f72 2069 6e66 6f20 696e 2063 7572   for info in cur
+00001e60: 736f 722e 6465 7363 7269 7074 696f 6e5d  sor.description]
+00001e70: 0d0a 2020 2020 2020 2020 0d0a 0d0a 2020  ..        ....  
+00001e80: 2020 6465 6620 6765 745f 7461 626c 655f    def get_table_
+00001e90: 636f 6c75 6d6e 7328 7365 6c66 2c20 7461  columns(self, ta
+00001ea0: 626c 653a 2073 7472 7c74 7570 6c65 2920  ble: str|tuple) 
+00001eb0: 2d3e 206c 6973 745b 436f 6c75 6d6e 5d3a  -> list[Column]:
+00001ec0: 0d0a 2020 2020 2020 2020 7175 6572 7920  ..        query 
+00001ed0: 3d20 7365 6c66 2e67 6574 5f73 656c 6563  = self.get_selec
+00001ee0: 745f 7461 626c 655f 7175 6572 7928 7461  t_table_query(ta
+00001ef0: 626c 652c 2073 6368 656d 615f 6f6e 6c79  ble, schema_only
+00001f00: 3d54 7275 6529 0d0a 2020 2020 2020 2020  =True)..        
+00001f10: 7769 7468 2073 656c 662e 6578 6563 7574  with self.execut
+00001f20: 655f 6765 745f 6375 7273 6f72 2871 7565  e_get_cursor(que
+00001f30: 7279 2920 6173 2063 7572 736f 723a 0d0a  ry) as cursor:..
+00001f40: 2020 2020 2020 2020 2020 2020 7265 7475              retu
+00001f50: 726e 2073 656c 662e 6765 745f 6375 7273  rn self.get_curs
+00001f60: 6f72 5f63 6f6c 756d 6e73 2863 7572 736f  or_columns(curso
+00001f70: 7229 0d0a 0d0a 0d0a 2020 2020 6465 6620  r)......    def 
+00001f80: 6765 745f 7461 626c 655f 636f 6c75 6d6e  get_table_column
+00001f90: 5f6e 616d 6573 2873 656c 662c 2074 6162  _names(self, tab
+00001fa0: 6c65 3a20 7374 727c 7475 706c 6529 202d  le: str|tuple) -
+00001fb0: 3e20 6c69 7374 5b73 7472 5d3a 0d0a 2020  > list[str]:..  
+00001fc0: 2020 2020 2020 636f 6c75 6d6e 5f6e 616d        column_nam
+00001fd0: 6573 203d 205b 5d0d 0a20 2020 2020 2020  es = []..       
+00001fe0: 2066 6f72 2063 6f6c 756d 6e20 696e 2073   for column in s
+00001ff0: 656c 662e 6765 745f 7461 626c 655f 636f  elf.get_table_co
+00002000: 6c75 6d6e 7328 7461 626c 6529 3a0d 0a20  lumns(table):.. 
+00002010: 2020 2020 2020 2020 2020 2063 6f6c 756d             colum
+00002020: 6e5f 6e61 6d65 732e 6170 7065 6e64 2863  n_names.append(c
+00002030: 6f6c 756d 6e2e 6e61 6d65 290d 0a20 2020  olumn.name)..   
+00002040: 2020 2020 2072 6574 7572 6e20 636f 6c75       return colu
+00002050: 6d6e 5f6e 616d 6573 0d0a 0d0a 0d0a 2020  mn_names......  
+00002060: 2020 4063 6c61 7373 6d65 7468 6f64 0d0a    @classmethod..
+00002070: 2020 2020 6465 6620 7370 6c69 745f 6e61      def split_na
+00002080: 6d65 2863 6c73 2c20 6675 6c6c 5f6e 616d  me(cls, full_nam
+00002090: 653a 2073 7472 7c74 7570 6c65 2920 2d3e  e: str|tuple) ->
+000020a0: 2074 7570 6c65 5b73 7472 2c73 7472 5d3a   tuple[str,str]:
+000020b0: 0d0a 2020 2020 2020 2020 6966 2069 7369  ..        if isi
+000020c0: 6e73 7461 6e63 6528 6675 6c6c 5f6e 616d  nstance(full_nam
+000020d0: 652c 2074 7570 6c65 293a 0d0a 2020 2020  e, tuple):..    
+000020e0: 2020 2020 2020 2020 7265 7475 726e 2066          return f
+000020f0: 756c 6c5f 6e61 6d65 0d0a 2020 2020 2020  ull_name..      
+00002100: 2020 0d0a 2020 2020 2020 2020 7472 793a    ..        try:
+00002110: 0d0a 2020 2020 2020 2020 2020 2020 706f  ..            po
+00002120: 7320 3d20 6675 6c6c 5f6e 616d 652e 696e  s = full_name.in
+00002130: 6465 7828 272e 2729 0d0a 2020 2020 2020  dex('.')..      
+00002140: 2020 2020 2020 7363 6865 6d61 5f6e 616d        schema_nam
+00002150: 6520 3d20 6675 6c6c 5f6e 616d 655b 303a  e = full_name[0:
+00002160: 706f 735d 0d0a 2020 2020 2020 2020 2020  pos]..          
+00002170: 2020 6e61 6d65 203d 2066 756c 6c5f 6e61    name = full_na
+00002180: 6d65 5b70 6f73 2b31 3a5d 0d0a 2020 2020  me[pos+1:]..    
+00002190: 2020 2020 6578 6365 7074 2056 616c 7565      except Value
+000021a0: 4572 726f 723a 0d0a 2020 2020 2020 2020  Error:..        
+000021b0: 2020 2020 7363 6865 6d61 5f6e 616d 6520      schema_name 
+000021c0: 3d20 636c 732e 6465 6661 756c 745f 7363  = cls.default_sc
+000021d0: 6865 6d61 5f6e 616d 650d 0a20 2020 2020  hema_name..     
+000021e0: 2020 2020 2020 206e 616d 6520 3d20 6675         name = fu
+000021f0: 6c6c 5f6e 616d 650d 0a0d 0a20 2020 2020  ll_name....     
+00002200: 2020 2072 6574 7572 6e20 2873 6368 656d     return (schem
+00002210: 615f 6e61 6d65 2c20 6e61 6d65 290d 0a20  a_name, name).. 
+00002220: 2020 200d 0a0d 0a20 2020 2064 6566 2067     ....    def g
+00002230: 6574 5f73 656c 6563 745f 7461 626c 655f  et_select_table_
+00002240: 7175 6572 7928 7365 6c66 2c20 7461 626c  query(self, tabl
+00002250: 653a 2073 7472 7c74 7570 6c65 2c20 7363  e: str|tuple, sc
+00002260: 6865 6d61 5f6f 6e6c 7920 3d20 4661 6c73  hema_only = Fals
+00002270: 6529 3a0d 0a20 2020 2020 2020 2022 2222  e):..        """
+00002280: 0d0a 2020 2020 2020 2020 4275 696c 6420  ..        Build 
+00002290: 6120 7175 6572 7920 6f6e 2074 6865 2067  a query on the g
+000022a0: 6976 656e 2074 6162 6c65 2e0d 0a0d 0a20  iven table..... 
+000022b0: 2020 2020 2020 2049 6620 6073 6368 656d         If `schem
+000022c0: 615f 6f6e 6c79 6020 6973 2067 6976 656e  a_only` is given
+000022d0: 2c20 6e6f 2072 6f77 2077 696c 6c20 6265  , no row will be
+000022e0: 2072 6574 7572 6e65 6420 2874 6869 7320   returned (this 
+000022f0: 6973 2075 7365 6420 746f 2067 6574 2069  is used to get i
+00002300: 6e66 6f72 6d61 7469 6f6e 206f 6e20 7468  nformation on th
+00002310: 6520 7461 626c 6529 2e0d 0a20 2020 2020  e table)...     
+00002320: 2020 204f 7468 6572 7769 7365 2c20 616c     Otherwise, al
+00002330: 6c20 726f 7773 2077 696c 6c20 6265 2072  l rows will be r
+00002340: 6574 7572 6e65 642e 0d0a 0d0a 2020 2020  eturned.....    
+00002350: 2020 2020 5468 6520 7265 7475 726e 2074      The return t
+00002360: 7970 6520 6f66 2074 6869 7320 6675 6e63  ype of this func
+00002370: 7469 6f6e 2064 6570 656e 6473 206f 6e20  tion depends on 
+00002380: 7468 6520 6461 7461 6261 7365 2065 6e67  the database eng
+00002390: 696e 652e 0d0a 2020 2020 2020 2020 4974  ine...        It
+000023a0: 2069 7320 7061 7373 6564 2064 6972 6563   is passed direc
+000023b0: 746c 7920 746f 2074 6865 2063 7572 736f  tly to the curso
+000023c0: 7227 7320 6578 6563 7574 6520 6675 6e63  r's execute func
+000023d0: 7469 6f6e 2066 6f72 2074 6869 7320 656e  tion for this en
+000023e0: 6769 6e65 2e0d 0a20 2020 2020 2020 2022  gine...        "
+000023f0: 2222 0d0a 2020 2020 2020 2020 7261 6973  ""..        rais
+00002400: 6520 4e6f 7449 6d70 6c65 6d65 6e74 6564  e NotImplemented
+00002410: 4572 726f 7228 290d 0a20 2020 200d 0a0d  Error()..    ...
+00002420: 0a20 2020 2064 6566 2065 7865 6375 7465  .    def execute
+00002430: 5f67 6574 5f63 7572 736f 7228 7365 6c66  _get_cursor(self
+00002440: 2c20 7175 6572 793a 2073 7472 2c20 7061  , query: str, pa
+00002450: 7261 6d73 3a20 6c69 7374 7c74 7570 6c65  rams: list|tuple
+00002460: 7c64 6963 7420 3d20 4e6f 6e65 2c20 6c69  |dict = None, li
+00002470: 6d69 743a 2069 6e74 203d 204e 6f6e 6529  mit: int = None)
+00002480: 202d 3e20 545f 4375 7273 6f72 3a0d 0a20   -> T_Cursor:.. 
+00002490: 2020 2020 2020 2022 2222 0d0a 2020 2020         """..    
+000024a0: 2020 2020 4d75 7374 2062 6520 636c 6f73      Must be clos
+000024b0: 6564 2e0d 0a20 2020 2020 2020 2022 2222  ed...        """
+000024c0: 0d0a 2020 2020 2020 2020 7261 6973 6520  ..        raise 
+000024d0: 4e6f 7449 6d70 6c65 6d65 6e74 6564 4572  NotImplementedEr
+000024e0: 726f 7228 290d 0a20 2020 200d 0a0d 0a20  ror()..    .... 
+000024f0: 2020 2064 6566 2065 7865 6375 7465 5f67     def execute_g
+00002500: 6574 5f73 6361 6c61 7228 7365 6c66 2c20  et_scalar(self, 
+00002510: 7175 6572 793a 2073 7472 2c20 7061 7261  query: str, para
+00002520: 6d73 3a20 6c69 7374 7c74 7570 6c65 7c64  ms: list|tuple|d
+00002530: 6963 7420 3d20 4e6f 6e65 2c20 6c69 6d69  ict = None, limi
+00002540: 743a 2069 6e74 203d 204e 6f6e 6529 3a0d  t: int = None):.
+00002550: 0a20 2020 2020 2020 2077 6974 6820 7365  .        with se
+00002560: 6c66 2e65 7865 6375 7465 5f67 6574 5f63  lf.execute_get_c
+00002570: 7572 736f 7228 7175 6572 792c 2070 6172  ursor(query, par
+00002580: 616d 732c 206c 696d 6974 3d6c 696d 6974  ams, limit=limit
+00002590: 2920 6173 2063 7572 736f 723a 0d0a 2020  ) as cursor:..  
+000025a0: 2020 2020 2020 2020 2020 7265 7375 6c74            result
+000025b0: 203d 206e 6578 7428 6375 7273 6f72 290d   = next(cursor).
+000025c0: 0a20 2020 2020 2020 2020 2020 200d 0a20  .            .. 
+000025d0: 2020 2020 2020 2020 2020 2023 2043 6865             # Che
+000025e0: 636b 206f 6e6c 7920 6f6e 6520 726f 770d  ck only one row.
+000025f0: 0a20 2020 2020 2020 2020 2020 2074 7279  .            try
+00002600: 3a0d 0a20 2020 2020 2020 2020 2020 2020  :..             
+00002610: 2020 206e 6578 7428 6375 7273 6f72 290d     next(cursor).
+00002620: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00002630: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro
+00002640: 7228 6622 7365 7665 7261 6c20 726f 7773  r(f"several rows
+00002650: 2072 6574 7572 6e65 6420 6279 2071 7565   returned by que
+00002660: 7279 2229 0d0a 2020 2020 2020 2020 2020  ry")..          
+00002670: 2020 6578 6365 7074 2053 746f 7049 7465    except StopIte
+00002680: 7261 7469 6f6e 3a0d 0a20 2020 2020 2020  ration:..       
+00002690: 2020 2020 2020 2020 2070 6173 730d 0a0d           pass...
+000026a0: 0a20 2020 2020 2020 2020 2020 2023 2043  .            # C
+000026b0: 6865 636b 206f 6e6c 7920 6f6e 6520 7661  heck only one va
+000026c0: 6c75 650d 0a20 2020 2020 2020 2020 2020  lue..           
+000026d0: 2069 6620 6c65 6e28 7265 7375 6c74 2920   if len(result) 
+000026e0: 3e20 313a 0d0a 2020 2020 2020 2020 2020  > 1:..          
+000026f0: 2020 2020 2020 7261 6973 6520 5661 6c75        raise Valu
+00002700: 6545 7272 6f72 2822 7365 7665 7261 6c20  eError("several 
+00002710: 7661 6c75 6573 2072 6574 7572 6e65 6420  values returned 
+00002720: 6279 2071 7565 7279 2229 0d0a 0d0a 2020  by query")....  
+00002730: 2020 2020 2020 2020 2020 7265 7475 726e            return
+00002740: 2072 6573 756c 745b 305d 0d0a 0d0a 0d0a   result[0]......
+00002750: 2020 2020 6465 6620 6578 6563 7574 6528      def execute(
+00002760: 7365 6c66 2c20 7175 6572 793a 2073 7472  self, query: str
+00002770: 2c20 7061 7261 6d73 3a20 6c69 7374 7c74  , params: list|t
+00002780: 7570 6c65 7c64 6963 7420 3d20 4e6f 6e65  uple|dict = None
+00002790: 2c20 6c69 6d69 743a 2069 6e74 203d 204e  , limit: int = N
+000027a0: 6f6e 6529 3a0d 0a20 2020 2020 2020 2077  one):..        w
+000027b0: 6974 6820 7365 6c66 2e65 7865 6375 7465  ith self.execute
+000027c0: 5f67 6574 5f63 7572 736f 7228 7175 6572  _get_cursor(quer
+000027d0: 792c 2070 6172 616d 732c 206c 696d 6974  y, params, limit
+000027e0: 3d6c 696d 6974 293a 0d0a 2020 2020 2020  =limit):..      
+000027f0: 2020 2020 2020 7061 7373 0d0a 0d0a 0d0a        pass......
+00002800: 2020 2020 6465 6620 6578 6563 7574 655f      def execute_
+00002810: 6669 6c65 2873 656c 662c 2070 6174 683a  file(self, path:
+00002820: 2073 7472 7c50 6174 682c 2070 6172 616d   str|Path, param
+00002830: 733a 206c 6973 747c 7475 706c 657c 6469  s: list|tuple|di
+00002840: 6374 203d 204e 6f6e 652c 206c 696d 6974  ct = None, limit
+00002850: 3a20 696e 7420 3d20 4e6f 6e65 293a 0d0a  : int = None):..
+00002860: 2020 2020 2020 2020 6966 206e 6f74 2069          if not i
+00002870: 7369 6e73 7461 6e63 6528 7061 7468 2c20  sinstance(path, 
+00002880: 5061 7468 293a 0d0a 2020 2020 2020 2020  Path):..        
+00002890: 2020 2020 7061 7468 203d 2050 6174 6828      path = Path(
+000028a0: 7061 7468 290d 0a20 2020 2020 2020 2020  path)..         
+000028b0: 2020 200d 0a20 2020 2020 2020 2071 7565     ..        que
+000028c0: 7279 203d 2070 6174 682e 7265 6164 5f74  ry = path.read_t
+000028d0: 6578 7428 656e 636f 6469 6e67 3d27 7574  ext(encoding='ut
+000028e0: 662d 3827 290d 0a20 2020 2020 2020 2073  f-8')..        s
+000028f0: 656c 662e 6578 6563 7574 6528 7175 6572  elf.execute(quer
+00002900: 792c 2070 6172 616d 732c 206c 696d 6974  y, params, limit
+00002910: 3d6c 696d 6974 290d 0a0d 0a0d 0a20 2020  =limit)......   
+00002920: 2064 6566 2063 616c 6c5f 7072 6f63 6564   def call_proced
+00002930: 7572 6528 7365 6c66 2c20 6e61 6d65 3a20  ure(self, name: 
+00002940: 7374 727c 7475 706c 652c 202a 6172 6773  str|tuple, *args
+00002950: 293a 0d0a 2020 2020 2020 2020 7261 6973  ):..        rais
+00002960: 6520 4e6f 7449 6d70 6c65 6d65 6e74 6564  e NotImplemented
+00002970: 4572 726f 7228 290d 0a0d 0a0d 0a20 2020  Error()......   
+00002980: 2064 6566 2074 6162 6c65 5f65 7869 7374   def table_exist
+00002990: 7328 7365 6c66 2c20 7461 626c 653a 2073  s(self, table: s
+000029a0: 7472 7c74 7570 6c65 2920 2d3e 2062 6f6f  tr|tuple) -> boo
+000029b0: 6c3a 0d0a 2020 2020 2020 2020 7261 6973  l:..        rais
+000029c0: 6520 4e6f 7449 6d70 6c65 6d65 6e74 6564  e NotImplemented
+000029d0: 4572 726f 7228 290d 0a0d 0a0d 0a20 2020  Error()......   
+000029e0: 2064 6566 2074 7275 6e63 6174 655f 7461   def truncate_ta
+000029f0: 626c 6528 7365 6c66 2c20 7461 626c 653a  ble(self, table:
+00002a00: 2073 7472 7c74 7570 6c65 293a 0d0a 2020   str|tuple):..  
+00002a10: 2020 2020 2020 7261 6973 6520 4e6f 7449        raise NotI
+00002a20: 6d70 6c65 6d65 6e74 6564 4572 726f 7228  mplementedError(
+00002a30: 290d 0a0d 0a0d 0a20 2020 2064 6566 2063  )......    def c
+00002a40: 6f70 795f 6672 6f6d 5f63 7376 2873 656c  opy_from_csv(sel
+00002a50: 662c 2066 703a 2049 4f42 6173 652c 2074  f, fp: IOBase, t
+00002a60: 6162 6c65 3a20 7374 727c 7475 706c 652c  able: str|tuple,
+00002a70: 2063 6f6c 756d 6e73 3a20 6c69 7374 5b73   columns: list[s
+00002a80: 7472 5d20 3d20 4e6f 6e65 2c20 6465 6c69  tr] = None, deli
+00002a90: 6d69 7465 723a 2073 7472 203d 204e 6f6e  miter: str = Non
+00002aa0: 652c 2071 756f 7465 6368 6172 3a20 7374  e, quotechar: st
+00002ab0: 7220 3d20 2722 272c 206e 756c 6c63 6861  r = '"', nullcha
+00002ac0: 723a 2073 7472 203d 2027 272c 206e 6f68  r: str = '', noh
+00002ad0: 6561 6465 723a 2062 6f6f 6c20 3d20 4661  eader: bool = Fa
+00002ae0: 6c73 6529 3a0d 0a20 2020 2020 2020 2072  lse):..        r
+00002af0: 6169 7365 204e 6f74 496d 706c 656d 656e  aise NotImplemen
+00002b00: 7465 6445 7272 6f72 2829 0d0a 2020 2020  tedError()..    
+00002b10: 0d0a 0d0a 2020 2020 6465 6620 6465 706c  ....    def depl
+00002b20: 6f79 5f73 716c 2873 656c 662c 202a 7061  oy_sql(self, *pa
+00002b30: 7468 733a 2050 6174 687c 7374 722c 2065  ths: Path|str, e
+00002b40: 6e63 6f64 696e 6720 3d20 2275 7466 2d38  ncoding = "utf-8
+00002b50: 2229 3a0d 0a20 2020 2020 2020 2061 6374  "):..        act
+00002b60: 7561 6c5f 7061 7468 733a 206c 6973 745b  ual_paths: list[
+00002b70: 5061 7468 5d20 3d20 5b5d 0d0a 2020 2020  Path] = []..    
+00002b80: 2020 2020 666f 7220 7061 7468 2069 6e20      for path in 
+00002b90: 7061 7468 733a 0d0a 2020 2020 2020 2020  paths:..        
+00002ba0: 2020 2020 6966 2069 7369 6e73 7461 6e63      if isinstanc
+00002bb0: 6528 7061 7468 2c20 7374 7229 3a0d 0a20  e(path, str):.. 
+00002bc0: 2020 2020 2020 2020 2020 2020 2020 2070                 p
+00002bd0: 6174 6820 3d20 5061 7468 2870 6174 6829  ath = Path(path)
+00002be0: 0d0a 2020 2020 2020 2020 2020 2020 6163  ..            ac
+00002bf0: 7475 616c 5f70 6174 6873 2e61 7070 656e  tual_paths.appen
+00002c00: 6428 7061 7468 290d 0a0d 0a20 2020 2020  d(path)....     
+00002c10: 2020 2061 6374 7561 6c5f 7061 7468 732e     actual_paths.
+00002c20: 736f 7274 2829 0d0a 0d0a 2020 2020 2020  sort()....      
+00002c30: 2020 666f 7220 7061 7468 2069 6e20 6163    for path in ac
+00002c40: 7475 616c 5f70 6174 6873 3a0d 0a20 2020  tual_paths:..   
+00002c50: 2020 2020 2020 2020 2069 6620 7061 7468           if path
+00002c60: 2e69 735f 6469 7228 293a 0d0a 2020 2020  .is_dir():..    
+00002c70: 2020 2020 2020 2020 2020 2020 7375 6270              subp
+00002c80: 6174 6873 203d 2073 6f72 7465 6428 7061  aths = sorted(pa
+00002c90: 7468 2e69 7465 7264 6972 2829 290d 0a20  th.iterdir()).. 
+00002ca0: 2020 2020 2020 2020 2020 2020 2020 2073                 s
+00002cb0: 656c 662e 6465 706c 6f79 5f73 716c 282a  elf.deploy_sql(*
+00002cc0: 7375 6270 6174 6873 2c20 656e 636f 6469  subpaths, encodi
+00002cd0: 6e67 3d65 6e63 6f64 696e 6729 0d0a 0d0a  ng=encoding)....
+00002ce0: 2020 2020 2020 2020 2020 2020 656c 6966              elif
+00002cf0: 206e 6f74 2070 6174 682e 6e61 6d65 2e65   not path.name.e
+00002d00: 6e64 7377 6974 6828 222e 7371 6c22 293a  ndswith(".sql"):
+00002d10: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
+00002d20: 2020 636f 6e74 696e 7565 2023 2069 676e    continue # ign
+00002d30: 6f72 650d 0a0d 0a20 2020 2020 2020 2020  ore....         
+00002d40: 2020 2065 6c69 6620 7061 7468 2e6e 616d     elif path.nam
+00002d50: 652e 656e 6473 7769 7468 2822 5f72 6576  e.endswith("_rev
+00002d60: 6572 742e 7371 6c22 293a 0d0a 2020 2020  ert.sql"):..    
+00002d70: 2020 2020 2020 2020 2020 2020 636f 6e74              cont
+00002d80: 696e 7565 2023 2069 676e 6f72 650d 0a0d  inue # ignore...
+00002d90: 0a20 2020 2020 2020 2020 2020 2065 6c73  .            els
+00002da0: 653a 0d0a 2020 2020 2020 2020 2020 2020  e:..            
+00002db0: 2020 2020 6c6f 6767 6572 2e69 6e66 6f28      logger.info(
+00002dc0: 2265 7865 6375 7465 2025 7322 2c20 7061  "execute %s", pa
+00002dd0: 7468 290d 0a20 2020 2020 2020 2020 2020  th)..           
+00002de0: 2020 2020 2071 7565 7279 203d 2070 6174       query = pat
+00002df0: 682e 7265 6164 5f74 6578 7428 656e 636f  h.read_text(enco
+00002e00: 6469 6e67 3d65 6e63 6f64 696e 6729 0d0a  ding=encoding)..
+00002e10: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002e20: 7365 6c66 2e65 7865 6375 7465 2871 7565  self.execute(que
+00002e30: 7279 290d 0a0d 0a0d 0a20 2020 2064 6566  ry)......    def
+00002e40: 2072 6576 6572 745f 7371 6c28 7365 6c66   revert_sql(self
+00002e50: 2c20 2a70 6174 6873 3a20 5061 7468 7c73  , *paths: Path|s
+00002e60: 7472 2c20 656e 636f 6469 6e67 203d 2022  tr, encoding = "
+00002e70: 7574 662d 3822 293a 0d0a 2020 2020 2020  utf-8"):..      
+00002e80: 2020 6163 7475 616c 5f70 6174 6873 3a20    actual_paths: 
+00002e90: 6c69 7374 5b50 6174 685d 203d 205b 5d0d  list[Path] = [].
+00002ea0: 0a20 2020 2020 2020 2066 6f72 2070 6174  .        for pat
+00002eb0: 6820 696e 2070 6174 6873 3a0d 0a20 2020  h in paths:..   
+00002ec0: 2020 2020 2020 2020 2069 6620 6973 696e           if isin
+00002ed0: 7374 616e 6365 2870 6174 682c 2073 7472  stance(path, str
+00002ee0: 293a 0d0a 2020 2020 2020 2020 2020 2020  ):..            
+00002ef0: 2020 2020 7061 7468 203d 2050 6174 6828      path = Path(
+00002f00: 7061 7468 290d 0a20 2020 2020 2020 2020  path)..         
+00002f10: 2020 2061 6374 7561 6c5f 7061 7468 732e     actual_paths.
+00002f20: 6170 7065 6e64 2870 6174 6829 0d0a 0d0a  append(path)....
+00002f30: 2020 2020 2020 2020 6163 7475 616c 5f70          actual_p
+00002f40: 6174 6873 2e73 6f72 7428 7265 7665 7273  aths.sort(revers
+00002f50: 653d 5472 7565 290d 0a0d 0a20 2020 2020  e=True)....     
+00002f60: 2020 2066 6f72 2070 6174 6820 696e 2061     for path in a
+00002f70: 6374 7561 6c5f 7061 7468 733a 0d0a 2020  ctual_paths:..  
+00002f80: 2020 2020 2020 2020 2020 6966 2070 6174            if pat
+00002f90: 682e 6973 5f64 6972 2829 3a0d 0a20 2020  h.is_dir():..   
+00002fa0: 2020 2020 2020 2020 2020 2020 2073 7562               sub
+00002fb0: 7061 7468 7320 3d20 736f 7274 6564 2870  paths = sorted(p
+00002fc0: 6174 682e 6974 6572 6469 7228 2929 0d0a  ath.iterdir())..
+00002fd0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00002fe0: 7365 6c66 2e72 6576 6572 745f 7371 6c28  self.revert_sql(
+00002ff0: 2a73 7562 7061 7468 732c 2065 6e63 6f64  *subpaths, encod
+00003000: 696e 673d 656e 636f 6469 6e67 290d 0a0d  ing=encoding)...
+00003010: 0a20 2020 2020 2020 2020 2020 2065 6c69  .            eli
+00003020: 6620 6e6f 7420 7061 7468 2e6e 616d 652e  f not path.name.
+00003030: 656e 6473 7769 7468 2822 5f72 6576 6572  endswith("_rever
+00003040: 742e 7371 6c22 293a 0d0a 2020 2020 2020  t.sql"):..      
+00003050: 2020 2020 2020 2020 2020 636f 6e74 696e            contin
+00003060: 7565 2023 2069 676e 6f72 650d 0a0d 0a20  ue # ignore.... 
+00003070: 2020 2020 2020 2020 2020 2065 6c73 653a             else:
+00003080: 0d0a 2020 2020 2020 2020 2020 2020 2020  ..              
+00003090: 2020 6c6f 6767 6572 2e69 6e66 6f28 2265    logger.info("e
+000030a0: 7865 6375 7465 2025 7322 2c20 7061 7468  xecute %s", path
+000030b0: 290d 0a20 2020 2020 2020 2020 2020 2020  )..             
+000030c0: 2020 2071 7565 7279 203d 2070 6174 682e     query = path.
+000030d0: 7265 6164 5f74 6578 7428 656e 636f 6469  read_text(encodi
+000030e0: 6e67 3d65 6e63 6f64 696e 6729 0d0a 2020  ng=encoding)..  
+000030f0: 2020 2020 2020 2020 2020 2020 2020 7365                se
+00003100: 6c66 2e65 7865 6375 7465 2871 7565 7279  lf.execute(query
+00003110: 290d 0a0d 0a0d 0a63 6c61 7373 2043 6f6c  )......class Col
+00003120: 756d 6e3a 0d0a 2020 2020 6465 6620 5f5f  umn:..    def __
+00003130: 696e 6974 5f5f 2873 656c 662c 206e 616d  init__(self, nam
+00003140: 653a 2073 7472 2c20 2a2c 2070 6f73 6974  e: str, *, posit
+00003150: 696f 6e3a 2069 6e74 203d 204e 6f6e 652c  ion: int = None,
+00003160: 2070 7974 686f 6e5f 7479 7065 3a20 7479   python_type: ty
+00003170: 7065 203d 204e 6f6e 652c 206f 7269 6769  pe = None, origi
+00003180: 6e61 6c5f 7479 7065 3a20 7374 7220 3d20  nal_type: str = 
+00003190: 4e6f 6e65 2c20 7072 6563 6973 696f 6e3a  None, precision:
+000031a0: 2069 6e74 203d 204e 6f6e 652c 2073 6361   int = None, sca
+000031b0: 6c65 3a20 696e 7420 3d20 4e6f 6e65 2c20  le: int = None, 
+000031c0: 6e75 6c6c 5f6f 6b3a 2062 6f6f 6c20 3d20  null_ok: bool = 
+000031d0: 4e6f 6e65 2c20 6469 7370 6c61 795f 7369  None, display_si
+000031e0: 7a65 3a20 696e 7420 3d20 4e6f 6e65 2c20  ze: int = None, 
+000031f0: 696e 7465 726e 616c 5f73 697a 653a 2069  internal_size: i
+00003200: 6e74 203d 204e 6f6e 652c 2070 7269 6d61  nt = None, prima
+00003210: 7279 5f6b 6579 3a20 626f 6f6c 203d 2046  ry_key: bool = F
+00003220: 616c 7365 2c20 6465 6661 756c 743a 2073  alse, default: s
+00003230: 7472 7c4c 6974 6572 616c 2827 5f5f 6e6f  tr|Literal('__no
+00003240: 775f 5f27 2920 3d20 4e6f 6e65 293a 0d0a  w__') = None):..
+00003250: 2020 2020 2020 2020 7365 6c66 2e6e 616d          self.nam
+00003260: 653a 2073 7472 7c4e 6f6e 6520 3d20 6e61  e: str|None = na
+00003270: 6d65 0d0a 2020 2020 2020 2020 2222 2220  me..        """ 
+00003280: 4e6f 6d20 6465 206c 6120 636f 6c6f 6e6e  Nom de la colonn
+00003290: 652e 2050 6575 7420 c3aa 7472 6520 7669  e. Peut ..tre vi
+000032a0: 6465 2028 7061 7220 6578 656d 706c 6520  de (par exemple 
+000032b0: 706f 7572 206c 6573 2063 6f6c 6f6e 6e65  pour les colonne
+000032c0: 7320 6361 6c63 756c c3a9 6573 2073 616e  s calcul..es san
+000032d0: 7320 616c 6961 7320 64c3 a966 696e 6929  s alias d..fini)
+000032e0: 2e20 2222 220d 0a0d 0a20 2020 2020 2020  . """....       
+000032f0: 2073 656c 662e 706f 7369 7469 6f6e 3a20   self.position: 
+00003300: 696e 747c 4e6f 6e65 203d 2070 6f73 6974  int|None = posit
+00003310: 696f 6e0d 0a20 2020 2020 2020 2022 2222  ion..        """
+00003320: 204f 7264 696e 616c 2070 6f73 6974 696f   Ordinal positio
+00003330: 6e2c 2073 7461 7274 696e 6720 6672 6f6d  n, starting from
+00003340: 2031 2e20 2222 220d 0a0d 0a20 2020 2020   1. """....     
+00003350: 2020 2073 656c 662e 7079 7468 6f6e 5f74     self.python_t
+00003360: 7970 653a 2074 7970 657c 4e6f 6e65 203d  ype: type|None =
+00003370: 2070 7974 686f 6e5f 7479 7065 0d0a 2020   python_type..  
+00003380: 2020 2020 2020 2222 2220 5479 7065 2050        """ Type P
+00003390: 7974 686f 6e20 7574 696c 6973 c3a9 2070  ython utilis.. p
+000033a0: 6172 206c 6520 6472 6976 6572 2070 6f75  ar le driver pou
+000033b0: 7220 7265 7072 c3a9 7365 6e74 6572 206c  r repr..senter l
+000033c0: 6573 2076 616c 6575 7273 2e0d 0a20 2020  es valeurs...   
+000033d0: 2020 2020 2041 7474 656e 7469 6f6e 203a       Attention :
+000033e0: 206c 6573 2076 616c 6575 7273 2064 6520   les valeurs de 
+000033f0: 7479 7065 2053 514c 2044 4154 4520 6f75  type SQL DATE ou
+00003400: 2044 4154 4554 494d 4520 7065 7576 656e   DATETIME peuven
+00003410: 7420 c3aa 7472 6520 6d61 7070 c3a9 6573  t ..tre mapp..es
+00003420: 2061 7520 7479 7065 2050 7974 686f 6e20   au type Python 
+00003430: 6073 7472 6020 7375 6976 616e 7420 6c65  `str` suivant le
+00003440: 2064 7269 7665 7220 7574 696c 6973 c3a9   driver utilis..
+00003450: 2e20 2222 220d 0a20 2020 2020 2020 2023  . """..        #
+00003460: 2043 662e 2068 7474 7073 3a2f 2f73 7461   Cf. https://sta
+00003470: 636b 6f76 6572 666c 6f77 2e63 6f6d 2f71  ckoverflow.com/q
+00003480: 7565 7374 696f 6e73 2f37 3137 3235 3430  uestions/7172540
+00003490: 2f70 796f 6462 632d 7265 7475 726e 732d  /pyodbc-returns-
+000034a0: 7371 6c2d 7365 7276 6572 2d64 6174 652d  sql-server-date-
+000034b0: 6669 656c 6473 2d61 732d 7374 7269 6e67  fields-as-string
+000034c0: 730d 0a0d 0a20 2020 2020 2020 2073 656c  s....        sel
+000034d0: 662e 6f72 6967 696e 616c 5f74 7970 653a  f.original_type:
+000034e0: 2073 7472 7c4e 6f6e 6520 3d20 6f72 6967   str|None = orig
+000034f0: 696e 616c 5f74 7970 650d 0a0d 0a20 2020  inal_type....   
+00003500: 2020 2020 2073 656c 662e 7072 6563 6973       self.precis
+00003510: 696f 6e3a 2069 6e74 7c4e 6f6e 6520 3d20  ion: int|None = 
+00003520: 7072 6563 6973 696f 6e0d 0a20 2020 2020  precision..     
+00003530: 2020 2022 2222 2054 6f74 616c 206e 756d     """ Total num
+00003540: 6265 7220 6f66 2073 6967 6e69 6669 6361  ber of significa
+00003550: 6e74 2064 6967 6974 732e 0d0a 2020 2020  nt digits...    
+00003560: 2020 2020 2d20 6d73 7371 6c3a 2061 6c77      - mssql: alw
+00003570: 6179 7320 7365 742e 2022 2222 0d0a 2020  ays set. """..  
+00003580: 2020 2020 2020 0d0a 2020 2020 2020 2020        ..        
+00003590: 7365 6c66 2e73 6361 6c65 3a20 696e 747c  self.scale: int|
+000035a0: 4e6f 6e65 203d 2073 6361 6c65 0d0a 2020  None = scale..  
+000035b0: 2020 2020 2020 2222 2220 436f 756e 7420        """ Count 
+000035c0: 6f66 2064 6563 696d 616c 2064 6967 6974  of decimal digit
+000035d0: 7320 696e 2074 6865 2066 7261 6374 696f  s in the fractio
+000035e0: 6e61 6c20 7061 7274 206f 6620 7468 6520  nal part of the 
+000035f0: 7479 7065 2e0d 0a20 2020 2020 2020 202d  type...        -
+00003600: 206d 7373 716c 3a20 616c 7761 7973 2073   mssql: always s
+00003610: 6574 2c20 7573 696e 6720 3020 6966 206e  et, using 0 if n
+00003620: 6f6e 652e 2022 2222 0d0a 0d0a 2020 2020  one. """....    
+00003630: 2020 2020 7365 6c66 2e6e 756c 6c5f 6f6b      self.null_ok
+00003640: 3a20 626f 6f6c 7c4e 6f6e 6520 3d20 6e75  : bool|None = nu
+00003650: 6c6c 5f6f 6b0d 0a20 2020 2020 2020 2022  ll_ok..        "
+00003660: 2222 2049 6e64 6963 6174 6520 6966 2074  "" Indicate if t
+00003670: 6865 2063 6f6c 756d 6e20 6163 6365 7074  he column accept
+00003680: 7320 6e75 6c6c 2076 616c 7565 2e0d 0a20  s null value... 
+00003690: 2020 2020 2020 202d 2070 673a 2061 6c77         - pg: alw
+000036a0: 6179 7320 4e6f 6e65 2061 7320 6e6f 7420  ays None as not 
+000036b0: 6561 7379 2074 6f20 7265 7472 6965 7665  easy to retrieve
+000036c0: 2066 726f 6d20 7468 6520 6c69 6270 712e   from the libpq.
+000036d0: 0d0a 2020 2020 2020 2020 2222 220d 0a0d  ..        """...
+000036e0: 0a20 2020 2020 2020 2073 656c 662e 6469  .        self.di
+000036f0: 7370 6c61 795f 7369 7a65 3a20 696e 747c  splay_size: int|
+00003700: 4e6f 6e65 203d 2064 6973 706c 6179 5f73  None = display_s
+00003710: 697a 650d 0a20 2020 2020 2020 2022 2222  ize..        """
+00003720: 2054 6865 2061 6374 7561 6c20 6c65 6e67   The actual leng
+00003730: 7468 206f 6620 7468 6520 636f 6c75 6d6e  th of the column
+00003740: 2069 6e20 6279 7465 732e 0d0a 2020 2020   in bytes...    
+00003750: 2020 2020 2d20 6d73 7371 6c3a 2061 6c77      - mssql: alw
+00003760: 6179 7320 4e6f 6e65 2028 6f72 6967 696e  ays None (origin
+00003770: 616c 6c79 2061 6c77 6179 7320 7a65 726f  ally always zero
+00003780: 292e 2022 2222 0d0a 0d0a 2020 2020 2020  ). """....      
+00003790: 2020 7365 6c66 2e69 6e74 6572 6e61 6c5f    self.internal_
+000037a0: 7369 7a65 3a20 696e 747c 4e6f 6e65 203d  size: int|None =
+000037b0: 2069 6e74 6572 6e61 6c5f 7369 7a65 0d0a   internal_size..
+000037c0: 2020 2020 2020 2020 2222 2220 5468 6520          """ The 
+000037d0: 7369 7a65 2069 6e20 6279 7465 7320 6f66  size in bytes of
+000037e0: 2074 6865 2063 6f6c 756d 6e20 6173 736f   the column asso
+000037f0: 6369 6174 6564 2074 6f20 7468 6973 2063  ciated to this c
+00003800: 6f6c 756d 6e20 6f6e 2074 6865 2073 6572  olumn on the ser
+00003810: 7665 722e 0d0a 2020 2020 2020 2020 2d20  ver...        - 
+00003820: 6d73 7371 6c3a 2061 6c77 6179 7320 4e6f  mssql: always No
+00003830: 6e65 2028 6f72 6967 696e 616c 6c79 2061  ne (originally a
+00003840: 6c77 6179 7320 7361 6d65 2061 7320 7072  lways same as pr
+00003850: 6563 6973 696f 6e29 2e20 2222 220d 0a0d  ecision). """...
+00003860: 0a20 2020 2020 2020 2073 656c 662e 7072  .        self.pr
+00003870: 696d 6172 795f 6b65 793a 2062 6f6f 6c20  imary_key: bool 
+00003880: 3d20 7072 696d 6172 795f 6b65 790d 0a0d  = primary_key...
+00003890: 0a20 2020 2020 2020 2073 656c 662e 6465  .        self.de
+000038a0: 6661 756c 743a 2073 7472 7c4c 6974 6572  fault: str|Liter
+000038b0: 616c 2827 5f5f 6e6f 775f 5f27 297c 4e6f  al('__now__')|No
+000038c0: 6e65 203d 2064 6566 6175 6c74 0d0a 0d0a  ne = default....
+000038d0: 0d0a 2020 2020 6465 6620 5f63 6865 636b  ..    def _check
+000038e0: 2873 656c 6629 3a0d 0a20 2020 2020 2020  (self):..       
+000038f0: 2022 2222 0d0a 2020 2020 2020 2020 456e   """..        En
+00003900: 7375 7265 2063 6f6e 7369 7374 656e 6379  sure consistency
+00003910: 206f 6620 636f 6c75 6d6e 2064 6566 696e   of column defin
+00003920: 6974 696f 6e2e 0d0a 0d0a 2020 2020 2020  ition.....      
+00003930: 2020 4361 6c6c 6564 2061 6674 6572 2063    Called after c
+00003940: 7265 6174 696f 6e20 6f66 2043 6f6c 756d  reation of Colum
+00003950: 6e20 696e 7374 616e 6365 2061 6e64 2070  n instance and p
+00003960: 6f74 656e 7469 616c 2066 6978 2061 6461  otential fix ada
+00003970: 7074 6564 2074 6f20 7370 6563 6966 6963  pted to specific
+00003980: 2044 6174 6162 6173 6520 656e 6769 6e65   Database engine
+00003990: 732e 0d0a 2020 2020 2020 2020 2222 220d  s...        """.
+000039a0: 0a20 2020 2020 2020 2069 6620 7365 6c66  .        if self
+000039b0: 2e70 7974 686f 6e5f 7479 7065 2069 7320  .python_type is 
+000039c0: 6e6f 7420 4e6f 6e65 2061 6e64 206e 6f74  not None and not
+000039d0: 2069 7369 6e73 7461 6e63 6528 7365 6c66   isinstance(self
+000039e0: 2e70 7974 686f 6e5f 7479 7065 2c20 7479  .python_type, ty
+000039f0: 7065 293a 0d0a 2020 2020 2020 2020 2020  pe):..          
+00003a00: 2020 7261 6973 6520 5661 6c75 6545 7272    raise ValueErr
+00003a10: 6f72 2866 2269 6e76 616c 6964 2070 7974  or(f"invalid pyt
+00003a20: 686f 6e5f 7479 7065 205c 227b 7365 6c66  hon_type \"{self
+00003a30: 2e70 7974 686f 6e5f 7479 7065 7d5c 2220  .python_type}\" 
+00003a40: 287b 7479 7065 2873 656c 662e 7079 7468  ({type(self.pyth
+00003a50: 6f6e 5f74 7970 6529 2e5f 5f6e 616d 655f  on_type).__name_
+00003a60: 5f7d 293a 2065 7870 6563 7465 6420 7479  _}): expected ty
+00003a70: 7065 2069 6e73 7461 6e63 6522 290d 0a0d  pe instance")...
+00003a80: 0a0d 0a20 2020 2064 6566 205f 5f73 7472  ...    def __str
+00003a90: 5f5f 2873 656c 6629 202d 3e20 7374 723a  __(self) -> str:
+00003aa0: 0d0a 2020 2020 2020 2020 7265 7475 726e  ..        return
+00003ab0: 2073 656c 662e 6e61 6d65 0d0a             self.name..
```

## zut/db/mssql.py

```diff
@@ -1,69 +1,74 @@
-"""
-Accs  la base de donnes interface.
-"""
-from __future__ import annotations
-import logging
-import re
-from .commons import Column, DbWrapper
-
-try:
-    from pyodbc import Connection, Cursor, connect
-    _available = True
-except ImportError:
-    Connection = type(None)
-    Cursor = type(None)
-    _available = False
-
-logger = logging.getLogger(__name__)
-
-
-class MssqlWrapper(DbWrapper[Connection, Cursor]):
-    @classmethod
-    def is_available(cls):
-        return _available
-    
-    default_schema_name = 'dbo'
-
-    def _create_connection(self) -> Connection:
-        server = self._host or 'localhost'
-        if self._port:
-            server += f',{self._port}'
-        
-        connection_string = 'Driver={SQL Server};Server=%s;Database=%s;' % (server, self._database)
-        loggable_connection_string = connection_string
-
-        if self._user:
-            connection_string += 'User=%s;Password=%s;' % (self._user, self._password)
-            loggable_connection_string += 'User=%s;Password=%s;' % (self._user, re.sub(r'.', '*', self._password))
-        else:
-            credentials = 'Trusted_Connection=yes;'
-            connection_string += credentials
-            loggable_connection_string += credentials
-
-        logger.debug("create mssql connection: %s", loggable_connection_string)
-        return connect(connection_string)
-    
-
-    def execute_get_cursor(self, query: str, params: list|tuple|dict = None, limit: int = None):
-        # Example of positional param: cursor.execute(... ? TODO, *["bar"])
-        # Example of named param:      not possible directly, use build_query_with_positional_params()
-        if limit is not None:
-            query = self.limit_query(query, limit=limit)
-        
-        if isinstance(params, dict):
-            query, params = self.build_query_with_positional_params(query, params)
-        elif not params:
-            params = []
-        
-        cursor = self.cursor()
-        cursor.execute(query, *params)
-        return cursor
-
-
-    def _limit_parsed_query(self, query: str, limit: int):
-        return f"SELECT TOP {limit} * FROM ({query}) s"
-
-
-    def _fix_cursor_column_definition(self, column: Column):
-        # internal_size isn't meaningfull: it always equals precision
-        column.internal_size = None
+"""
+Accs  la base de donnes interface.
+"""
+from __future__ import annotations
+import logging
+import re
+from .commons import Column, DbWrapper
+
+try:
+    from pyodbc import Connection, Cursor, connect
+    _available = True
+except ImportError:
+    Connection = type(None)
+    Cursor = type(None)
+    _available = False
+
+logger = logging.getLogger(__name__)
+
+
+class MssqlWrapper(DbWrapper[Connection, Cursor]):
+    @classmethod
+    def is_available(cls):
+        return _available
+    
+    scheme = 'mssql'
+    default_schema_name = 'dbo'
+    compatible_django_engines = [
+        'mssql', # `mssql-django` (Microsoft official fork of `django-mssql-backend`): https://github.com/microsoft/mssql-django
+        'sql_server.pyodbc', # `django-mssql-backend`: https://pypi.org/project/django-mssql-backend/
+    ]
+
+    def _create_connection(self) -> Connection:
+        server = self._host or 'localhost'
+        if self._port:
+            server += f',{self._port}'
+        
+        connection_string = 'Driver={SQL Server};Server=%s;Database=%s;' % (server, self._name)
+        loggable_connection_string = connection_string
+
+        if self._user:
+            connection_string += 'User=%s;Password=%s;' % (self._user, self._password)
+            loggable_connection_string += 'User=%s;Password=%s;' % (self._user, re.sub(r'.', '*', self._password))
+        else:
+            credentials = 'Trusted_Connection=yes;'
+            connection_string += credentials
+            loggable_connection_string += credentials
+
+        logger.debug("create mssql connection: %s", loggable_connection_string)
+        return connect(connection_string)
+    
+
+    def execute_get_cursor(self, query: str, params: list|tuple|dict = None, limit: int = None):
+        # Example of positional param: cursor.execute(... ? TODO, *["bar"])
+        # Example of named param:      not possible directly, use build_query_with_positional_params()
+        if limit is not None:
+            query = self.limit_query(query, limit=limit)
+        
+        if isinstance(params, dict):
+            query, params = self.build_query_with_positional_params(query, params)
+        elif not params:
+            params = []
+        
+        cursor = self.cursor()
+        cursor.execute(query, *params)
+        return cursor
+
+
+    def _limit_parsed_query(self, query: str, limit: int):
+        return f"SELECT TOP {limit} * FROM ({query}) s"
+
+
+    def _fix_cursor_column_definition(self, column: Column):
+        # internal_size isn't meaningfull: it always equals precision
+        column.internal_size = None
```

## zut/db/pg.py

```diff
@@ -1,255 +1,257 @@
-from __future__ import annotations
-from io import IOBase
-import re
-import logging
-from typing import Any
-from ..csv import get_default_csv_delimiter
-from .commons import Column, DbWrapper
-
-try:
-    from psycopg2 import connect, sql, DataError
-    from psycopg2.extensions import connection, cursor, string_types
-    _available = True
-except ImportError:
-    connection = type(None)
-    cursor = type(None)
-    _available = False
-
-logger = logging.getLogger(__name__)
-
-
-class PgWrapper(DbWrapper[connection, cursor]):
-    @classmethod
-    def is_available(cls):
-        return _available
-    
-    default_schema_name = 'public'
-
-    def _create_connection(self):
-        connect_kwargs = {'database': self._database}        
-        if self._host:
-            connect_kwargs['host'] = self._host
-        if self._user:
-            connect_kwargs['user'] = self._user
-        if self._password:
-            connect_kwargs['password'] = self._password
-        if self._port:
-            connect_kwargs['port'] = self._port
-
-        if logger.isEnabledFor(logging.DEBUG):
-            loggable_connect_kwargs = dict(connect_kwargs)
-            if 'password' in loggable_connect_kwargs:
-                loggable_connect_kwargs['password'] = re.sub(r'.', '*', loggable_connect_kwargs['password'])
-
-            logger.debug(f"create pg connection: %s", ', '.join(f"{key}={value}" for key, value in loggable_connect_kwargs.items()))
-
-        conn = connect(**connect_kwargs)
-        conn.autocommit = True
-        return conn
-    
-
-    def execute_get_cursor(self, query: str, params: list|tuple|dict = None, limit: int = None):
-        # Example of positional param: cursor.execute("INSERT INTO foo VALUES (%s)", ["bar"])
-        # Example of named param: cursor.execute("INSERT INTO foo VALUES (%(foo)s)", {"foo": "bar"})
-        if limit is not None:
-            query = self.limit_query(query, limit=limit)
-        
-        if params is None:
-            params = []
-
-        cursor = self.cursor()
-        cursor.execute(query, params)
-        return cursor
-    
-
-    def _limit_parsed_query(self, query: str, limit: int):
-        return f"SELECT * FROM ({query}) s LIMIT {limit}"
-
-
-    def get_select_table_query(self, table: str|tuple, schema_only = False):
-        schema, table = self.split_name(table)
-        
-        query = 'SELECT * FROM {}.{}'
-        params = [sql.Identifier(schema), sql.Identifier(table)]
-        if schema_only:
-            query += ' WHERE false'
-
-        return sql.SQL(query).format(*params)
-    
-
-    def call_procedure(self, name: str|tuple, *args):
-        schema_name, name = self.split_name(name)
-
-        query = "CALL {}.{}("
-        params = [sql.Identifier(schema_name), sql.Identifier(name)]
-
-        first = True
-        for arg in args:
-            if first:
-                first = False
-            else:
-                query += ", "
-            query += "{}"
-            params += [self.get_flexible_param(arg)]
-
-        query += ")"
-
-        logger.info(f"execute {schema_name}.{name}")
-
-        previous_notices_len = len(self.connection.notices)
-        log_handler = PgLogHandler(f"pg:{schema_name}.{name}")
-        try:
-            with self.cursor() as cursor:
-                cursor.execute(sql.SQL(query).format(*params))
-        finally:
-            for notice in self.connection.notices[previous_notices_len:]:
-                log_handler.append(notice, (schema_name, name))
-
-
-    def get_flexible_param(self, value: Any) -> sql.Composable:
-        if value is None:
-            return sql.SQL("null")
-        elif value == '__now__':
-            return sql.SQL("NOW()")
-        elif isinstance(value, sql.Composable):
-            return value
-        else:
-            return sql.Literal(value)
-
-
-    def table_exists(self, table: str|tuple) -> bool:
-        schema, table = self.split_name(table)
-
-        query = "SELECT EXISTS (SELECT FROM pg_tables WHERE schemaname = {} AND tablename = {})"
-        params = [sql.Literal(schema), sql.Literal(table)]
-
-        return self.execute_get_scalar(sql.SQL(query).format(*params))
-        
-
-    def truncate_table(self, table: str|tuple):
-        schema, table = self.split_name(table)
-
-        query = "TRUNCATE {}.{}"
-        params = [sql.Identifier(schema), sql.Identifier(table)]
-
-        self.execute(sql.SQL(query).format(*params))
-    
-
-    def copy_from_csv(self, fp: IOBase, table: str|tuple, columns: list[str] = None, delimiter: str = None, quotechar: str = '"', nullchar: str = '', noheader: bool = False):
-        schema, table = self.split_name(table)
-
-        if delimiter is None:
-            delimiter = get_default_csv_delimiter()
-
-        query = "COPY {}.{}"
-        params = [sql.Identifier(schema), sql.Identifier(table)]
-
-        if columns:
-            query += " ("
-            for i, column in enumerate(columns):
-                if i > 0:
-                    query += ", "
-                query += "{}"
-
-                params.append(sql.Identifier(column))
-            query += ")"
-
-        query += " FROM STDIN WITH CSV"
-
-        query += ' NULL {}'
-        params.append(sql.Literal(nullchar))
-
-        query += ' DELIMITER {}'
-        params.append(sql.Literal(delimiter))
-
-        query += ' QUOTE {}'
-        params.append(sql.Literal(quotechar))
-        
-        query += ' ESCAPE {}'
-        params.append(sql.Literal(quotechar))
-
-        if not noheader:
-            query += " HEADER"
-
-        with self.cursor() as cursor:
-            cursor.copy_expert(sql.SQL(query).format(*params), fp)
-
-
-    def _fix_cursor_column_definition(self, column: Column):
-        # `python_type` is actually a type code.
-        # We will determine the actual python_type from the type_code.
-        # see: https://stackoverflow.com/a/53327695
-
-        type_code: Any = column.python_type
-
-        column.python_type = None
-        column.original_type = None
-
-        try:
-            string_type = string_types[type_code]
-            column.original_type = string_type.name
-
-            if 'BOOL' in column.original_type:
-                value = 'true'
-                column.python_type = type(string_type(value, cursor))
-            elif 'DATE' in column.original_type:
-                value = '2000-01-01'
-                column.python_type = type(string_type(value, cursor))
-            elif 'ARRAY' in column.original_type:
-                value = '{}'
-                column.python_type = type(string_type(value, cursor))
-            elif 'JSON' in column.original_type:
-                column.python_type = None
-            else:
-                value = '100'
-                column.python_type = type(string_type(value, cursor))
-
-        except DataError as err:
-            logger.error(f"cannot determine Python type of column \"{column.name}\" ({column.original_type or f'type_code={type_code}'}): {err}")
-        
-
-class PgLogHandler:
-    """
-    Usage example:
-    ```
-    from django.apps import AppConfig
-    from django.db.backends.signals import connection_created
-    from zut.db.pg import PgLogHandler
-
-    def connection_created_receiver(sender, connection, **kwargs):
-        if connection.alias == "default":
-            connection.connection.notices = PgLogHandler(f"pg:{connection.alias}")
-
-    class MainConfig(AppConfig):
-        default_auto_field = "django.db.models.BigAutoField"
-        name = "main"
-        
-        def ready(self):
-            connection_created.connect(connection_created_receiver)
-    ```
-    """
-    _pg_msg_re = re.compile(r"^(?P<pglevel>[A-Z]+)\:\s(?P<message>.+(?:\r?\n.*)*)$", re.MULTILINE)
-
-    def __init__(self, logger_name: str = 'pg'):
-        self._logger = logging.getLogger(logger_name)
-    
-    def append(self, fullmsg: str):
-        fullmsg = fullmsg.strip()
-        m = self._pg_msg_re.match(fullmsg)
-        if not m:
-            self._logger.warning(fullmsg)
-            return
-
-        message = m.group("message").strip()
-        pglevel = m.group("pglevel")
-        if pglevel == "EXCEPTION":
-            level = logging.ERROR
-        elif pglevel == "WARNING":
-            level = logging.WARNING
-        else:
-            level = logging.INFO
-
-        if level <= logging.INFO and message.endswith("\" does not exist, skipping"):
-            return
-
-        self._logger.log(level, message)
+from __future__ import annotations
+from io import IOBase
+import re
+import logging
+from typing import Any
+from ..csv import get_default_csv_delimiter
+from .commons import Column, DbWrapper
+
+try:
+    from psycopg2 import connect, sql, DataError
+    from psycopg2.extensions import connection, cursor, string_types
+    _available = True
+except ImportError:
+    connection = type(None)
+    cursor = type(None)
+    _available = False
+
+logger = logging.getLogger(__name__)
+
+
+class PgWrapper(DbWrapper[connection, cursor]):
+    @classmethod
+    def is_available(cls):
+        return _available
+    
+    scheme = 'pg'
+    default_schema_name = 'public'
+    compatible_django_engines = ['django.db.backends.postgresql', 'django.contrib.gis.db.backends.postgis']
+
+    def _create_connection(self):
+        connect_kwargs = {'database': self._name}        
+        if self._host:
+            connect_kwargs['host'] = self._host
+        if self._user:
+            connect_kwargs['user'] = self._user
+        if self._password:
+            connect_kwargs['password'] = self._password
+        if self._port:
+            connect_kwargs['port'] = self._port
+
+        if logger.isEnabledFor(logging.DEBUG):
+            loggable_connect_kwargs = dict(connect_kwargs)
+            if 'password' in loggable_connect_kwargs:
+                loggable_connect_kwargs['password'] = re.sub(r'.', '*', loggable_connect_kwargs['password'])
+
+            logger.debug(f"create pg connection: %s", ', '.join(f"{key}={value}" for key, value in loggable_connect_kwargs.items()))
+
+        conn = connect(**connect_kwargs)
+        conn.autocommit = True
+        return conn
+    
+
+    def execute_get_cursor(self, query: str, params: list|tuple|dict = None, limit: int = None):
+        # Example of positional param: cursor.execute("INSERT INTO foo VALUES (%s)", ["bar"])
+        # Example of named param: cursor.execute("INSERT INTO foo VALUES (%(foo)s)", {"foo": "bar"})
+        if limit is not None:
+            query = self.limit_query(query, limit=limit)
+        
+        if params is None:
+            params = []
+
+        cursor = self.cursor()
+        cursor.execute(query, params)
+        return cursor
+    
+
+    def _limit_parsed_query(self, query: str, limit: int):
+        return f"SELECT * FROM ({query}) s LIMIT {limit}"
+
+
+    def get_select_table_query(self, table: str|tuple, schema_only = False):
+        schema, table = self.split_name(table)
+        
+        query = 'SELECT * FROM {}.{}'
+        params = [sql.Identifier(schema), sql.Identifier(table)]
+        if schema_only:
+            query += ' WHERE false'
+
+        return sql.SQL(query).format(*params)
+    
+
+    def call_procedure(self, name: str|tuple, *args):
+        schema_name, name = self.split_name(name)
+
+        query = "CALL {}.{}("
+        params = [sql.Identifier(schema_name), sql.Identifier(name)]
+
+        first = True
+        for arg in args:
+            if first:
+                first = False
+            else:
+                query += ", "
+            query += "{}"
+            params += [self.get_flexible_param(arg)]
+
+        query += ")"
+
+        logger.info(f"execute {schema_name}.{name}")
+
+        previous_notices_len = len(self.connection.notices)
+        log_handler = PgLogHandler(f"pg:{schema_name}.{name}")
+        try:
+            with self.cursor() as cursor:
+                cursor.execute(sql.SQL(query).format(*params))
+        finally:
+            for notice in self.connection.notices[previous_notices_len:]:
+                log_handler.append(notice, (schema_name, name))
+
+
+    def get_flexible_param(self, value: Any) -> sql.Composable:
+        if value is None:
+            return sql.SQL("null")
+        elif value == '__now__':
+            return sql.SQL("NOW()")
+        elif isinstance(value, sql.Composable):
+            return value
+        else:
+            return sql.Literal(value)
+
+
+    def table_exists(self, table: str|tuple) -> bool:
+        schema, table = self.split_name(table)
+
+        query = "SELECT EXISTS (SELECT FROM pg_tables WHERE schemaname = {} AND tablename = {})"
+        params = [sql.Literal(schema), sql.Literal(table)]
+
+        return self.execute_get_scalar(sql.SQL(query).format(*params))
+        
+
+    def truncate_table(self, table: str|tuple):
+        schema, table = self.split_name(table)
+
+        query = "TRUNCATE {}.{}"
+        params = [sql.Identifier(schema), sql.Identifier(table)]
+
+        self.execute(sql.SQL(query).format(*params))
+    
+
+    def copy_from_csv(self, fp: IOBase, table: str|tuple, columns: list[str] = None, delimiter: str = None, quotechar: str = '"', nullchar: str = '', noheader: bool = False):
+        schema, table = self.split_name(table)
+
+        if delimiter is None:
+            delimiter = get_default_csv_delimiter()
+
+        query = "COPY {}.{}"
+        params = [sql.Identifier(schema), sql.Identifier(table)]
+
+        if columns:
+            query += " ("
+            for i, column in enumerate(columns):
+                if i > 0:
+                    query += ", "
+                query += "{}"
+
+                params.append(sql.Identifier(column))
+            query += ")"
+
+        query += " FROM STDIN WITH CSV"
+
+        query += ' NULL {}'
+        params.append(sql.Literal(nullchar))
+
+        query += ' DELIMITER {}'
+        params.append(sql.Literal(delimiter))
+
+        query += ' QUOTE {}'
+        params.append(sql.Literal(quotechar))
+        
+        query += ' ESCAPE {}'
+        params.append(sql.Literal(quotechar))
+
+        if not noheader:
+            query += " HEADER"
+
+        with self.cursor() as cursor:
+            cursor.copy_expert(sql.SQL(query).format(*params), fp)
+
+
+    def _fix_cursor_column_definition(self, column: Column):
+        # `python_type` is actually a type code.
+        # We will determine the actual python_type from the type_code.
+        # see: https://stackoverflow.com/a/53327695
+
+        type_code: Any = column.python_type
+
+        column.python_type = None
+        column.original_type = None
+
+        try:
+            string_type = string_types[type_code]
+            column.original_type = string_type.name
+
+            if 'BOOL' in column.original_type:
+                value = 'true'
+                column.python_type = type(string_type(value, cursor))
+            elif 'DATE' in column.original_type:
+                value = '2000-01-01'
+                column.python_type = type(string_type(value, cursor))
+            elif 'ARRAY' in column.original_type:
+                value = '{}'
+                column.python_type = type(string_type(value, cursor))
+            elif 'JSON' in column.original_type:
+                column.python_type = None
+            else:
+                value = '100'
+                column.python_type = type(string_type(value, cursor))
+
+        except DataError as err:
+            logger.error(f"cannot determine Python type of column \"{column.name}\" ({column.original_type or f'type_code={type_code}'}): {err}")
+        
+
+class PgLogHandler:
+    """
+    Usage example:
+    ```
+    from django.apps import AppConfig
+    from django.db.backends.signals import connection_created
+    from zut.db.pg import PgLogHandler
+
+    def connection_created_receiver(sender, connection, **kwargs):
+        if connection.alias == "default":
+            connection.connection.notices = PgLogHandler(f"pg:{connection.alias}")
+
+    class MainConfig(AppConfig):
+        default_auto_field = "django.db.models.BigAutoField"
+        name = "main"
+        
+        def ready(self):
+            connection_created.connect(connection_created_receiver)
+    ```
+    """
+    _pg_msg_re = re.compile(r"^(?P<pglevel>[A-Z]+)\:\s(?P<message>.+(?:\r?\n.*)*)$", re.MULTILINE)
+
+    def __init__(self, logger_name: str = 'pg'):
+        self._logger = logging.getLogger(logger_name)
+    
+    def append(self, fullmsg: str):
+        fullmsg = fullmsg.strip()
+        m = self._pg_msg_re.match(fullmsg)
+        if not m:
+            self._logger.warning(fullmsg)
+            return
+
+        message = m.group("message").strip()
+        pglevel = m.group("pglevel")
+        if pglevel == "EXCEPTION":
+            level = logging.ERROR
+        elif pglevel == "WARNING":
+            level = logging.WARNING
+        else:
+            level = logging.INFO
+
+        if level <= logging.INFO and message.endswith("\" does not exist, skipping"):
+            return
+
+        self._logger.log(level, message)
```

## zut/db/sql_pg/010_extensions.sql

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-create extension if not exists unaccent;
+create extension if not exists unaccent;
```

## zut/db/sql_pg/020_slugify.sql

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-create or replace function slugify(input text)
-returns text as $$
-    with "unaccented" as (
-		select unaccent(input) as value
-    ),
-    "clarified" as (
-		select regexp_replace(lower(value), '[^\w\s-]', '', 'g') as value
-	      from "unaccented"
-    ),
-    "hyphenated" as (
-		select trim(both '-_' from regexp_replace(value, '[-\s]+', '-', 'g')) as value
-		from "clarified"
-    )
-    select coalesce(value, 'none') from "hyphenated";
-$$ language sql immutable;
+create or replace function slugify(input text)
+returns text as $$
+    with "unaccented" as (
+		select unaccent(input) as value
+    ),
+    "clarified" as (
+		select regexp_replace(lower(value), '[^\w\s-]', '', 'g') as value
+	      from "unaccented"
+    ),
+    "hyphenated" as (
+		select trim(both '-_' from regexp_replace(value, '[-\s]+', '-', 'g')) as value
+		from "clarified"
+    )
+    select coalesce(value, 'none') from "hyphenated";
+$$ language sql immutable;
```

## zut/django/management/commands/reinit.py

```diff
@@ -1,186 +1,221 @@
-from __future__ import annotations
-import logging, re
-from pathlib import Path
-from psycopg2.sql import SQL, Literal
-from django.conf import settings
-from django.db import connection
-from django.core.management import base, call_command, get_commands
-from zut.env import get_venv
-from zut.db import get_backend, Backend, BackendNotSupported
-
-logger = logging.getLogger(__name__)
-
-_migration_name_re = re.compile(r"^(\d+)_")
-
-class Command(base.BaseCommand):
-    def add_arguments(self, parser):
-        parser.add_argument("--drop", dest="action", action="store_const", const="drop")
-        parser.add_argument("--bak", dest="action", action="store_const", const="bak")
-        parser.add_argument("--bak-to", dest="action")
-        parser.add_argument("remake_migrations_for_apps", nargs="*", help="apps for which migrations are remade")
-
-    def handle(self, action: str = None, remake_migrations_for_apps: list[str] = [], **kwargs):
-        if not settings.DEBUG:
-            raise ValueError("reinit may be used only in DEBUG mode")
-        if not action:
-            raise ValueError("please confirm what to do with current data: --drop, --bak or --bak-to")
-
-        backend = get_backend(connection=connection)
-        if backend != Backend.POSTGRESQL:
-            raise BackendNotSupported(backend)
-
-        self.REMAKE_MIGRATIONS_AFTER: dict[str,int] = getattr(settings, "REMAKE_MIGRATIONS_AFTER", {})
-        self.BASE_DIR: Path = settings.BASE_DIR
-        self.remake_migrations_for_apps = remake_migrations_for_apps
-
-        if action == "drop":
-            self.drop()
-        else:
-            self.move_to_schema(action)
-
-        self.delete_nonmanual_migrations()
-        self.rename_manual_migrations()
-
-        logger.info("make migrations")
-        call_command("makemigrations")
-
-        self.restore_renamed_migrations()
-        
-        logger.info("migrate")
-        call_command("migrate")
-
-        logger.info("createsuperuser")
-        call_command("createsuperuser", "--noinput")
-
-        defined_commands = get_commands()
-
-        if "seed" in defined_commands:
-            logger.info("seed")
-            call_command("seed")
-
-
-    def move_to_schema(self, new_schema, old_schema="public"):
-        sql = """do language plpgsql
-    $$declare
-        old_schema name = {};
-        new_schema name = {};
-        sql_query text;
-    begin
-        sql_query = format('create schema %I', new_schema);
-
-        raise notice 'applying %', sql_query;
-        execute sql_query;
-    
-        for sql_query in
-            select
-                format('alter %s %I.%I set schema %I', case when table_type = 'VIEW' then 'view' else 'table' end, table_schema, table_name, new_schema)
-            from information_schema.tables
-            where table_schema = old_schema
-            and table_name not in ('geography_columns', 'geometry_columns', 'spatial_ref_sys') -- postgis
-        loop
-            raise notice 'applying %', sql_query;
-            execute sql_query;
-        end loop;
-    end;$$;
-    """
-
-        with connection.cursor() as cursor:
-            cursor.execute(SQL(sql).format(Literal(old_schema), Literal(new_schema if new_schema else "public")))
-
-
-    def drop(self, schema="public"):
-        sql = """do language plpgsql
-    $$declare
-        old_schema name = {};
-        sql_query text;
-    begin
-        -- First, remove foreign-key constraints
-        for sql_query in
-            select
-                format('alter table %I.%I drop constraint %I', table_schema, table_name, constraint_name)
-            from information_schema.table_constraints
-            where table_schema = old_schema and constraint_type = 'FOREIGN KEY'
-            and table_name not in ('geography_columns', 'geometry_columns', 'spatial_ref_sys') -- postgis
-        loop
-            raise notice 'applying %', sql_query;
-            execute sql_query;
-        end loop;
-
-        -- Then, drop tables
-        for sql_query in
-            select
-                format('drop %s if exists %I.%I cascade'
-                    ,case when table_type = 'VIEW' then 'view' else 'table' end
-                    ,table_schema
-                    ,table_name
-                )
-            from information_schema.tables
-            where table_schema = old_schema
-            and table_name not in ('geography_columns', 'geometry_columns', 'spatial_ref_sys') -- postgis
-        loop
-            raise notice 'applying %', sql_query;
-            execute sql_query;
-        end loop;
-    end;$$;
-    """
-
-        with connection.cursor() as cursor:
-            cursor.execute(SQL(sql).format(Literal(schema)))
-
-    def should_remake_migration(self, path: Path, remake_manuals=False):
-        if not remake_manuals and path.name.endswith("_manual.py"):
-            return False
-
-        if get_venv() in path.parents:
-            # exclude migrations located in ".venv" directory
-            return False
-
-        app_name = path.parent.parent.name
-        if self.remake_migrations_for_apps and not app_name in self.remake_migrations_for_apps:
-            return False
-
-        if self.REMAKE_MIGRATIONS_AFTER and app_name in self.REMAKE_MIGRATIONS_AFTER:
-            m = _migration_name_re.match(path.name)
-            if not m:
-                return False
-
-            migration_number = int(m.group(1))
-            if migration_number <= self.REMAKE_MIGRATIONS_AFTER[app_name]:
-                return False
-        
-            else:
-                return True
-
-        elif self.remake_migrations_for_apps and app_name in self.remake_migrations_for_apps:
-            return True
-
-        else:
-            return False
-
-    def delete_nonmanual_migrations(self):
-        """ Delete non-manual migrations """
-        for path in self.BASE_DIR.glob("*/migrations/0*.py"):
-            if self.should_remake_migration(path):
-                logger.info(f"delete {path}")
-                path.unlink()
-            else:
-                logger.info(f"preserve {path}")
-
-    def rename_manual_migrations(self):
-        self.renamed: dict[Path,Path] = {}
-
-        """ Rename manual migrations to py~ """
-        for path in self.BASE_DIR.glob("*/migrations/*_manual.py"):
-            if self.should_remake_migration(path, remake_manuals=True):
-                target = path.with_name(f"{path.name}~")
-                logger.info(f"rename {path} to {target}")
-                path.rename(target)
-                self.renamed[path] = target
-            else:
-                logger.info(f"preserve {path}")
-    
-    def restore_renamed_migrations(self):
-        """ Restore migrations from py~ """
-        for origin, renamed in self.renamed.items():
-            logger.info(f"rename {renamed} to {origin}")
-            renamed.rename(origin)
+from __future__ import annotations
+import logging, re
+from pathlib import Path
+from psycopg2 import sql
+from django.conf import settings
+from django.db import connection
+from django.core.management import base, call_command, get_commands
+from zut.misc import is_in_venv
+from zut.db import PgWrapper
+
+logger = logging.getLogger(__name__)
+
+
+class Command(base.BaseCommand):
+    REINIT_SEALED_MIGRATIONS: dict[str,int] = getattr(settings, "REINIT_SEALED_MIGRATIONS", {})
+    REINIT_APPS_ORDER: list[str] = getattr(settings, "REINIT_APPS_ORDER", [])
+    REINIT_POST_COMMANDS: list[str|list[str]] = getattr(settings, "REINIT_POST_COMMANDS", [])
+    BASE_DIR: Path = settings.BASE_DIR
+    _migration_name_re = re.compile(r"^(\d+)_")
+
+
+    def add_arguments(self, parser):
+        parser.add_argument("-d", "--drop", dest="schema", action="store_const", const="drop", help="drop existing objects and data")
+        parser.add_argument("-b", "--bak", dest="schema", action="store_const", const="bak", help="move existing objects and data to schema \"bak\"")
+        parser.add_argument("-t", "--bak-to", dest="schema", help="move existing objects and data to the given schema")
+        parser.add_argument("apps", nargs="*", help="apps for which migrations are remade")
+
+
+    def handle(self, schema: str = None, apps: list[str] = [], **kwargs):
+        if not settings.DEBUG:
+            raise ValueError("reinit may be used only in DEBUG mode")
+        if not schema:
+            raise ValueError("please confirm what to do with current data: --drop, --bak or --bak-to")
+        
+        if not settings.DATABASES['default']['ENGINE'] in PgWrapper.compatible_django_engines:
+            raise ValueError(f"not a postgresql django engine: {settings.DATABASES['ENGINE']}")
+
+        self.apps = apps
+
+        if schema == "drop":
+            self.drop()
+        else:
+            self.move_to_schema(schema)
+
+        self.delete_nonmanual_migrations()
+        renamed = self.rename_manual_migrations()
+
+        logger.info("make migrations")
+        for app in self.REINIT_APPS_ORDER:
+            call_command("makemigrations", app)
+        call_command("makemigrations")
+
+        self.restore_renamed_migrations(renamed)
+        
+        logger.info("migrate")
+        call_command("migrate")
+
+        for post_command in self.REINIT_POST_COMMANDS:
+            if not isinstance(post_command, list):
+                post_command = [post_command]
+            logger.info(' '.join(post_command))
+            call_command(*post_command)
+
+
+    def move_to_schema(self, new_schema, old_schema="public"):
+        query = """do language plpgsql
+    $$declare
+        old_schema name = {};
+        new_schema name = {};
+        sql_query text;
+    begin
+        sql_query = format('create schema %I', new_schema);
+
+        raise notice 'applying %', sql_query;
+        execute sql_query;
+    
+        for sql_query in
+            select
+                format('alter %s %I.%I set schema %I', case when table_type = 'VIEW' then 'view' else 'table' end, table_schema, table_name, new_schema)
+            from information_schema.tables
+            where table_schema = old_schema
+            and table_name not in ('geography_columns', 'geometry_columns', 'spatial_ref_sys') -- postgis
+        loop
+            raise notice 'applying %', sql_query;
+            execute sql_query;
+        end loop;
+    end;$$;
+    """
+
+        with connection.cursor() as cursor:
+            cursor.execute(sql.SQL(query).format(sql.Literal(old_schema), sql.Literal(new_schema if new_schema else "public")))
+
+
+    def drop(self, schema="public"):
+        query = """do language plpgsql
+    $$declare
+        old_schema name = {};
+        sql_query text;
+    begin
+        -- First, remove foreign-key constraints
+        for sql_query in
+            select
+                format('alter table %I.%I drop constraint %I', table_schema, table_name, constraint_name)
+            from information_schema.table_constraints
+            where table_schema = old_schema and constraint_type = 'FOREIGN KEY'
+            and table_name not in ('geography_columns', 'geometry_columns', 'spatial_ref_sys') -- postgis
+        loop
+            raise notice 'applying %', sql_query;
+            execute sql_query;
+        end loop;
+
+        -- Then, drop tables
+        for sql_query in
+            select
+                format('drop %s if exists %I.%I cascade'
+                    ,case when table_type = 'VIEW' then 'view' else 'table' end
+                    ,table_schema
+                    ,table_name
+                )
+            from information_schema.tables
+            where table_schema = old_schema
+            and table_name not in ('geography_columns', 'geometry_columns', 'spatial_ref_sys') -- postgis
+        loop
+            raise notice 'applying %', sql_query;
+            execute sql_query;
+        end loop;
+    end;$$;
+    """
+
+        with connection.cursor() as cursor:
+            cursor.execute(sql.SQL(query).format(sql.Literal(schema)))
+
+
+    def should_remake_migration(self, path: Path, remake_manuals=False):
+        if not remake_manuals and path.name.endswith("_manual.py"):
+            return False
+
+        if is_in_venv(path):
+            # exclude migrations located in venv
+            logger.debug(f"preserve (venv): {path}")
+            return False
+
+        for part in path.parts:
+            if part.startswith('.'):
+                logger.debug(f"preserve (hidden): {path}")
+                return False
+
+            if re.match(r'^\d+\-', part):
+                logger.debug(f"preserve (digit+hyphen): {path}")
+                return False
+
+        app_name = path.parent.parent.name
+        if self.apps:           
+            if app_name in self.apps:
+                if self.is_sealed_migration(app_name, path):
+                    return False
+            else:
+                logger.info(f"preserve (app {app_name}): {path}")
+                return False
+
+        else:
+            if self.is_sealed_migration(app_name, path):
+                return False
+
+        return True
+
+
+    def get_migration_number(self, path: Path):
+        m = self._migration_name_re.match(path.name)
+        if not m:
+            return None
+
+        return int(m.group(1))
+
+
+    def is_sealed_migration(self, app_name: str, path: Path):
+        if not self.REINIT_SEALED_MIGRATIONS:
+            return False
+        if not app_name in self.REINIT_SEALED_MIGRATIONS:
+            return False
+        
+        migration_number = self.get_migration_number(path)
+        if migration_number is None:
+            logger.warning(f"preserve (no migration number): {path}")
+            return True
+        elif migration_number <= self.REINIT_SEALED_MIGRATIONS[app_name]:
+            logger.info(f"preserve (sealed: app {app_name} migration <= {self.REINIT_SEALED_MIGRATIONS[app_name]}): {path}")
+            return True
+        else:
+            return False
+    
+
+    def delete_nonmanual_migrations(self):
+        """ Delete non-manual migrations """
+        for path in self.BASE_DIR.glob("*/migrations/0*.py"):
+            if self.should_remake_migration(path):
+                logger.info(f"delete {path}")
+                path.unlink()
+
+
+    def rename_manual_migrations(self) -> dict[Path,Path]:
+        """ Rename manual migrations to py~ """
+        renamed: dict[Path,Path] = {}
+
+        for path in self.BASE_DIR.glob("*/migrations/*_manual.py"):
+            if self.should_remake_migration(path, remake_manuals=True):
+                target = path.with_name(f"{path.name}~")
+                logger.info(f"rename {path} to {target}")
+                path.rename(target)
+                renamed[path] = target
+            else:
+                logger.info(f"preserve {path}")
+
+        return renamed
+
+    
+    def restore_renamed_migrations(self, renamed: dict[Path,Path]):
+        """ Restore migrations from py~ """
+        for origin, newpath in renamed.items():
+            logger.info(f"rename {newpath} to {origin}")
+            newpath.rename(origin)
```

## zut/django/middleware/__init__.py

 * *Ordering differences only*

```diff
@@ -1,80 +1,80 @@
-import logging
-from django.http import HttpResponseForbidden
-from django.core.exceptions import ImproperlyConfigured
-
-try:
-    from django.contrib.auth.mixins import AccessMixin
-    from django.contrib.auth.views import LoginView, LogoutView, redirect_to_login
-    from django.views.generic.base import RedirectView
-    improperly_configured_error = None
-        
-    try:
-        from rest_framework.views import APIView
-        _with_rest_framework = True
-    except ImportError:
-        _with_rest_framework = False
-except ImproperlyConfigured as improperly_configured_error:
-    # delay exception in class instantiation to avoid failure during unittest discovery
-    pass
-
-
-logger = logging.getLogger(__name__)
-
-class StaffAuthorizationMiddleware:
-    no_default_for = ["/", "/status"]
-
-    def __init__(self, get_response):
-        if improperly_configured_error:
-            raise improperly_configured_error
-        self.get_response = get_response
-
-    def __call__(self, request):
-        return self.get_response(request)
-
-    def process_view(self, request, view_func, view_args, view_kwargs):
-        if request.path in self.no_default_for:
-            return # no default permission
-
-        if view_func.__module__.startswith("django.contrib.admin."):
-            return # no default permission
-
-        if not hasattr(view_func, "view_class"):
-            # function-based view
-            if not self.is_authorized(request.user):
-                logger.debug("function-based view %s: default authorization required", ".".join([view_func.__module__, view_func.__name__]))
-                return self._deny_or_login(request)
-            
-        else:
-            # class-based view
-            view = view_func.view_class
-
-            if issubclass(view, (LoginView, LogoutView, RedirectView)):
-                return # no default permission
-
-            if _with_rest_framework and issubclass(view, APIView):
-                # API view (Django Rest Framework)
-                if not view.permission_classes:
-                    if not self.is_authorized(request.user):
-                        logger.debug("no permission_classes for rest_framework view %s: default authorization required required", ".".join([view.__module__, view.__name__]))
-                        return HttpResponseForbidden() # do not redirect to login page (this is supposed to be accessed by javascript or as an API)
-
-            else:
-                # Standard class-based view
-                if not issubclass(view, AccessMixin):
-                    if not self.is_authorized(request.user):
-                        logger.debug("no AccessMixin for view %s: default authorization required", ".".join([view.__module__, view.__name__]))
-                        return self._deny_or_login(request)
-
-    def is_authorized(self, user):
-        return user.is_staff
-
-    def _deny_or_login(self, request):
-        if request.user.is_authenticated:
-            return HttpResponseForbidden()
-        else:
-            return redirect_to_login(next=request.get_full_path())
-
-
-class AdminAuthorizationMiddleware(StaffAuthorizationMiddleware):
-    def is_authorized(self, user):
-        return user.is_admin
+import logging
+from django.http import HttpResponseForbidden
+from django.core.exceptions import ImproperlyConfigured
+
+try:
+    from django.contrib.auth.mixins import AccessMixin
+    from django.contrib.auth.views import LoginView, LogoutView, redirect_to_login
+    from django.views.generic.base import RedirectView
+    improperly_configured_error = None
+        
+    try:
+        from rest_framework.views import APIView
+        _with_rest_framework = True
+    except ImportError:
+        _with_rest_framework = False
+except ImproperlyConfigured as improperly_configured_error:
+    # delay exception in class instantiation to avoid failure during unittest discovery
+    pass
+
+
+logger = logging.getLogger(__name__)
+
+class StaffAuthorizationMiddleware:
+    no_default_for = ["/", "/status"]
+
+    def __init__(self, get_response):
+        if improperly_configured_error:
+            raise improperly_configured_error
+        self.get_response = get_response
+
+    def __call__(self, request):
+        return self.get_response(request)
+
+    def process_view(self, request, view_func, view_args, view_kwargs):
+        if request.path in self.no_default_for:
+            return # no default permission
+
+        if view_func.__module__.startswith("django.contrib.admin."):
+            return # no default permission
+
+        if not hasattr(view_func, "view_class"):
+            # function-based view
+            if not self.is_authorized(request.user):
+                logger.debug("function-based view %s: default authorization required", ".".join([view_func.__module__, view_func.__name__]))
+                return self._deny_or_login(request)
+            
+        else:
+            # class-based view
+            view = view_func.view_class
+
+            if issubclass(view, (LoginView, LogoutView, RedirectView)):
+                return # no default permission
+
+            if _with_rest_framework and issubclass(view, APIView):
+                # API view (Django Rest Framework)
+                if not view.permission_classes:
+                    if not self.is_authorized(request.user):
+                        logger.debug("no permission_classes for rest_framework view %s: default authorization required required", ".".join([view.__module__, view.__name__]))
+                        return HttpResponseForbidden() # do not redirect to login page (this is supposed to be accessed by javascript or as an API)
+
+            else:
+                # Standard class-based view
+                if not issubclass(view, AccessMixin):
+                    if not self.is_authorized(request.user):
+                        logger.debug("no AccessMixin for view %s: default authorization required", ".".join([view.__module__, view.__name__]))
+                        return self._deny_or_login(request)
+
+    def is_authorized(self, user):
+        return user.is_staff
+
+    def _deny_or_login(self, request):
+        if request.user.is_authenticated:
+            return HttpResponseForbidden()
+        else:
+            return redirect_to_login(next=request.get_full_path())
+
+
+class AdminAuthorizationMiddleware(StaffAuthorizationMiddleware):
+    def is_authorized(self, user):
+        return user.is_admin
```

## zut/django/templatetags/__init__.py

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-#TODO/ROADMAP: additional tags and filters: timeordate, hostlink, etc
+#TODO/ROADMAP: additional tags and filters: timeordate, hostlink, etc
```

## zut/django/templatetags/static_lib.py

 * *Ordering differences only*

```diff
@@ -1,90 +1,90 @@
-import logging
-from django import template
-from django.utils.safestring import mark_safe
-from django.templatetags.static import static
-from django.conf import settings
-
-logger = logging.getLogger(__name__)
-
-register = template.Library()
-
-# avoid logging warnings for every request
-_missing_version: list[str] = []
-_missing_integrity: list[str] = []
-
-def _get_lib_url(package, file, version=None):
-    LOCAL_STATIC_LIB = getattr(settings, "LOCAL_STATIC_LIB", False)
-    if LOCAL_STATIC_LIB:
-        return f"{static(LOCAL_STATIC_LIB if isinstance(LOCAL_STATIC_LIB, str) else 'lib')}/{package}/{file}"
-    else:
-        if version:
-            return f"https://cdn.jsdelivr.net/npm/{package}@{version}/{file}"
-        else:
-            return f"https://cdn.jsdelivr.net/npm/{package}/{file}"
-
-
-@register.simple_tag
-def style_lib(package, file, version=None, integrity=None):
-    """
-    Usage example in Django base template:
-
-        {% load static %}
-        {% load static_lib %}
-        ...
-        <head>
-        ...
-        {% style_lib  'bootstrap' 'dist/css/bootstrap.min.css' '5.2.0' 'sha256-7ZWbZUAi97rkirk4DcEp4GWDPkWpRMcNaEyXGsNXjLg=' %}
-        ...
-        </head>
-    """
-    url = _get_lib_url(package, file, version)
-
-    if not version and not url in _missing_version:
-        logger.warning(f"missing version for style_lib: {url}")
-        _missing_version.append(url)
-        
-    html = f"<link rel=\"stylesheet\" href=\"{url}\""
-    
-    if integrity:
-        html += f" integrity=\"{integrity}\" crossorigin=\"anonymous\""
-    else:
-        logger.warning(f"missing integrity hash for style_lib: {url}")
-        _missing_integrity.append(url)
-
-    html += f" />"
-    return mark_safe(html)
-
-
-@register.simple_tag
-def script_lib(package, file, version=None, integrity=None, defer=False):
-    """
-    Usage example in Django base template:
-
-        {% load static %}
-        {% load static_lib %}
-        ...
-        <head>
-        ...
-        {% script_lib 'bootstrap' 'dist/js/bootstrap.bundle.min.js' '5.2.0' 'sha256-wMCQIK229gKxbUg3QWa544ypI4OoFlC2qQl8Q8xD8x8=' %}
-        ...
-        </head>
-    """
-    url = _get_lib_url(package, file, version)
-    
-    if not version and not url in _missing_version:
-        logger.warning(f"missing version for script_lib: {url}")
-        _missing_version.append(url)
-
-    html = f"<script"
-    if defer:
-        html=" defer"
-    html += f" src=\"{url}\""
-    
-    if integrity:
-        html += f" integrity=\"{integrity}\" crossorigin=\"anonymous\""
-    elif not url in _missing_integrity:
-        logger.warning(f"missing integrity hash for script_lib: {url}")
-        _missing_integrity.append(url)
-
-    html += f"></script>"
-    return mark_safe(html)
+import logging
+from django import template
+from django.utils.safestring import mark_safe
+from django.templatetags.static import static
+from django.conf import settings
+
+logger = logging.getLogger(__name__)
+
+register = template.Library()
+
+# avoid logging warnings for every request
+_missing_version: list[str] = []
+_missing_integrity: list[str] = []
+
+def _get_lib_url(package, file, version=None):
+    LOCAL_STATIC_LIB = getattr(settings, "LOCAL_STATIC_LIB", False)
+    if LOCAL_STATIC_LIB:
+        return f"{static(LOCAL_STATIC_LIB if isinstance(LOCAL_STATIC_LIB, str) else 'lib')}/{package}/{file}"
+    else:
+        if version:
+            return f"https://cdn.jsdelivr.net/npm/{package}@{version}/{file}"
+        else:
+            return f"https://cdn.jsdelivr.net/npm/{package}/{file}"
+
+
+@register.simple_tag
+def style_lib(package, file, version=None, integrity=None):
+    """
+    Usage example in Django base template:
+
+        {% load static %}
+        {% load static_lib %}
+        ...
+        <head>
+        ...
+        {% style_lib  'bootstrap' 'dist/css/bootstrap.min.css' '5.2.0' 'sha256-7ZWbZUAi97rkirk4DcEp4GWDPkWpRMcNaEyXGsNXjLg=' %}
+        ...
+        </head>
+    """
+    url = _get_lib_url(package, file, version)
+
+    if not version and not url in _missing_version:
+        logger.warning(f"missing version for style_lib: {url}")
+        _missing_version.append(url)
+        
+    html = f"<link rel=\"stylesheet\" href=\"{url}\""
+    
+    if integrity:
+        html += f" integrity=\"{integrity}\" crossorigin=\"anonymous\""
+    else:
+        logger.warning(f"missing integrity hash for style_lib: {url}")
+        _missing_integrity.append(url)
+
+    html += f" />"
+    return mark_safe(html)
+
+
+@register.simple_tag
+def script_lib(package, file, version=None, integrity=None, defer=False):
+    """
+    Usage example in Django base template:
+
+        {% load static %}
+        {% load static_lib %}
+        ...
+        <head>
+        ...
+        {% script_lib 'bootstrap' 'dist/js/bootstrap.bundle.min.js' '5.2.0' 'sha256-wMCQIK229gKxbUg3QWa544ypI4OoFlC2qQl8Q8xD8x8=' %}
+        ...
+        </head>
+    """
+    url = _get_lib_url(package, file, version)
+    
+    if not version and not url in _missing_version:
+        logger.warning(f"missing version for script_lib: {url}")
+        _missing_version.append(url)
+
+    html = f"<script"
+    if defer:
+        html=" defer"
+    html += f" src=\"{url}\""
+    
+    if integrity:
+        html += f" integrity=\"{integrity}\" crossorigin=\"anonymous\""
+    elif not url in _missing_integrity:
+        logger.warning(f"missing integrity hash for script_lib: {url}")
+        _missing_integrity.append(url)
+
+    html += f"></script>"
+    return mark_safe(html)
```

## zut/django/views/MarkdownTemplateView.py

 * *Ordering differences only*

```diff
@@ -1,43 +1,43 @@
-from __future__ import annotations
-from cmarkgfm import github_flavored_markdown_to_html
-from django.views.generic import TemplateView
-from django.template.loader import get_template
-from html.parser import HTMLParser
-
-class MarkdownTemplateView(TemplateView):
-    extends = "base.html"
-
-    def render_to_response(self, context, **response_kwargs):
-        markdown_template = get_template(self.template_name)
-        markdown_text = markdown_template.render(context, self.request)
-        html_content = github_flavored_markdown_to_html(markdown_text)
-
-        new_context = {}
-        new_context["extends"] = self.extends
-        new_context["title"] = MarkdownTemplateView.find_html_tag_content(html_content, "h1")
-        new_context["html_content"] = html_content
-        return super().render_to_response(new_context, **response_kwargs)
-
-    def get_template_names(self):
-        return ["_content.html"] # located in main/templates
-
-    @staticmethod
-    def find_html_tag_content(html: str, tag: str):        
-        class HTMLTagParser(HTMLParser):
-            def __init__(self, searched_tag):
-                super().__init__()
-                self.searched_tag = searched_tag
-                self.match = False
-                self.data = None
-
-            def handle_starttag(self, tag, attributes):
-                self.match = tag == self.searched_tag
-
-            def handle_data(self, data):
-                if self.match:
-                    self.data = data
-                    self.match = False
-
-        parser = HTMLTagParser(tag)
-        parser.feed(html)
-        return parser.data
+from __future__ import annotations
+from cmarkgfm import github_flavored_markdown_to_html
+from django.views.generic import TemplateView
+from django.template.loader import get_template
+from html.parser import HTMLParser
+
+class MarkdownTemplateView(TemplateView):
+    extends = "base.html"
+
+    def render_to_response(self, context, **response_kwargs):
+        markdown_template = get_template(self.template_name)
+        markdown_text = markdown_template.render(context, self.request)
+        html_content = github_flavored_markdown_to_html(markdown_text)
+
+        new_context = {}
+        new_context["extends"] = self.extends
+        new_context["title"] = MarkdownTemplateView.find_html_tag_content(html_content, "h1")
+        new_context["html_content"] = html_content
+        return super().render_to_response(new_context, **response_kwargs)
+
+    def get_template_names(self):
+        return ["_content.html"] # located in main/templates
+
+    @staticmethod
+    def find_html_tag_content(html: str, tag: str):        
+        class HTMLTagParser(HTMLParser):
+            def __init__(self, searched_tag):
+                super().__init__()
+                self.searched_tag = searched_tag
+                self.match = False
+                self.data = None
+
+            def handle_starttag(self, tag, attributes):
+                self.match = tag == self.searched_tag
+
+            def handle_data(self, data):
+                if self.match:
+                    self.data = data
+                    self.match = False
+
+        parser = HTMLTagParser(tag)
+        parser.feed(html)
+        return parser.data
```

## zut/django/views/__init__.py

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-from .MarkdownTemplateView import MarkdownTemplateView
+from .MarkdownTemplateView import MarkdownTemplateView
```

## zut/inout/InCsv.py

 * *Ordering differences only*

```diff
@@ -1,57 +1,57 @@
-from __future__ import annotations
-
-import csv
-from io import IOBase
-from pathlib import Path
-from typing import Any
-
-from ..csv import get_default_csv_delimiter
-from ..text import reconfigure_encoding
-from .. import filesh
-from .InTable import InTable
-
-class InCsv(InTable):
-    def __init__(self, src: Path|str|IOBase, *, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"', **kwargs):     
-        super().__init__(src, **kwargs)
-
-        self._encoding = encoding
-
-        self._delimiter = delimiter
-        if self._delimiter is None:
-            self._delimiter = get_default_csv_delimiter()
-
-        self._quotechar = quotechar
-        
-        # in _prepare:
-        self._file: IOBase = None
-        self._must_close_file: bool = None
-
-
-    def _prepare(self):
-        if isinstance(self._src, IOBase):
-            self._file = self._src
-            self._must_close_file = False
-        
-        else:
-            self._file = filesh.open_file(self._src, 'r', newline='', encoding=self._encoding)
-            self._must_close_file = True
-
-        self._encoding = reconfigure_encoding(self._file, self._encoding)        
-        self._csv_reader = csv.reader(self._file, delimiter=self._delimiter, quotechar=self._quotechar)
-        
-        self.headers = []        
-        try:
-            for header in next(self._csv_reader):
-                self.headers.append(str(header))
-        except StopIteration:
-            self._file.close()
-            raise ValueError(f"no headers found in {self._name}")
-
-
-    def _get_next_values(self) -> list[Any]:
-        return next(self._csv_reader)
-
-
-    def _end(self):
-        if self._file and self._must_close_file:
-            self._file.close()
+from __future__ import annotations
+
+import csv
+from io import IOBase
+from pathlib import Path
+from typing import Any
+
+from ..csv import get_default_csv_delimiter
+from ..text import reconfigure_encoding
+from .. import filesh
+from .InTable import InTable
+
+class InCsv(InTable):
+    def __init__(self, src: Path|str|IOBase, *, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"', **kwargs):     
+        super().__init__(src, **kwargs)
+
+        self._encoding = encoding
+
+        self._delimiter = delimiter
+        if self._delimiter is None:
+            self._delimiter = get_default_csv_delimiter()
+
+        self._quotechar = quotechar
+        
+        # in _prepare:
+        self._file: IOBase = None
+        self._must_close_file: bool = None
+
+
+    def _prepare(self):
+        if isinstance(self._src, IOBase):
+            self._file = self._src
+            self._must_close_file = False
+        
+        else:
+            self._file = filesh.open_file(self._src, 'r', newline='', encoding=self._encoding)
+            self._must_close_file = True
+
+        self._encoding = reconfigure_encoding(self._file, self._encoding)        
+        self._csv_reader = csv.reader(self._file, delimiter=self._delimiter, quotechar=self._quotechar)
+        
+        self.headers = []        
+        try:
+            for header in next(self._csv_reader):
+                self.headers.append(str(header))
+        except StopIteration:
+            self._file.close()
+            raise ValueError(f"no headers found in {self._name}")
+
+
+    def _get_next_values(self) -> list[Any]:
+        return next(self._csv_reader)
+
+
+    def _end(self):
+        if self._file and self._must_close_file:
+            self._file.close()
```

## zut/inout/InDb.py

```diff
@@ -1,55 +1,64 @@
-from __future__ import annotations
-
-import logging
-from hashlib import sha1
-
-from ..db import get_db_with_schema_and_table
-from ..colors import Colors
-from .InTable import InTable
-
-logger = logging.getLogger(__name__)
-
-
-class InDb(InTable):
-    def __init__(self, src: str, query: str = None, params: list|tuple|dict = None, limit: int = None, **kwargs):
-        super().__init__(src, **kwargs)
-
-        self._db, schema, table = get_db_with_schema_and_table(self._src)
-
-        self._query = query
-        self._limit = limit
-        self._params = params
-        if self._query:
-            sha1_prefix = sha1(self._query.encode('utf-8')).hexdigest()[0:8]
-            self._name = f"{self._name}#{sha1_prefix}"
-        elif table:
-            if self._params:
-                raise ValueError(f'table query cannot contain params')
-            self._name = f"{self._name}#{schema}.{table}"
-            self._query = self._db.get_select_table_query((schema, table))
-        else:
-            raise ValueError(f'neither a table or a query was given')
-
-        if self._debug:
-            logger.debug(f"execute {self._name} with params {self._params}, limit={self._limit}\n{Colors.CYAN}{self._query}{Colors.RESET}")
-
-        # set in __enter__() -> _prepare():
-        self._cursor = None
-
-
-    def _prepare(self):
-        self._db.__enter__()
-
-        self._cursor = self._db.execute_get_cursor(self._query, self._params, limit=self._limit)
-
-        self.headers = self._db.get_cursor_column_names(self._cursor)
-
-
-    def _get_next_values(self):
-        cursor_row = next(self._cursor)
-        return list(cursor_row)
-
-
-    def _end(self):
-        self._cursor.close()
-        self._db.__exit__()
+from __future__ import annotations
+
+import logging
+from hashlib import sha1
+
+from ..db import create_db_wrapper_with_schema_and_table, DbWrapper
+from ..colors import Colors
+from .InTable import InTable
+
+logger = logging.getLogger(__name__)
+
+
+class InDb(InTable):
+    def __init__(self, src: str|DbWrapper, query: str = None, params: list|tuple|dict = None, limit: int = None, **kwargs):
+        if isinstance(src, DbWrapper):
+            self._db = src
+            schema = None
+            table = None
+            super().__init__(self._db.get_uri(with_password=False), **kwargs)
+            self._must_exit_db = False
+        else:
+            super().__init__(src, **kwargs)
+            self._db, schema, table = create_db_wrapper_with_schema_and_table(self._src)
+            self._name = self._db.get_uri(table=(schema, table), with_password=False)
+            self._must_exit_db = True
+        
+
+        self._query = query
+        self._limit = limit
+        self._params = params
+        if self._query:
+            sha1_prefix = sha1(self._query.encode('utf-8')).hexdigest()[0:8]
+            self._name = f"{self._name}#{sha1_prefix}"
+        elif table:
+            if self._params:
+                raise ValueError(f'table query cannot contain params')
+            self._query = self._db.get_select_table_query((schema, table))
+        else:
+            raise ValueError(f'neither a table or a query was given')
+
+        if self._debug:
+            logger.debug(f"execute {self._name} with params {self._params}, limit={self._limit}\n{Colors.CYAN}{self._query}{Colors.RESET}")
+
+        # set in __enter__() -> _prepare():
+        self._cursor = None
+
+
+    def _prepare(self):
+        self._db.__enter__()
+
+        self._cursor = self._db.execute_get_cursor(self._query, self._params, limit=self._limit)
+
+        self.headers = self._db.get_cursor_column_names(self._cursor)
+
+
+    def _get_next_values(self):
+        cursor_row = next(self._cursor)
+        return list(cursor_row)
+
+
+    def _end(self):
+        self._cursor.close()
+        if self._must_exit_db:
+            self._db.__exit__()
```

## zut/inout/InExcel.py

 * *Ordering differences only*

```diff
@@ -1,48 +1,48 @@
-from __future__ import annotations
-import logging
-from pathlib import Path
-from .InTable import InTable
-from .OutExcel import OutExcel
-from .utils import get_inout_name, split_excel_path
-
-try:
-    from ..excel import ExcelWorkbook
-    _available = True
-except ImportError:
-    _available = False
-
-
-logger = logging.getLogger(__name__)
-
-
-class InExcel(InTable):
-    @classmethod
-    def is_available(cls):
-        return _available
-
-
-    def __init__(self, src, **kwargs):
-        # Prepare arguments for base classes (InTable)
-        if not isinstance(src, (str,Path)):
-            raise ValueError(f"InExcel's src must be a str or path, got {type(src).__name__}: {src}")
-        
-        src, self._table_name = split_excel_path(src)
-        if not self._table_name:
-            self._table_name = OutExcel._DEFAULT_TABLE_NAME
-        
-        # Initialize base classes (InTable)
-        super().__init__(src=src, **kwargs)
-        
-        # Modify attributes set by base classes (InTable)
-        self._name = get_inout_name(self._src) + f'#{self._table_name}'
-
-
-    def _prepare(self):
-        wb = ExcelWorkbook.get_or_create_cached(self._src)
-        self._table = wb.get_table(self._table_name)
-        self.headers = self._table.column_names if self._table.has_headers else None
-        self._iterator = iter(self._table)
-
-
-    def _get_next_values(self):
-        return next(self._iterator)
+from __future__ import annotations
+import logging
+from pathlib import Path
+from .InTable import InTable
+from .OutExcel import OutExcel
+from .utils import get_inout_name, split_excel_path
+
+try:
+    from ..excel import ExcelWorkbook
+    _available = True
+except ImportError:
+    _available = False
+
+
+logger = logging.getLogger(__name__)
+
+
+class InExcel(InTable):
+    @classmethod
+    def is_available(cls):
+        return _available
+
+
+    def __init__(self, src, **kwargs):
+        # Prepare arguments for base classes (InTable)
+        if not isinstance(src, (str,Path)):
+            raise ValueError(f"InExcel's src must be a str or path, got {type(src).__name__}: {src}")
+        
+        src, self._table_name = split_excel_path(src)
+        if not self._table_name:
+            self._table_name = OutExcel._DEFAULT_TABLE_NAME
+        
+        # Initialize base classes (InTable)
+        super().__init__(src=src, **kwargs)
+        
+        # Modify attributes set by base classes (InTable)
+        self._name = get_inout_name(self._src) + f'#{self._table_name}'
+
+
+    def _prepare(self):
+        wb = ExcelWorkbook.get_or_create_cached(self._src)
+        self._table = wb.get_table(self._table_name)
+        self.headers = self._table.column_names if self._table.has_headers else None
+        self._iterator = iter(self._table)
+
+
+    def _get_next_values(self):
+        return next(self._iterator)
```

## zut/inout/InTable.py

```diff
@@ -1,132 +1,132 @@
-from __future__ import annotations
-
-import logging
-from datetime import datetime, timezone
-from pathlib import Path
-from time import time_ns
-from typing import Any, Callable
-from ..misc import Literal
-from ..tabular import Row
-from ..datetime import is_aware, make_aware
-from .utils import normalize_inout, get_inout_name
-
-logger = logging.getLogger(__name__)
-
-
-class InTable:
-    @classmethod
-    def is_available(cls):
-        return True
-    
-    def __init__(self, src, *, src_dir: str|Path|None = None, title: str|None = None, debug: bool = None, naive_tz: Literal['local']|timezone = 'local', formatters: dict[int|str,Callable] = None, **kwargs):
-        if not self.is_available():
-            raise ValueError(f"cannot use {type(self).__name__} (not available)")
-
-        self._src = normalize_inout(src, directory=src_dir, title=title, **kwargs)
-        self._name = get_inout_name(self._src)
-
-        self._debug = debug
-        self._naive_tz = naive_tz
-        self._formatters = formatters
-
-        # set in _prepare():
-        self.headers: list[str] = None
-        self._prepare_duration: int = None
-
-        # set in __iter__():
-        self._extract_t0: int = None
-        self._extract_duration: int = None
-        self._row_count: int = None           
-
-
-    def __enter__(self):        
-        if self._debug:
-            logger.debug(f"prepare {self._name}")
-            t0 = time_ns()
-
-        self._prepare()
-
-        # headers are now set, update formatters to point on index
-        if self._formatters:
-            new_entries = {}
-            for key, formatter in self._formatters.items():
-                if not isinstance(key, str):
-                    continue
-                try:
-                    index = self.headers.index(key)
-                except ValueError:
-                    continue
-                new_entries[index] = formatter
-            
-            for index, formatter in new_entries.items():
-                self._formatters[index] = formatter
-
-
-        if self._debug:
-            self._prepare_duration = time_ns() - t0
-
-        return self
-
-
-    def __exit__(self, exc_type = None, exc_val = None, exc_tb = None):
-        self._end()
-
-        if self._debug:
-            if self._extract_duration is not None:
-                logger.debug(f"{self._row_count:,d} rows extracted from {self._name} (total duration: {(self._prepare_duration + self._extract_duration)/1e6:,.0f} ms, prepare: {self._prepare_duration/1e6:,.0f} ms, extract: {self._extract_duration/1e6:,.0f} ms)")
-            else:
-                logger.debug(f"prepared {self._name} (duration: {self._prepare_duration/1e6:,.0f} ms)")
-
-
-    def __iter__(self):
-        return self
-
-
-    def __next__(self):
-        if self._row_count is None:
-            if self._debug:
-                self._extract_t0 = time_ns()
-                self._extract_duration = 0
-            self._row_count = 0
-
-        values = self._get_next_values()
-
-        if self._debug:
-            self._extract_duration = time_ns() - self._extract_t0
-
-        self._row_count += 1
-
-        if self._naive_tz is not None or self._formatters:
-            if isinstance(values, Row):
-                values = list(values.values)
-            
-            for i, value in enumerate(values):
-                if self._formatters and i in self._formatters:
-                    formatter = self._formatters[i]
-                    value = formatter(value)
-
-                if self._naive_tz and isinstance(value, datetime) and not is_aware(value):
-                    value = make_aware(value, self._naive_tz)
-
-                values[i] = value
-        
-        if isinstance(values, Row):
-            return values
-        else:
-            return Row(values, headers=self.headers)
-
-
-    # -------------------------------------------------------------------------
-    # For subclasses
-    #
-
-    def _prepare(self):
-        pass
-
-
-    def _get_next_values(self) -> list[Any]:
-        raise StopIteration()
-
-
-    def _end(self):
-        pass
+from __future__ import annotations
+
+import logging
+from datetime import datetime, timezone
+from pathlib import Path
+from time import time_ns
+from typing import Any, Callable
+from ..misc import Literal
+from ..tabular import Row
+from ..datetime import is_aware, make_aware
+from .utils import normalize_inout, get_inout_name
+
+logger = logging.getLogger(__name__)
+
+
+class InTable:
+    @classmethod
+    def is_available(cls):
+        return True
+    
+    def __init__(self, src, *, src_dir: str|Path|None = None, title: str|None = None, debug: bool = None, naive_tz: Literal['local']|timezone = 'local', formatters: dict[int|str,Callable] = None, **kwargs):
+        if not self.is_available():
+            raise ValueError(f"cannot use {type(self).__name__} (not available)")
+
+        self._src = normalize_inout(src, dir=src_dir, title=title, **kwargs)
+        self._name = get_inout_name(self._src)
+
+        self._debug = debug
+        self._naive_tz = naive_tz
+        self._formatters = formatters
+
+        # set in _prepare():
+        self.headers: list[str] = None
+        self.prepare_duration: int = None
+
+        # set in __iter__():
+        self._extract_t0: int = None
+        self.extract_duration: int = None
+        self.row_count: int = None           
+
+
+    def __enter__(self):        
+        if self._debug:
+            logger.debug(f"prepare {self._name}")
+            t0 = time_ns()
+
+        self._prepare()
+
+        # headers are now set, update formatters to point on index
+        if self._formatters:
+            new_entries = {}
+            for key, formatter in self._formatters.items():
+                if not isinstance(key, str):
+                    continue
+                try:
+                    index = self.headers.index(key)
+                except ValueError:
+                    continue
+                new_entries[index] = formatter
+            
+            for index, formatter in new_entries.items():
+                self._formatters[index] = formatter
+
+
+        if self._debug:
+            self.prepare_duration = time_ns() - t0
+
+        return self
+
+
+    def __exit__(self, exc_type = None, exc_val = None, exc_tb = None):
+        self._end()
+
+        if self._debug:
+            if self.extract_duration is not None:
+                logger.debug(f"{self.row_count:,d} rows extracted from {self._name} (total duration: {(self.prepare_duration + self.extract_duration)/1e6:,.0f} ms, prepare: {self.prepare_duration/1e6:,.0f} ms, extract: {self.extract_duration/1e6:,.0f} ms)")
+            else:
+                logger.debug(f"prepared {self._name} (duration: {self.prepare_duration/1e6:,.0f} ms)")
+
+
+    def __iter__(self):
+        return self
+
+
+    def __next__(self):
+        if self.row_count is None:
+            if self._debug:
+                self._extract_t0 = time_ns()
+                self.extract_duration = 0
+            self.row_count = 0
+
+        values = self._get_next_values()
+
+        if self._debug:
+            self.extract_duration = time_ns() - self._extract_t0
+
+        self.row_count += 1
+
+        if self._naive_tz is not None or self._formatters:
+            if isinstance(values, Row):
+                values = list(values.values)
+            
+            for i, value in enumerate(values):
+                if self._formatters and i in self._formatters:
+                    formatter = self._formatters[i]
+                    value = formatter(value)
+
+                if self._naive_tz and isinstance(value, datetime) and not is_aware(value):
+                    value = make_aware(value, self._naive_tz)
+
+                values[i] = value
+        
+        if isinstance(values, Row):
+            return values
+        else:
+            return Row(values, headers=self.headers)
+
+
+    # -------------------------------------------------------------------------
+    # For subclasses
+    #
+
+    def _prepare(self):
+        pass
+
+
+    def _get_next_values(self) -> list[Any]:
+        raise StopIteration()
+
+
+    def _end(self):
+        pass
```

## zut/inout/OutCsv.py

 * *Ordering differences only*

```diff
@@ -1,122 +1,122 @@
-from __future__ import annotations
-import csv
-import logging
-from datetime import datetime
-from decimal import Decimal
-
-from io import IOBase
-import os
-from pathlib import Path
-from typing import Any, Iterable
-from ..misc import Literal
-from ..text import ValueString
-
-from ..numeric import get_default_decimal_separator
-from ..csv import get_csv_headers, get_default_csv_delimiter
-from .OutTable import OutTable
-
-logger = logging.getLogger(__name__)
-
-
-class OutCsv(OutTable):
-    _file: IOBase
-    
-    def __init__(self, out: str|Path|IOBase|Literal[False]|None = None, *, delimiter: str = None, quotechar: str = '"', decimal_separator: str = None, **kwargs):
-        self._delimiter = delimiter
-        if self._delimiter is None:
-            self._delimiter = get_default_csv_delimiter()
-
-        self._quotechar = quotechar
-
-        self._decimal_separator = decimal_separator
-        if self._decimal_separator is None:
-            self._decimal_separator = get_default_decimal_separator()
-
-        self._writer: csv._writer = None
-
-        super().__init__(out, newline='', **kwargs)
-
-
-    # -------------------------------------------------------------------------
-    # OutTable subclassing
-    #
-    
-    def _get_existing_headers(self) -> list[str]|None:
-        if not self._append:
-            return []  # file will be truncated
-        
-        if not isinstance(self._out, str) or not os.path.exists(self._out):
-            return []  # we consider output as a new file
-        
-        # From now on, we append to an existing file
-        existing_headers = get_csv_headers(self._out, encoding=self._encoding, delimiter=self._delimiter, quotechar=self._quotechar)
-        if existing_headers is None:
-            return []  # existing file is empty
-        
-        return existing_headers
-
-
-    def _export_new_headers(self, headers: list[str]):
-        self._get_writer().writerow(headers)
-        self._file.flush()
-
-
-    def _add_new_header(self, header: str, index: int):
-        # cannot modify existing headers (we don't modify existing file)
-        logger.warning(f"header \"{header}\" not found in existing headers: values will be appended at column {index + 1} without a column title")
-
-
-    def _export_prepared_row(self, row: Iterable):
-        self._get_writer().writerow(row)
-        self._file.flush()
-
-
-    def _format_value(self, value: Any, index: int) -> Any:
-        if isinstance(value, ValueString):
-            # For CSV we want the actual value (we can still apply formatting when opening in Excel)
-            # NOTE: we don't do it on the general case because the formatted ValueString may be interesting (e.g. for tabulate outputs)
-            value = value.value
-                
-        if isinstance(value, bool):
-            return 'true' if value else 'false'
-        elif isinstance(value, datetime):
-            # If output is expected in a given timezone, we make this datetime naive in the target timezone and display it in a format understandable by Excel
-            if value.tzinfo:
-                if self._tz:
-                    value: datetime = value.astimezone(None if self._tz == 'local' else self._tz)
-                    use_tzinfo = False
-                else:
-                    use_tzinfo = True
-            else:
-                use_tzinfo = False
-
-            # Format microseconds
-            if value.microsecond == 0:
-                mspart = ''
-            else:
-                mspart = '.' + value.strftime('%f')
-            
-            # Format tzinfo and microseconds
-            if use_tzinfo:
-                tzpart = value.strftime('%z')
-                if len(tzpart) == 5:
-                    tzpart = tzpart[0:3] + ':' + tzpart[3:]
-            else:
-                tzpart = ''
-
-            return value.strftime("%Y-%m-%d %H:%M:%S") + mspart + tzpart
-        elif isinstance(value, (float,Decimal)) and self._decimal_separator and self._decimal_separator != '.':
-            return str(value).replace('.', self._decimal_separator)
-        else:
-            return super()._format_value(value, index)
-        
-
-    # -------------------------------------------------------------------------
-    # Internal helpers
-    #    
-
-    def _get_writer(self):
-        if not self._writer:
-            # NOTE: setting a value for escapechar (with doublequote=False) fails: value containing escapechar is not quoted (cf. test_table_json)
-            self._writer = csv.writer(self._file, delimiter=self._delimiter, quotechar=self._quotechar)
-        return self._writer
+from __future__ import annotations
+import csv
+import logging
+from datetime import datetime
+from decimal import Decimal
+
+from io import IOBase
+import os
+from pathlib import Path
+from typing import Any, Iterable
+from ..misc import Literal
+from ..text import ValueString
+
+from ..numeric import get_default_decimal_separator
+from ..csv import get_csv_headers, get_default_csv_delimiter
+from .OutTable import OutTable
+
+logger = logging.getLogger(__name__)
+
+
+class OutCsv(OutTable):
+    _file: IOBase
+    
+    def __init__(self, out: str|Path|IOBase|Literal[False]|None = None, *, delimiter: str = None, quotechar: str = '"', decimal_separator: str = None, **kwargs):
+        self._delimiter = delimiter
+        if self._delimiter is None:
+            self._delimiter = get_default_csv_delimiter()
+
+        self._quotechar = quotechar
+
+        self._decimal_separator = decimal_separator
+        if self._decimal_separator is None:
+            self._decimal_separator = get_default_decimal_separator()
+
+        self._writer: csv._writer = None
+
+        super().__init__(out, newline='', **kwargs)
+
+
+    # -------------------------------------------------------------------------
+    # OutTable subclassing
+    #
+    
+    def _get_existing_headers(self) -> list[str]|None:
+        if not self._append:
+            return []  # file will be truncated
+        
+        if not isinstance(self._out, str) or not os.path.exists(self._out):
+            return []  # we consider output as a new file
+        
+        # From now on, we append to an existing file
+        existing_headers = get_csv_headers(self._out, encoding=self._encoding, delimiter=self._delimiter, quotechar=self._quotechar)
+        if existing_headers is None:
+            return []  # existing file is empty
+        
+        return existing_headers
+
+
+    def _export_new_headers(self, headers: list[str]):
+        self._get_writer().writerow(headers)
+        self._file.flush()
+
+
+    def _add_new_header(self, header: str, index: int):
+        # cannot modify existing headers (we don't modify existing file)
+        logger.warning(f"header \"{header}\" not found in existing headers: values will be appended at column {index + 1} without a column title")
+
+
+    def _export_prepared_row(self, row: Iterable):
+        self._get_writer().writerow(row)
+        self._file.flush()
+
+
+    def _format_value(self, value: Any, index: int) -> Any:
+        if isinstance(value, ValueString):
+            # For CSV we want the actual value (we can still apply formatting when opening in Excel)
+            # NOTE: we don't do it on the general case because the formatted ValueString may be interesting (e.g. for tabulate outputs)
+            value = value.value
+                
+        if isinstance(value, bool):
+            return 'true' if value else 'false'
+        elif isinstance(value, datetime):
+            # If output is expected in a given timezone, we make this datetime naive in the target timezone and display it in a format understandable by Excel
+            if value.tzinfo:
+                if self._tz:
+                    value: datetime = value.astimezone(None if self._tz == 'local' else self._tz)
+                    use_tzinfo = False
+                else:
+                    use_tzinfo = True
+            else:
+                use_tzinfo = False
+
+            # Format microseconds
+            if value.microsecond == 0:
+                mspart = ''
+            else:
+                mspart = '.' + value.strftime('%f')
+            
+            # Format tzinfo and microseconds
+            if use_tzinfo:
+                tzpart = value.strftime('%z')
+                if len(tzpart) == 5:
+                    tzpart = tzpart[0:3] + ':' + tzpart[3:]
+            else:
+                tzpart = ''
+
+            return value.strftime("%Y-%m-%d %H:%M:%S") + mspart + tzpart
+        elif isinstance(value, (float,Decimal)) and self._decimal_separator and self._decimal_separator != '.':
+            return str(value).replace('.', self._decimal_separator)
+        else:
+            return super()._format_value(value, index)
+        
+
+    # -------------------------------------------------------------------------
+    # Internal helpers
+    #    
+
+    def _get_writer(self):
+        if not self._writer:
+            # NOTE: setting a value for escapechar (with doublequote=False) fails: value containing escapechar is not quoted (cf. test_table_json)
+            self._writer = csv.writer(self._file, delimiter=self._delimiter, quotechar=self._quotechar)
+        return self._writer
```

## zut/inout/OutDb.py

```diff
@@ -1,69 +1,75 @@
-from __future__ import annotations
-from io import StringIO
-import logging
-
-from ..db import get_db_with_schema_and_table
-from .OutCsv import OutCsv
-
-logger = logging.getLogger(__name__)
-
-
-class OutDb(OutCsv):
-    def __init__(self, out: str, decimal_separator: str = None, **kwargs):
-        # Prepare arguments for base classes (OutCsv, OutTable, OutFile)
-        if not isinstance(out, str):
-            raise ValueError(f"OutDb's out must be a str, got {type(out).__name__}: {out}")
-    
-        decimal_separator = '.'
-        
-        # Initialize base classes (OutCsv, OutTable, OutFile)
-        super().__init__(StringIO(), decimal_separator=decimal_separator, **kwargs)
-
-        # Modify attributes set by base classes (OutCsv, OutTable, OutFile)
-        self._name = out
-
-        self._db, self._schema_name, self._table_name = get_db_with_schema_and_table(out)
-        if not self._table_name:
-            raise ValueError(f"invalid db target: table name not provided")
-
-
-    # -------------------------------------------------------------------------
-    # OutFile subclassing
-    #
-
-    def _open_file(self):
-        super()._open_file()
-
-        # Connect to database
-        self._db.__enter__()
-
-        # Create, drop or truncate table
-        if not self._append:
-            logger.debug(f"truncate table %s.%s", self._schema_name, self._table_name)
-            self._db.truncate_table((self._schema_name, self._table_name))
-                
-
-    def _close_file(self):
-        if not self._headers:
-            if self._row_count == 0:
-                return
-            raise ValueError(f"cannot export rows to database: no headers")
-                
-        logger.debug(f"copy data to table %s.%s", self._schema_name, self._table_name)
-        self._out.seek(0)
-        self._db.copy_from_csv(self._out, (self._schema_name, self._table_name), columns=self._headers, delimiter=self._delimiter, quotechar=self._quotechar)
-        self._db.__exit__()
-
-
-    # -------------------------------------------------------------------------
-    # OutTable subclassing
-    #
-    
-    def _get_existing_headers(self) -> list[str]|None:
-        headers = self._db.get_table_column_names((self._schema_name, self._table_name))
-        super()._export_new_headers(headers) # write headers to memory StringIO
-        return headers
-
-
-    def _add_new_header(self, header: str, index: int):
-        raise ValueError(f"new header ({header}) cannot be added to table ({self._schema_name}, {self._table_name})")
+from __future__ import annotations
+from io import StringIO
+import logging
+
+from ..db import create_db_wrapper_with_schema_and_table
+from .OutCsv import OutCsv
+
+logger = logging.getLogger(__name__)
+
+
+class OutDb(OutCsv):
+    def __init__(self, out: str, decimal_separator: str = None, **kwargs):
+        # Prepare arguments for base classes (OutCsv, OutTable, OutFile)
+        if not isinstance(out, str):
+            raise ValueError(f"OutDb's out must be a str, got {type(out).__name__}: {out}")
+    
+        decimal_separator = '.'
+        
+        # Initialize base classes (OutCsv, OutTable, OutFile)
+        super().__init__(StringIO(), decimal_separator=decimal_separator, **kwargs)
+
+        # Modify attributes set by base classes (OutCsv, OutTable, OutFile)
+        self._db, self._schema_name, self._table_name = create_db_wrapper_with_schema_and_table(out.format(**kwargs))
+        if not self._table_name:
+            raise ValueError(f"invalid db target: table name not provided")
+
+        self._name = self._db.get_uri(table=(self._schema_name, self._table_name), with_password=False)
+
+    # -------------------------------------------------------------------------
+    # OutFile subclassing
+    #
+
+    def _open_file(self):
+        super()._open_file()
+
+        # Connect to database
+        self._db.__enter__()
+
+        # Create, drop or truncate table
+        if not self._append:
+            logger.debug(f"truncate table %s.%s", self._schema_name, self._table_name)
+            self._db.truncate_table((self._schema_name, self._table_name))
+                
+
+    def _close_file(self):
+        if not self._headers:
+            if self._row_count == 0:
+                return
+            raise ValueError(f"cannot export rows to database: no headers")
+                
+        logger.debug(f"copy data to table %s.%s", self._schema_name, self._table_name)
+        self._out.seek(0)
+        self._db.copy_from_csv(self._out, (self._schema_name, self._table_name), columns=self._headers, delimiter=self._delimiter, quotechar=self._quotechar)
+        self._db.__exit__()
+
+
+    # -------------------------------------------------------------------------
+    # OutTable subclassing
+    #
+    
+    def _get_existing_headers(self) -> list[str]|None:
+        # Only export given headers, but check that they are in the target table
+        column_names = self._db.get_table_column_names((self._schema_name, self._table_name))
+        if not self._headers:
+            raise ValueError(f'headers must be set')
+        
+        missing_columns = []
+        for header in self._headers:
+            if not header in column_names:
+                missing_columns.append(header)
+
+        if missing_columns:
+            raise ValueError(f"column not found in out table: {', '.join(missing_columns)}")
+
+        return []
```

## zut/inout/OutExcel.py

 * *Ordering differences only*

```diff
@@ -1,100 +1,100 @@
-from __future__ import annotations
-from datetime import datetime
-import logging
-import os
-from pathlib import Path
-from typing import Any, Iterable
-from .OutTable import OutTable
-from .utils import get_inout_name, split_excel_path
-from ..misc import Literal
-
-try:
-    from ..excel import ExcelWorkbook
-    _available = True
-except ImportError:
-    _available = False
-
-
-logger = logging.getLogger(__name__)
-
-
-class OutExcel(OutTable):
-    _file: ExcelWorkbook
-    _DEFAULT_TABLE_NAME = 'Out'
-
-    @classmethod
-    def is_available(cls):
-        return _available
-    
-
-    def __init__(self, out = None, **kwargs):
-        # Prepare arguments for base classes (OutTable, OutFile)
-        if not isinstance(out, (str,Path)):
-            raise ValueError(f"OutExcel's out must be a str or path, got {type(out).__name__}: {out}")
-        
-        out, self._table_name = split_excel_path(out)
-        if not self._table_name:
-            self._table_name = self._DEFAULT_TABLE_NAME
-        
-        # Initialize base classes (OutTable, OutFile)
-        super().__init__(out=out, **kwargs)
-        
-        # Modify attributes set by base classes (OutTable, OutFile)
-        self._name = get_inout_name(self._out) + f'#{self._table_name}'
-
-
-    # -------------------------------------------------------------------------
-    # OutFile subclassing
-    #
-
-    def _open_file(self):
-        self._print_title()
-
-        parent = os.path.dirname(self._out)
-        if parent and not os.path.exists(parent):
-            os.makedirs(parent)
-
-        self._file = ExcelWorkbook.get_or_create_cached(self._out)
-        self._must_close_file = True
-
-        self._table = self._file.get_table(self._table_name, default=None)
-        if not self._table:
-            self._table = self._file.create_table(self._table_name, no_headers=True if not self._headers else False)
-            #TODO: test out_table without headers.
-            #FIXME: Probably we need to call insert_col at some point
-        
-        if not self._append:
-            self._table.truncate()
-
-
-    # -------------------------------------------------------------------------
-    # OutTable subclassing
-    #
-
-    def _get_existing_headers(self) -> list[str]|None:
-        return self._table.column_names if self._table.has_headers else None
-    
-
-    def _add_new_header(self, header: str, index: int):
-        self._table.insert_col(header)
-    
-
-    def _export_prepared_row(self, row: Iterable):
-        table_row = self._table.insert_row()
-        
-        for i, value in enumerate(row):
-            if i < len(table_row):            
-                table_row[i] = value
-            else:
-                logger.warning(f'ignore values from index {i} ({value})')
-                break
-
-
-    def _format_value(self, value: Any, index: int) -> Any:
-        if isinstance(value, datetime) and value.tzinfo:
-            # Excel does not support timezones in datetimes
-            if self._tz:
-                value = value.astimezone(None if self._tz == 'local' else self._tz)
-            return value.replace(tzinfo=None)
-        else:
-            return super()._format_value(value, index)
+from __future__ import annotations
+from datetime import datetime
+import logging
+import os
+from pathlib import Path
+from typing import Any, Iterable
+from .OutTable import OutTable
+from .utils import get_inout_name, split_excel_path
+from ..misc import Literal
+
+try:
+    from ..excel import ExcelWorkbook
+    _available = True
+except ImportError:
+    _available = False
+
+
+logger = logging.getLogger(__name__)
+
+
+class OutExcel(OutTable):
+    _file: ExcelWorkbook
+    _DEFAULT_TABLE_NAME = 'Out'
+
+    @classmethod
+    def is_available(cls):
+        return _available
+    
+
+    def __init__(self, out = None, **kwargs):
+        # Prepare arguments for base classes (OutTable, OutFile)
+        if not isinstance(out, (str,Path)):
+            raise ValueError(f"OutExcel's out must be a str or path, got {type(out).__name__}: {out}")
+        
+        out, self._table_name = split_excel_path(out)
+        if not self._table_name:
+            self._table_name = self._DEFAULT_TABLE_NAME
+        
+        # Initialize base classes (OutTable, OutFile)
+        super().__init__(out=out, **kwargs)
+        
+        # Modify attributes set by base classes (OutTable, OutFile)
+        self._name = get_inout_name(self._out) + f'#{self._table_name}'
+
+
+    # -------------------------------------------------------------------------
+    # OutFile subclassing
+    #
+
+    def _open_file(self):
+        self._print_title()
+
+        parent = os.path.dirname(self._out)
+        if parent and not os.path.exists(parent):
+            os.makedirs(parent)
+
+        self._file = ExcelWorkbook.get_or_create_cached(self._out)
+        self._must_close_file = True
+
+        self._table = self._file.get_table(self._table_name, default=None)
+        if not self._table:
+            self._table = self._file.create_table(self._table_name, no_headers=True if not self._headers else False)
+            #TODO: test out_table without headers.
+            #FIXME: Probably we need to call insert_col at some point
+        
+        if not self._append:
+            self._table.truncate()
+
+
+    # -------------------------------------------------------------------------
+    # OutTable subclassing
+    #
+
+    def _get_existing_headers(self) -> list[str]|None:
+        return self._table.column_names if self._table.has_headers else None
+    
+
+    def _add_new_header(self, header: str, index: int):
+        self._table.insert_col(header)
+    
+
+    def _export_prepared_row(self, row: Iterable):
+        table_row = self._table.insert_row()
+        
+        for i, value in enumerate(row):
+            if i < len(table_row):            
+                table_row[i] = value
+            else:
+                logger.warning(f'ignore values from index {i} ({value})')
+                break
+
+
+    def _format_value(self, value: Any, index: int) -> Any:
+        if isinstance(value, datetime) and value.tzinfo:
+            # Excel does not support timezones in datetimes
+            if self._tz:
+                value = value.astimezone(None if self._tz == 'local' else self._tz)
+            return value.replace(tzinfo=None)
+        else:
+            return super()._format_value(value, index)
```

## zut/inout/OutFile.py

```diff
@@ -1,103 +1,103 @@
-from __future__ import annotations
-from io import IOBase
-from atexit import register as register_atexit
-import logging
-import os
-from pathlib import Path
-import sys
-from typing import Any
-from ..misc import Literal
-from .. import filesh
-
-from .utils import get_inout_name, normalize_inout, Closable
-
-logger = logging.getLogger(__name__)
-
-
-class OutFile:
-    def __init__(self, out: str|Path|IOBase|Literal[False]|None = None, *, out_dir: str|Path|None = None, title: str|None = None, append: bool = False, encoding: str = 'utf-8-sig', newline: str = None, atexit: bool = False, **kwargs):
-        self._out = normalize_inout(out, directory=out_dir, title=title, **kwargs)
-        self._name = get_inout_name(self._out)
-        
-        self._title = title
-        self._append = append
-        self._encoding = encoding
-        self._newline = newline
-
-        self._file: IOBase = None
-        self._must_close_file: bool = True
-
-        self._end_atexit = atexit
-        """ If true, end export at program exit (instead of at context exit) """
-
-
-    # -------------------------------------------------------------------------
-    # Enter/exit context
-    #
-
-    def __enter__(self) -> IOBase:
-        if self._end_atexit:
-            register_atexit(self._end)
-
-        self._open_file()
-        return self._file
-
-
-    def __exit__(self, exc_type = None, exc_val = None, exc_tb = None):
-        if not self._end_atexit:
-            self._end()
-
-            
-    def _end(self):
-        """
-        This method is executed when context ends (__exit__),
-        except if `_end_atexit` (in this case it is executed when program ends).
-        """
-        self._close_file()
-
-
-    # -------------------------------------------------------------------------
-    # Open file
-    #
-
-    def _open_file(self):
-        self._print_title()
-
-        if self._out in [sys.stdout, sys.stderr]:
-            self._file = self._out
-            self._must_close_file = False
-            
-        else:
-            if isinstance(self._out, IOBase):
-                self._file = self._out
-                self._must_close_file = False
-            
-            else:
-                parent = os.path.dirname(self._out)
-                if parent and not os.path.exists(parent):
-                    os.makedirs(parent)
-
-                self._file = filesh.open_file(self._out, 'a' if self._append else 'w', newline=self._newline, encoding=self._encoding)
-                self._must_close_file = True
-
-
-    def _print_title(self):
-        if self._title is False:
-            return
-        if self._out == os.devnull:
-            return
-
-        if self._out in [sys.stdout, sys.stderr]:
-            if self._title:
-                print(f"\n########## {self._title} ##########\n", file=self._out)
-        else:
-            logger.info(f"{'append' if self._append else 'export'}{f' {self._title}' if self._title else ''} to {self._name}")
-
-
-    # -------------------------------------------------------------------------
-    # Close file
-    #    
-
-    def _close_file(self):
-        if self._file and self._must_close_file:
-            self._file.close()
+from __future__ import annotations
+from io import IOBase
+from atexit import register as register_atexit
+import logging
+import os
+from pathlib import Path
+import sys
+from typing import Any
+from ..misc import Literal
+from .. import filesh
+
+from .utils import get_inout_name, normalize_inout, Closable
+
+logger = logging.getLogger(__name__)
+
+
+class OutFile:
+    def __init__(self, out: str|Path|IOBase|Literal[False]|None = None, *, out_dir: str|Path|None = None, title: str|None = None, append: bool = False, encoding: str = 'utf-8-sig', newline: str = None, atexit: bool = False, **kwargs):
+        self._out = normalize_inout(out, dir=out_dir, title=title, **kwargs)
+        self._name = get_inout_name(self._out)
+        
+        self._title = title
+        self._append = append
+        self._encoding = encoding
+        self._newline = newline
+
+        self._file: IOBase = None
+        self._must_close_file: bool = True
+
+        self._end_atexit = atexit
+        """ If true, end export at program exit (instead of at context exit) """
+
+
+    # -------------------------------------------------------------------------
+    # Enter/exit context
+    #
+
+    def __enter__(self) -> IOBase:
+        if self._end_atexit:
+            register_atexit(self._end)
+
+        self._open_file()
+        return self._file
+
+
+    def __exit__(self, exc_type = None, exc_val = None, exc_tb = None):
+        if not self._end_atexit:
+            self._end()
+
+            
+    def _end(self):
+        """
+        This method is executed when context ends (__exit__),
+        except if `_end_atexit` (in this case it is executed when program ends).
+        """
+        self._close_file()
+
+
+    # -------------------------------------------------------------------------
+    # Open file
+    #
+
+    def _open_file(self):
+        self._print_title()
+
+        if self._out in [sys.stdout, sys.stderr]:
+            self._file = self._out
+            self._must_close_file = False
+            
+        else:
+            if isinstance(self._out, IOBase):
+                self._file = self._out
+                self._must_close_file = False
+            
+            else:
+                parent = os.path.dirname(self._out)
+                if parent and not os.path.exists(parent):
+                    os.makedirs(parent)
+
+                self._file = filesh.open_file(self._out, 'a' if self._append else 'w', newline=self._newline, encoding=self._encoding)
+                self._must_close_file = True
+
+
+    def _print_title(self):
+        if self._title is False:
+            return
+        if self._out == os.devnull:
+            return
+
+        if self._out in [sys.stdout, sys.stderr]:
+            if self._title:
+                print(f"\n########## {self._title} ##########\n", file=self._out)
+        else:
+            logger.info(f"{'append' if self._append else 'export'}{f' {self._title}' if self._title else ''} to {self._name}")
+
+
+    # -------------------------------------------------------------------------
+    # Close file
+    #    
+
+    def _close_file(self):
+        if self._file and self._must_close_file:
+            self._file.close()
```

## zut/inout/OutTable.py

 * *Ordering differences only*

```diff
@@ -1,281 +1,281 @@
-from __future__ import annotations
-from datetime import datetime, timezone
-from decimal import Decimal
-from enum import Enum
-from atexit import register as register_atexit
-from io import IOBase
-import json
-import logging
-from pathlib import Path
-from types import FunctionType
-from typing import Any, Iterable
-from ..misc import Literal, is_list_or_tuple_of
-from .OutFile import OutFile
-from .utils import Closable
-
-logger = logging.getLogger(__name__)
-
-
-class OutTable(OutFile):
-    _file: Closable
-
-    """
-    A mixin for appending and exporting tabular data.
-
-    Must be subclassed (to implement read/write of value).
-    """
-    @classmethod
-    def is_available(cls):
-        return True
-    
-    def __init__(self, out: str|Path|IOBase|Literal[False]|None = None, append: bool = False, headers: list[str] = None, tz: Literal['local', 'utc']|timezone = None, **kwargs):
-        if not self.is_available():
-            raise ValueError(f"cannot use {type(self).__name__} (not available)")
-    
-        super().__init__(out, **kwargs)
-        
-        self._append = append
-
-        self._start_atonce = True if headers else False # we consider than if headers are given now, they are final, so we don't need to wait for dict rows
-        """ If true, delay rows until end of export """
-
-        self._actually_started = False
-
-        self._headers: list[str]|None = [str(header) for header in headers] if headers else None        
-        self._row_count = 0
-
-        self._delayed_rows: list[Iterable] = []
-
-        self._reordering: list[int]|None = None
-        self._reordering_default: list|None = None
-
-        self._tz = timezone.utc if tz == 'utc' else tz
-
-
-    # -------------------------------------------------------------------------
-    # Actions on start/end (see OutFile)
-    #
-
-    def __enter__(self) -> OutTable:
-        if self._end_atexit:
-            register_atexit(self._end)
-
-        if self._start_atonce:
-            if self._headers:
-                self._open_file()
-                self._export_headers()
-                self._actually_started = True
-
-        return self
-    
-
-    def _end(self):
-        """
-        This method is executed when context ends (__exit__),
-        except if `_end_atexit` (in this case it is executed when program ends).
-        """
-        if not self._actually_started:
-            self._open_file()
-            self._export_headers(at_end=True)
-            self._actually_started = True
-
-        self._export_delayed_rows()
-
-        self._delayed_rows = []
-        self._close_file()
-    
-
-    # -------------------------------------------------------------------------
-    # Following methods are the core table export mechanism.
-    # They are not supposed to be subclassed.
-    #
-
-    def _export_headers(self, at_end=False):
-        existing_headers = self._get_existing_headers()
-
-        if not self._headers:
-            if existing_headers:
-                if not (at_end and not self._delayed_rows):
-                    raise ValueError(f"cannot export table without headers: existing table contains headers ({', '.join(existing_headers)})")
-            return
-        
-        if existing_headers is None:
-            if self._headers:
-                if not (at_end and not self._delayed_rows):
-                    raise ValueError(f"cannot export table with headers: existing table cannot contain headers")
-            return
-        
-        if existing_headers == self._headers:
-            return
-
-        if len(existing_headers) == 0:
-            self._export_new_headers(self._headers)
-            return
-
-        # From now own, we must compare existing and target headers
-        self._reordering = []
-        self._reordering_default = [None] * len(existing_headers)
-
-        for header in self._headers:
-            try:
-                index = existing_headers.index(header)
-                self._reordering.append(index)
-            except ValueError:
-                self._reordering_default.append(None)
-                index = len(self._reordering_default) - 1
-                self._reordering.append(index)
-                self._add_new_header(header, index)
-    
-
-    def append(self, row):
-        if isinstance(row, dict):
-            row = self._dict_to_row(row)        
-        else:
-            row = self._iterable_to_row(row)
-
-        if self._start_atonce:
-            self._export_row(row)
-        else:
-            self._delayed_rows.append(row)
-        
-        self._row_count += 1
-
-
-    def _export_row(self, row: Iterable):
-        if not self._actually_started:
-            self._open_file()
-            self._export_headers()
-            self._actually_started = True
-
-        self._export_prepared_row(self._prepare_row(row))
-
-
-    def _prepare_row(self, data: Iterable):
-        if not self._reordering:
-            row = [self._format_value(value, i) for i, value in enumerate(data)]
-            if self._headers:
-                while len(row) < len(self._headers):
-                    row.append(None)
-            return row
-        
-        else:
-            row = list(self._reordering_default)
-            for i, value in enumerate(data):
-                value = self._format_value(value, i)
-                if i < len(self._reordering):
-                    index = self._reordering[i]
-                    row[index] = value
-                else:
-                    row.append(value)
-
-            return row
-
-
-    def _iterable_to_row(self, data: Iterable):
-        if self._headers is not None and len(data) != len(self._headers):
-            logger.warning(f"row {self._row_count+1} length: {len(data)} (expected headers length: {len(self._headers)})")
-        
-        return data
-        
-
-    def _dict_to_row(self, data: dict):
-        row = [None] * (len(self._headers) if self._headers else 0)
-
-        for header, value in data.items():
-            index = self._fetch_header(header)
-            while index >= len(row):
-                row.append(None)
-            row[index] = value
-            
-        return row
-
-
-    def _fetch_header(self, header: str) -> int:
-        if not isinstance(header, str):
-            header = str(header)
-
-        if self._headers is None:
-            self._headers = []
-
-        try:
-            index = self._headers.index(header)
-        except:
-            self._headers.append(header)
-            index = len(self._headers) - 1
-
-            if self._actually_started:
-                logger.warning(f"row {self._row_count + 1} header \"{header}\" not found in exported headers: values will be appended at column {len(self._headers)} with an empty header")
-            
-        return index
-
-
-    # -------------------------------------------------------------------------
-    # These methods are available for subclassing. 
-    #
-    
-    def _get_existing_headers(self) -> list[str]|None:
-        """
-        Get headers to consider in the existing target.
-
-        Return a list (including empty list) if headers make sense.
-
-        Return `None` if headers cannot be used at all.
-        """
-        raise NotImplementedError(f"method must be implemented by {type(self).__name__}")
-
-
-    def _export_new_headers(self, headers: list[str]):
-        """
-        Export headers when the list of exiting headers is empty.
-        """        
-        for index, header in enumerate(headers):
-            self._add_new_header(header, index)
-
-
-    def _add_new_header(self, header: str, index: int):
-        """
-        Add a new header to an existing list of headers.
-        """
-        raise NotImplementedError(f"method must be implemented by {type(self).__name__}")
-
-
-    def _export_prepared_row(self, row: Iterable):
-        """
-        Export a row that has been reordered (if necessary) to match existing headers + newly added headers.
-        """
-        raise NotImplementedError(f"method must be implemented by {type(self).__name__}")
-
-
-    def _format_value(self, value: Any, index: int) -> Any:
-        if value is None:
-            return None
-        elif isinstance(value, str):
-            return value
-        elif isinstance(value, Enum):
-            return value.name
-        elif isinstance(value, (dict,list,tuple)):
-            # Display as a list of elements separated by "|"
-            if is_list_or_tuple_of(value, (str,int,float,Decimal,bool,type(None))):
-                str_elements = []
-                for element in value:
-                    if element is None:
-                        str_element = ''
-                    else:
-                        str_element = self._format_value(element, index)
-                        if not isinstance(str_element, str):
-                            str_element = str(str_element)
-                        if '|' in str_element:
-                            return json.dumps(value) # cancel
-                    str_elements.append(str_element)
-                return '|'.join(str_elements)
-            else:
-                return json.dumps(value)
-        else:
-            # NOTE: we don't transform everything in strings, because the underlying output system may take into account the actual type
-            # (example: tabulate will align numeric values differently)
-            return value
-
-
-    def _export_delayed_rows(self):
-        for row in self._delayed_rows:
-            self._export_row(row)
+from __future__ import annotations
+from datetime import datetime, timezone
+from decimal import Decimal
+from enum import Enum
+from atexit import register as register_atexit
+from io import IOBase
+import json
+import logging
+from pathlib import Path
+from types import FunctionType
+from typing import Any, Iterable
+from ..misc import Literal, is_list_or_tuple_of
+from .OutFile import OutFile
+from .utils import Closable
+
+logger = logging.getLogger(__name__)
+
+
+class OutTable(OutFile):
+    _file: Closable
+
+    """
+    A mixin for appending and exporting tabular data.
+
+    Must be subclassed (to implement read/write of value).
+    """
+    @classmethod
+    def is_available(cls):
+        return True
+    
+    def __init__(self, out: str|Path|IOBase|Literal[False]|None = None, append: bool = False, headers: list[str] = None, tz: Literal['local', 'utc']|timezone = None, **kwargs):
+        if not self.is_available():
+            raise ValueError(f"cannot use {type(self).__name__} (not available)")
+    
+        super().__init__(out, **kwargs)
+        
+        self._append = append
+
+        self._start_atonce = True if headers else False # we consider than if headers are given now, they are final, so we don't need to wait for dict rows
+        """ If true, delay rows until end of export """
+
+        self._actually_started = False
+
+        self._headers: list[str]|None = [str(header) for header in headers] if headers else None        
+        self._row_count = 0
+
+        self._delayed_rows: list[Iterable] = []
+
+        self._reordering: list[int]|None = None
+        self._reordering_default: list|None = None
+
+        self._tz = timezone.utc if tz == 'utc' else tz
+
+
+    # -------------------------------------------------------------------------
+    # Actions on start/end (see OutFile)
+    #
+
+    def __enter__(self) -> OutTable:
+        if self._end_atexit:
+            register_atexit(self._end)
+
+        if self._start_atonce:
+            if self._headers:
+                self._open_file()
+                self._export_headers()
+                self._actually_started = True
+
+        return self
+    
+
+    def _end(self):
+        """
+        This method is executed when context ends (__exit__),
+        except if `_end_atexit` (in this case it is executed when program ends).
+        """
+        if not self._actually_started:
+            self._open_file()
+            self._export_headers(at_end=True)
+            self._actually_started = True
+
+        self._export_delayed_rows()
+
+        self._delayed_rows = []
+        self._close_file()
+    
+
+    # -------------------------------------------------------------------------
+    # Following methods are the core table export mechanism.
+    # They are not supposed to be subclassed.
+    #
+
+    def _export_headers(self, at_end=False):
+        existing_headers = self._get_existing_headers()
+
+        if not self._headers:
+            if existing_headers:
+                if not (at_end and not self._delayed_rows):
+                    raise ValueError(f"cannot export table without headers: existing table contains headers ({', '.join(existing_headers)})")
+            return
+        
+        if existing_headers is None:
+            if self._headers:
+                if not (at_end and not self._delayed_rows):
+                    raise ValueError(f"cannot export table with headers: existing table cannot contain headers")
+            return
+        
+        if existing_headers == self._headers:
+            return
+
+        if len(existing_headers) == 0:
+            self._export_new_headers(self._headers)
+            return
+
+        # From now own, we must compare existing and target headers
+        self._reordering = []
+        self._reordering_default = [None] * len(existing_headers)
+
+        for header in self._headers:
+            try:
+                index = existing_headers.index(header)
+                self._reordering.append(index)
+            except ValueError:
+                self._reordering_default.append(None)
+                index = len(self._reordering_default) - 1
+                self._reordering.append(index)
+                self._add_new_header(header, index)
+    
+
+    def append(self, row):
+        if isinstance(row, dict):
+            row = self._dict_to_row(row)        
+        else:
+            row = self._iterable_to_row(row)
+
+        if self._start_atonce:
+            self._export_row(row)
+        else:
+            self._delayed_rows.append(row)
+        
+        self._row_count += 1
+
+
+    def _export_row(self, row: Iterable):
+        if not self._actually_started:
+            self._open_file()
+            self._export_headers()
+            self._actually_started = True
+
+        self._export_prepared_row(self._prepare_row(row))
+
+
+    def _prepare_row(self, data: Iterable):
+        if not self._reordering:
+            row = [self._format_value(value, i) for i, value in enumerate(data)]
+            if self._headers:
+                while len(row) < len(self._headers):
+                    row.append(None)
+            return row
+        
+        else:
+            row = list(self._reordering_default)
+            for i, value in enumerate(data):
+                value = self._format_value(value, i)
+                if i < len(self._reordering):
+                    index = self._reordering[i]
+                    row[index] = value
+                else:
+                    row.append(value)
+
+            return row
+
+
+    def _iterable_to_row(self, data: Iterable):
+        if self._headers is not None and len(data) != len(self._headers):
+            logger.warning(f"row {self._row_count+1} length: {len(data)} (expected headers length: {len(self._headers)})")
+        
+        return data
+        
+
+    def _dict_to_row(self, data: dict):
+        row = [None] * (len(self._headers) if self._headers else 0)
+
+        for header, value in data.items():
+            index = self._fetch_header(header)
+            while index >= len(row):
+                row.append(None)
+            row[index] = value
+            
+        return row
+
+
+    def _fetch_header(self, header: str) -> int:
+        if not isinstance(header, str):
+            header = str(header)
+
+        if self._headers is None:
+            self._headers = []
+
+        try:
+            index = self._headers.index(header)
+        except:
+            self._headers.append(header)
+            index = len(self._headers) - 1
+
+            if self._actually_started:
+                logger.warning(f"row {self._row_count + 1} header \"{header}\" not found in exported headers: values will be appended at column {len(self._headers)} with an empty header")
+            
+        return index
+
+
+    # -------------------------------------------------------------------------
+    # These methods are available for subclassing. 
+    #
+    
+    def _get_existing_headers(self) -> list[str]|None:
+        """
+        Get headers to consider in the existing target.
+
+        Return a list (including empty list) if headers make sense.
+
+        Return `None` if headers cannot be used at all.
+        """
+        raise NotImplementedError(f"method must be implemented by {type(self).__name__}")
+
+
+    def _export_new_headers(self, headers: list[str]):
+        """
+        Export headers when the list of exiting headers is empty.
+        """        
+        for index, header in enumerate(headers):
+            self._add_new_header(header, index)
+
+
+    def _add_new_header(self, header: str, index: int):
+        """
+        Add a new header to an existing list of headers.
+        """
+        raise NotImplementedError(f"method must be implemented by {type(self).__name__}")
+
+
+    def _export_prepared_row(self, row: Iterable):
+        """
+        Export a row that has been reordered (if necessary) to match existing headers + newly added headers.
+        """
+        raise NotImplementedError(f"method must be implemented by {type(self).__name__}")
+
+
+    def _format_value(self, value: Any, index: int) -> Any:
+        if value is None:
+            return None
+        elif isinstance(value, str):
+            return value
+        elif isinstance(value, Enum):
+            return value.name
+        elif isinstance(value, (dict,list,tuple)):
+            # Display as a list of elements separated by "|"
+            if is_list_or_tuple_of(value, (str,int,float,Decimal,bool,type(None))):
+                str_elements = []
+                for element in value:
+                    if element is None:
+                        str_element = ''
+                    else:
+                        str_element = self._format_value(element, index)
+                        if not isinstance(str_element, str):
+                            str_element = str(str_element)
+                        if '|' in str_element:
+                            return json.dumps(value) # cancel
+                    str_elements.append(str_element)
+                return '|'.join(str_elements)
+            else:
+                return json.dumps(value)
+        else:
+            # NOTE: we don't transform everything in strings, because the underlying output system may take into account the actual type
+            # (example: tabulate will align numeric values differently)
+            return value
+
+
+    def _export_delayed_rows(self):
+        for row in self._delayed_rows:
+            self._export_row(row)
```

## zut/inout/OutTabulate.py

 * *Ordering differences only*

```diff
@@ -1,44 +1,44 @@
-from __future__ import annotations
-import logging
-from .OutTable import OutTable
-
-try:
-    from tabulate import tabulate
-    _available = True
-except ImportError:
-    _available = False
-
-
-logger = logging.getLogger(__name__)
-
-
-class OutTabulate(OutTable):
-    @classmethod
-    def is_available(cls):
-        return _available
-
-    def __init__(self, out = None, **kwargs):
-        super().__init__(out, **kwargs)
-        self._start_atonce = False
-
-    # -------------------------------------------------------------------------
-    # OutTable subclassing
-    #    
-    def _export_delayed_rows(self):
-        if not self._delayed_rows and not self._headers:
-            print("no data", file=self._file)
-            return
-        
-        rows = [self._prepare_row(row) for row in self._delayed_rows]
-        if self._headers:
-            result = tabulate(rows, headers=self._headers)
-        else:
-            result = tabulate(rows)
-        print(result, file=self._file)
-
-
-    def _get_existing_headers(self) -> list[str]|None:
-        return []
-
-    def _export_new_headers(self, headers: list[str]):
-        pass
+from __future__ import annotations
+import logging
+from .OutTable import OutTable
+
+try:
+    from tabulate import tabulate
+    _available = True
+except ImportError:
+    _available = False
+
+
+logger = logging.getLogger(__name__)
+
+
+class OutTabulate(OutTable):
+    @classmethod
+    def is_available(cls):
+        return _available
+
+    def __init__(self, out = None, **kwargs):
+        super().__init__(out, **kwargs)
+        self._start_atonce = False
+
+    # -------------------------------------------------------------------------
+    # OutTable subclassing
+    #    
+    def _export_delayed_rows(self):
+        if not self._delayed_rows and not self._headers:
+            print("no data", file=self._file)
+            return
+        
+        rows = [self._prepare_row(row) for row in self._delayed_rows]
+        if self._headers:
+            result = tabulate(rows, headers=self._headers)
+        else:
+            result = tabulate(rows)
+        print(result, file=self._file)
+
+
+    def _get_existing_headers(self) -> list[str]|None:
+        return []
+
+    def _export_new_headers(self, headers: list[str]):
+        pass
```

## zut/inout/__init__.py

```diff
@@ -1,130 +1,149 @@
-from __future__ import annotations
-from datetime import timezone
-
-import logging
-from io import IOBase
-from pathlib import Path
-import sys
-from typing import Any, Callable
-from ..misc import Literal
-
-from .OutFile import OutFile
-from .OutTable import OutTable
-from .OutTabulate import OutTabulate
-from .OutCsv import OutCsv
-from .OutExcel import OutExcel
-from .OutDb import OutDb
-from .InTable import InTable
-from .InDb import InDb
-from .InCsv import InCsv
-from .InExcel import InExcel
-from .utils import is_excel_path, split_excel_path
-
-logger = logging.getLogger(__name__)
-
-
-def out_file(out: str|Path|IOBase|Literal[False]|None = None, *, out_dir: str|Path|None = None, title: str|None = None, append: bool = False, encoding: str = 'utf-8-sig', newline: str = None, atexit: bool = False, format: OutFile = None, **kwargs) -> OutFile:
-    if not format:
-        format = OutFile
-    elif not isinstance(format, type) or not issubclass(format, OutFile):
-        raise ValueError(f"invalid format: {format}")
-    
-    return format(out, out_dir=out_dir, title=title, append=append, encoding=encoding, newline=newline, atexit=atexit, **kwargs)
-
-
-def out_table(out: Path|str|IOBase|Literal[False]|None = None, *, out_dir: str|Path|None = None, title: str|None = None, append: bool = False, encoding: str = 'utf-8-sig', headers: list[str] = None, delimiter: str = None, quotechar: str = '"', tz: Literal['local']|timezone = None, decimal_separator: str = None, atexit: bool = False, format: OutTable|Literal['csv','excel','tabulate'] = None, **kwargs) -> OutTable:
-    if not format:
-        if isinstance(out, str) and out.startswith('db:'):
-            format = OutDb
-        elif OutTabulate.is_available() and (out is None or out == 'stdout' or out == sys.stdout or out == 'stderr' or out == sys.stderr):
-            format = OutTabulate
-        elif isinstance(out, (str,Path)) and is_excel_path(out, accept_table_suffix=True):
-            format = OutExcel
-        else:
-            format = OutCsv
-    elif format == 'csv':
-        format = OutCsv
-    elif format == 'excel':
-        format = OutExcel
-    elif format == 'tabulate':
-        format = OutTabulate
-    elif not isinstance(format, type) or not issubclass(format, OutTable):
-        raise ValueError(f"invalid format: {format}")
-
-    return format(out, out_dir=out_dir, title=title, append=append, encoding=encoding, headers=headers, delimiter=delimiter, tz=tz, decimal_separator=decimal_separator, quotechar=quotechar, atexit=atexit, **kwargs)
-
-
-def in_table(src: Path|str|IOBase, query: str = None, *args, src_dir: str|Path|None = None, title: str|None = None, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"', formatters: dict[int|str,Callable] = None, debug: bool = False, limit: int = None, format: InTable|Literal['csv','excel'] = None, **kwargs) -> InTable:
-    if not format:
-        if isinstance(src, str) and src.startswith('db:'):
-            format = InDb
-        elif isinstance(src, (str,Path)) and is_excel_path(src, accept_table_suffix=True):
-            format = InExcel
-        else:
-            format = InCsv
-    elif format == 'csv':
-        format = InCsv
-    elif format == 'excel':
-        format = InExcel
-    elif not isinstance(format, type) or not issubclass(format, InTable):
-        raise ValueError(f"invalid format: {format}")
-    
-    return format(src, query=query, *args, src_dir=src_dir, title=title, encoding=encoding, delimiter=delimiter, quotechar=quotechar, formatters=formatters, debug=debug, limit=limit, **kwargs)
-
-
-def transfer_table(src: Path|str|IOBase, out: str|Path|IOBase, *, formatters: dict[int|str,Callable] = None, headers: list[str]|dict[str, Any] = None, src_dir: str|Path|None = None, out_dir: str|Path|None = None, append: bool = False, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"'):
-    """
-    - `formatters`: formatters to apply to source values. Keys are source headers.
-    - `headers`: target headers, or mapping (`dict`): keys are target headers, values are source headers (`str`) or value (`str` prefixed with `value:` or other types).
-    """
-    class RowIndex:
-        def __init__(self, index: int):
-            self.index = index
-
-        def __repr__(self):
-            return f"[{self.index}]"
-        
-
-    with in_table(src, src_dir=src_dir, formatters=formatters, encoding=encoding, delimiter=delimiter, quotechar=quotechar) as _src_table:
-        # Read/write headers and build mapping
-        if headers:
-            out_headers = []
-            row_transform_needed = False
-            out_row_model = []
-        
-            for target_header, spec in ([(header, header) for header in headers] if isinstance(headers, list) else headers.items()):
-                if isinstance(spec, str):
-                    row_transform_needed = True
-                    if spec.startswith('value:'):
-                        spec = spec[len('value:'):]
-                    else:
-                        try:
-                            index = _src_table.headers.index(spec)
-                        except ValueError:
-                            raise ValueError(f"header \"{spec}\" (for mapping to \"{target_header}\") not found in CSV file")
-                        spec = RowIndex(index)
-                else:
-                    spec = spec
-
-                out_headers.append(target_header)
-                out_row_model.append(spec)
-        else:
-            out_headers = _src_table.headers
-            row_transform_needed = False
-
-        # Read/write rows
-        with out_table(out, out_dir=out_dir, headers=out_headers, append=append, encoding=encoding, delimiter=delimiter, quotechar=quotechar) as _out_table:
-            for src_row in _src_table:
-                if row_transform_needed:
-                    out_row = []
-
-                    for spec in out_row_model:
-                        if isinstance(spec, RowIndex):
-                            value = src_row[spec.index]
-                        else:
-                            value = spec
-                        out_row.append(value)
-                else:
-                    out_row = src_row.values
-
-                _out_table.append(out_row)
+from __future__ import annotations
+from datetime import timezone
+
+import logging
+from io import IOBase
+from pathlib import Path
+import sys
+from typing import Any, Callable
+from ..misc import Literal
+from ..text import slugify
+from ..db import DbWrapper
+
+from .OutFile import OutFile
+from .OutTable import OutTable
+from .OutTabulate import OutTabulate
+from .OutCsv import OutCsv
+from .OutExcel import OutExcel
+from .OutDb import OutDb
+from .InTable import InTable
+from .InDb import InDb
+from .InCsv import InCsv
+from .InExcel import InExcel
+from .utils import is_excel_path, split_excel_path, normalize_inout
+
+logger = logging.getLogger(__name__)
+
+
+def out_file(out: str|Path|IOBase|Literal[False]|None = None, *, out_dir: str|Path|None = None, title: str|None = None, append: bool = False, encoding: str = 'utf-8-sig', newline: str = None, atexit: bool = False, format: OutFile = None, **kwargs) -> OutFile:
+    if not format:
+        format = OutFile
+    elif not isinstance(format, type) or not issubclass(format, OutFile):
+        raise ValueError(f"invalid format: {format}")
+    
+    return format(out, out_dir=out_dir, title=title, append=append, encoding=encoding, newline=newline, atexit=atexit, **kwargs)
+
+
+def out_table(out: Path|str|IOBase|Literal[False]|None = None, *, out_dir: str|Path|None = None, title: str|None = None, append: bool = False, encoding: str = 'utf-8-sig', headers: list[str] = None, delimiter: str = None, quotechar: str = '"', tz: Literal['local']|timezone = None, decimal_separator: str = None, atexit: bool = False, format: OutTable|Literal['csv','excel','tabulate'] = None, **kwargs) -> OutTable:
+    if not format:
+        if isinstance(out, str) and out.startswith('db:'):
+            format = OutDb
+        elif OutTabulate.is_available() and (out is None or out == 'stdout' or out == sys.stdout or out == 'stderr' or out == sys.stderr):
+            format = OutTabulate
+        elif isinstance(out, (str,Path)) and is_excel_path(out, accept_table_suffix=True):
+            format = OutExcel
+        else:
+            format = OutCsv
+    elif format == 'csv':
+        format = OutCsv
+    elif format == 'excel':
+        format = OutExcel
+    elif format == 'tabulate':
+        format = OutTabulate
+    elif not isinstance(format, type) or not issubclass(format, OutTable):
+        raise ValueError(f"invalid format: {format}")
+
+    return format(out, out_dir=out_dir, title=title, append=append, encoding=encoding, headers=headers, delimiter=delimiter, tz=tz, decimal_separator=decimal_separator, quotechar=quotechar, atexit=atexit, **kwargs)
+
+
+def in_table(src: Path|str|IOBase|DbWrapper, query: str = None, *args, src_dir: str|Path|None = None, title: str|None = None, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"', formatters: dict[int|str,Callable] = None, debug: bool = False, limit: int = None, format: InTable|Literal['csv','excel'] = None, **kwargs) -> InTable:
+    if not format:
+        if isinstance(src, DbWrapper) or (isinstance(src, str) and src.startswith('db:')):
+            format = InDb
+        elif isinstance(src, (str,Path)) and is_excel_path(src, accept_table_suffix=True):
+            format = InExcel
+        else:
+            format = InCsv
+    elif format == 'csv':
+        format = InCsv
+    elif format == 'excel':
+        format = InExcel
+    elif not isinstance(format, type) or not issubclass(format, InTable):
+        raise ValueError(f"invalid format: {format}")
+    
+    return format(src, query=query, *args, src_dir=src_dir, title=title, encoding=encoding, delimiter=delimiter, quotechar=quotechar, formatters=formatters, debug=debug, limit=limit, **kwargs)
+
+
+def transfer_table(src: Path|str|IOBase, out: str|Path|IOBase, *, formatters: dict[int|str,Callable] = None, headers: list[str]|dict[str, Any] = None, dir: str|Path|None = None, append: bool = False, encoding: str = 'utf-8', delimiter: str = None, quotechar: str = '"', **kwargs):
+    """
+    - `formatters`: formatters to apply to source values. Keys are source headers.
+    - `headers`: target headers, or mapping (`dict`): keys are target headers, values are source headers (`str`) or value (`str` prefixed with `value:` or other types). Use '*' as key and value to include non-mentionned source headers without modifications.
+    """
+    class RowIndex:
+        def __init__(self, index: int):
+            self.index = index
+
+        def __repr__(self):
+            return f"[{self.index}]"
+        
+
+    with in_table(src, src_dir=dir, formatters=formatters, encoding=encoding, delimiter=delimiter, quotechar=quotechar, **kwargs) as _src_table:
+        # Read/write headers and build mapping
+        if headers:
+            out_headers = []
+            row_transform_needed = True
+            out_row_model = []
+
+            default_spec = ('*' if '*' in headers else None) if isinstance(headers, list) else headers.pop('*', None)
+            if default_spec:
+                for src_index, src_header in enumerate(_src_table.headers):
+                    if default_spec == 'slugify':
+                        target_header = slugify(src_header)
+                    elif default_spec.startswith('slugify:'):
+                        target_header = slugify(src_header, separator=default_spec[len('slugify:'):])
+                    else:
+                        target_header = src_header
+
+                    out_headers.append(target_header)
+                    out_row_model.append(RowIndex(src_index))
+        
+
+            for target_header, spec in ([(header, header) for header in headers] if isinstance(headers, list) else headers.items()):
+                if isinstance(spec, str):
+                    if spec.startswith('value:'):
+                        spec = spec[len('value:'):]
+                    else:
+                        try:
+                            src_index = _src_table.headers.index(spec)
+                        except ValueError:
+                            raise ValueError(f"header \"{spec}\" (for mapping to \"{target_header}\") not found in CSV file")
+                        spec = RowIndex(src_index)
+                else:
+                    spec = spec
+
+                try:
+                    out_index = out_headers.index(target_header)
+                    out_row_model[out_index] = spec
+                except ValueError:
+                    out_headers.append(target_header)
+                    out_row_model.append(spec)
+        else:
+            out_headers = _src_table.headers
+            row_transform_needed = False
+
+        # Read/write rows
+        with out_table(out, out_dir=dir, headers=out_headers, append=append, encoding=encoding, delimiter=delimiter, quotechar=quotechar, **kwargs) as _out_table:
+            for src_row in _src_table:
+                if row_transform_needed:
+                    out_row = []
+
+                    for spec in out_row_model:
+                        if isinstance(spec, RowIndex):
+                            value = src_row[spec.index]
+                        else:
+                            value = spec
+                        out_row.append(value)
+                else:
+                    out_row = src_row.values
+
+                _out_table.append(out_row)
```

## zut/inout/utils.py

```diff
@@ -1,93 +1,93 @@
-from __future__ import annotations
-
-import os
-import re
-import sys
-from io import IOBase
-from pathlib import Path
-
-from ..misc import Literal, Protocol
-
-
-class Closable(Protocol):
-    def close(self):
-        ...
-
-
-def normalize_inout(inout: str|Path|IOBase|Literal[False]|None, directory: str|Path|None, **kwargs) -> str|IOBase:
-    if inout == 'stdout' or inout == sys.stdout or inout is None:
-        return sys.stdout
-    elif inout == 'stderr' or inout == sys.stderr:
-        return sys.stderr
-    elif inout == 'stdin' or inout == sys.stdin:
-        return sys.stdin
-    elif inout == False:
-        return os.devnull
-    elif isinstance(inout, IOBase):
-        return inout
-    elif isinstance(inout, (str,Path)):
-        inout = str(inout).format(**kwargs)
-        if directory and not ':' in inout and not inout.startswith('.'):
-            inout = os.path.join(str(directory), inout)
-        return os.path.expanduser(inout)
-    else:
-        raise ValueError(f'invalid inout type: {type(inout)}')
-
-
-def get_inout_name(inout: str|Path|IOBase|Literal[False]|None) -> str:
-    if inout == 'stdout' or inout == sys.stdout or inout is None:
-        return '<stdout>'
-    elif inout == 'stderr' or inout == sys.stderr:
-        return '<stderr>'
-    elif inout == 'stdin' or inout == sys.stdin:
-        return '<stdin>'
-    elif inout == False:
-        return '<devnull>'
-    elif isinstance(inout, IOBase):
-        return get_iobase_name(inout)
-    elif isinstance(inout, (str,Path)):
-        return inout
-    else:
-        raise ValueError(f'invalid inout type: {type(inout)}')
-
-
-def get_iobase_name(out: IOBase) -> str:
-    try:
-        name = out.name
-        if not name or not isinstance(name, str):
-            name = None
-    except AttributeError:
-        name = None
-
-    if name:
-        if name.startswith('<') and name.endswith('>'):
-            return name
-        else:
-            return f'<{name}>'
-
-    else:
-        return f"<{type(out).__name__}>"
-
-
-
-def is_excel_path(path: str|Path, accept_table_suffix = False):
-    if isinstance(path, Path):
-        path = str(path)
-    elif not isinstance(path, str):
-        raise ValueError(f'invalid path type: {type(path)}')
-    
-    return re.search(r'\.xlsx(?:#[^\.]+)?$' if accept_table_suffix else r'\.xlsx$', path, re.IGNORECASE)
-
-
-def split_excel_path(path: str|Path) -> tuple[Path,str]:
-    """ Return (actual path, table name) """
-    if isinstance(path, Path):
-        path = str(path)
-    elif not isinstance(path, str):
-        raise ValueError(f'invalid path type: {type(path)}')
-    
-    m = re.match(r'^(.+\.xlsx)(?:#([^\.]+))?$', path, re.IGNORECASE)
-    if not m:
-        return (Path(path), None)
-    
-    return (Path(m[1]), m[2] if m[2] else None)
+from __future__ import annotations
+
+import os
+import re
+import sys
+from io import IOBase
+from pathlib import Path
+
+from ..misc import Literal, Protocol
+
+
+class Closable(Protocol):
+    def close(self):
+        ...
+
+
+def normalize_inout(inout: str|Path|IOBase|Literal[False]|None, *, dir: str|Path|None, **kwargs) -> str|IOBase:
+    if inout == 'stdout' or inout == sys.stdout or inout is None:
+        return sys.stdout
+    elif inout == 'stderr' or inout == sys.stderr:
+        return sys.stderr
+    elif inout == 'stdin' or inout == sys.stdin:
+        return sys.stdin
+    elif inout == False:
+        return os.devnull
+    elif isinstance(inout, IOBase):
+        return inout
+    elif isinstance(inout, (str,Path)):
+        inout = str(inout).format(**kwargs)
+        if dir and not ':' in inout and not inout.startswith('.'):
+            inout = os.path.join(str(dir), inout)
+        return os.path.expanduser(inout)
+    else:
+        raise ValueError(f'invalid inout type: {type(inout)}')
+
+
+def get_inout_name(inout: str|Path|IOBase|Literal[False]|None) -> str:
+    if inout == 'stdout' or inout == sys.stdout or inout is None:
+        return '<stdout>'
+    elif inout == 'stderr' or inout == sys.stderr:
+        return '<stderr>'
+    elif inout == 'stdin' or inout == sys.stdin:
+        return '<stdin>'
+    elif inout == False:
+        return '<devnull>'
+    elif isinstance(inout, IOBase):
+        return get_iobase_name(inout)
+    elif isinstance(inout, (str,Path)):
+        return inout
+    else:
+        raise ValueError(f'invalid inout type: {type(inout)}')
+
+
+def get_iobase_name(out: IOBase) -> str:
+    try:
+        name = out.name
+        if not name or not isinstance(name, str):
+            name = None
+    except AttributeError:
+        name = None
+
+    if name:
+        if name.startswith('<') and name.endswith('>'):
+            return name
+        else:
+            return f'<{name}>'
+
+    else:
+        return f"<{type(out).__name__}>"
+
+
+
+def is_excel_path(path: str|Path, accept_table_suffix = False):
+    if isinstance(path, Path):
+        path = str(path)
+    elif not isinstance(path, str):
+        raise ValueError(f'invalid path type: {type(path)}')
+    
+    return re.search(r'\.xlsx(?:#[^\.]+)?$' if accept_table_suffix else r'\.xlsx$', path, re.IGNORECASE)
+
+
+def split_excel_path(path: str|Path) -> tuple[Path,str]:
+    """ Return (actual path, table name) """
+    if isinstance(path, Path):
+        path = str(path)
+    elif not isinstance(path, str):
+        raise ValueError(f'invalid path type: {type(path)}')
+    
+    m = re.match(r'^(.+\.xlsx)(?:#([^\.]+))?$', path, re.IGNORECASE)
+    if not m:
+        return (Path(path), None)
+    
+    return (Path(m[1]), m[2] if m[2] else None)
```

## zut/network/__init__.py

 * *Ordering differences only*

```diff
@@ -1,241 +1,241 @@
-from __future__ import annotations
-import logging, os, re, urllib.request, socket, base64
-from types import FunctionType
-from urllib.parse import unquote, urlparse
-from contextlib import closing
-from unittest import TestCase
-from ..colors import Colors
-from .commons import ProxyConfig, _proxyconfig, report_failure
-
-try:
-    from .winhttp import check_winhttp_connectivity
-    _with_winhttp = True
-except ImportError:
-    _with_winhttp = False
-
-try:
-    from ..credentials import get_password
-    _with_credentials = True
-except ImportError:
-    _with_credentials = False
-
-
-logger = logging.getLogger(__name__)
-
-IP_ADDRESS_PATTERN = re.compile(r"^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$")
-
-
-def check_socket(host, port, timeout=1):
-    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:
-        sock.settimeout(timeout)
-        try:
-            returncode = sock.connect_ex((host, port))
-            if returncode == 0:
-                return True
-            else:
-                logger.debug("socket connnect_ex returned %d", returncode)
-                return False
-        except Exception as e:
-            logger.debug("socket connnect_ex: %s", e)
-            return False
-
-
-class SimpleProxyHandler(urllib.request.BaseHandler):
-    # Inspired from urllib.request.ProxyHandler   
-    handler_order = 100 # Proxies must be in front
-
-    def __init__(self, config: ProxyConfig):
-        self.config = config
-
-    def finalize(self) -> str:
-        # Get proxy url (for logs)
-        proxy_logurl = self.config.get_proxy_url(include_password="*")
-        logger.debug(f"use proxy for urllib: {proxy_logurl}")
-
-        # Check proxy existency
-        if not check_socket(self.config.host, self.config.port):
-            logger.warning(f"cannot connect to proxy for urllib: {proxy_logurl}")
-
-        # Prepare usefull variables for add_header and set_proxy
-        self.hostport = self.config.hostport
-        self.scheme = self.config.scheme
-
-        if self.config.username:
-            if not self.config.password:
-                if self.config.password_func:
-                    raise ValueError(f"password function did not return any value for username: {self.config.username}")
-                else:
-                    raise ValueError(f"missing password for urllib proxy: {proxy_logurl}")
-            userpass = '%s:%s' % (self.config.username, self.config.password)
-            self.authorization = "Basic " + base64.b64encode(userpass.encode()).decode("ascii")
-        else:
-            self.authorization = None
-
-    def http_open(self, req):
-        if not req.host:
-            return None
-        
-        if urllib.request.proxy_bypass(req.host):
-            # NOTE: because Proxy-Authorization header is not encrypted, we must add it ONLY when we're actually talking to the proxy
-            return None
-
-        if not hasattr(self, "authorization"):
-            self.finalize()
-
-        if self.authorization:
-            req.add_header("Proxy-Authorization", self.authorization)
-        req.set_proxy(self.hostport, self.scheme)
-        return None
-    
-    def https_open(self, req):
-        return self.http_open(req)
-
-
-def configure_proxy(url: str = None, exclusions: str = None, password_func: FunctionType = None):
-    """
-    Configure proxy for urllib requests (and winhttp requests created with `zut.network.winhttp.create_winhttp_request`).
-    """
-    # Detect proxy URL
-    if url is None:
-        if "HTTP_PROXY" in os.environ:
-            url = os.environ["HTTP_PROXY"]
-        elif "http_proxy" in os.environ:
-            url = os.environ["http_proxy"]
-        elif "HTTPS_PROXY" in os.environ:
-            url = os.environ["HTTPS_PROXY"]
-        elif "https_proxy" in os.environ:
-            url = os.environ["https_proxy"]
-
-    # Detect proxy exclusions
-    if exclusions is None:
-        if "NO_PROXY" in os.environ:
-            exclusions = os.environ["NO_PROXY"]
-        elif "no_proxy" in os.environ:
-            exclusions = os.environ["no_proxy"]
-
-    # Update proxy configuration
-    _proxyconfig.update(url, exclusions=exclusions)
-    
-    # Detect and register proxy func
-    if _proxyconfig.username and not _proxyconfig._password:
-        if password_func is None:
-            service = os.environ.get("PROXY_PASSWORD_SERVICE", None)
-            if service:
-                if not _with_credentials:
-                    logger.error(f"cannot register PROXY_PASSWORD_SERVICE \"{service}\": zut[credentials] optional dependencies are not installed")
-                else:
-                    password_func = lambda username: get_password(service, username)
-                
-        if password_func:
-            _proxyconfig.set_password_func(password_func)
-
-    def _del_if_exists(data: dict, attr: str):
-        if attr in data:
-            del data[attr]
-
-    # Remove environment variables (would take precedence in some cases, which might cause issues: e.g. should contain password or not?)
-    _del_if_exists(os.environ, "http_proxy")
-    _del_if_exists(os.environ, "https_proxy")
-    _del_if_exists(os.environ, "HTTP_PROXY")
-    _del_if_exists(os.environ, "HTTPS_PROXY")
-
-    # Ensure no_proxy is specified (must be passed as environment variable for urllib)
-    env_exclusions = _proxyconfig.env_exclusions_str
-    if env_exclusions:
-        os.environ["no_proxy"] = env_exclusions
-        os.environ["NO_PROXY"] = env_exclusions
-    else:
-        _del_if_exists(os.environ, "no_proxy")
-        _del_if_exists(os.environ, "NO_PROXY")
-
-    # Stop if no proxy specified/detected
-    if not _proxyconfig.host:
-        return
-    
-    # Register proxy for urllib
-    try:
-        handler = SimpleProxyHandler(_proxyconfig)
-        opener = urllib.request.build_opener(handler)
-        urllib.request.install_opener(opener)
-    except Exception as e:
-        logger.error("cannot register proxy for urllib: %s", e)
-
-
-def get_configured_proxy_url(for_url: str = None, include_password: bool = False) -> str:
-    """
-    Return currently configured proxy URL.
-    If `for_url` is given, return None if the proxy is excluded for this url.
-    """
-    if for_url:
-        o = urlparse(for_url)
-        if urllib.request.proxy_bypass(o.netloc if o.netloc else o.path):
-            return None
-    return _proxyconfig.get_proxy_url(include_password=include_password)
-
-
-def get_configured_proxy_hostport(for_url: str = None, include_password: bool = False) -> tuple[str,int]:
-    """
-    Return currently configured proxy host and port.
-    If `for_url` is given, return None if the proxy is excluded for this url.
-    """
-    if for_url:
-        o = urlparse(for_url)
-        if urllib.request.proxy_bypass(o.netloc if o.netloc else o.path):
-            return None
-    return _proxyconfig.host, _proxyconfig.port
-
-
-def get_configured_proxy_exclusions() -> list[str]:
-    """
-    Return currently configured proxy exclusions.
-    """
-    return _proxyconfig.exclusions
-
-
-def check_urllib_connectivity(url: str, expected_regex: re.Pattern|str, label=None, timeout: float = None, case: TestCase = None) -> bool:
-    """
-    Check network connectivity using urllib library.
-    - `timeout`: in seconds (defaults to 3 seconds).
-
-    Return `True` on success, `False` on failure.
-    """        
-    if not isinstance(expected_regex, re.Pattern):
-        expected_regex = re.compile(expected_regex)
-
-    if label is None:
-        label = url
-
-    if not timeout:
-        timeout = float(os.environ.get("CHECK_CONNECTIVITY_TIMEOUT", 10))
-
-    msg=f"{label} urllib"
-
-    try:
-        res = urllib.request.urlopen(url, timeout=timeout)
-        text = res.read().decode('utf-8')
-        text_startup = text[0:20].replace('\n', '\\n').replace('\r', '\\r') + ('' if len(text) > 20 else '')
-        
-        if not expected_regex.match(text):
-            report_failure(case, f"{msg}: response ({Colors.RED}{text_startup}{Colors.RESET}) does not match expected regex ({Colors.GRAY}{expected_regex.pattern}{Colors.RESET}")
-            return False
-
-        logger.info(f"{msg}: {Colors.GREEN}success{Colors.RESET} (response: {Colors.GRAY}{text_startup}{Colors.RESET})")
-        return True
-    except urllib.error.URLError as e:
-        report_failure(case, f"{msg}: {Colors.RED}{e.reason}{Colors.RESET}")
-        return False
-
-
-def check_connectivity(url: str, expected_regex: re.Pattern|str, label=None, timeout: float = None, case: TestCase = None) -> bool:
-    """
-    Check network connectivity with urllib library, and winhttp library on Windows (if [winhttp] extra dependencies are installed).
-    - `timeout`: in seconds (defaults to 3 seconds).
-
-    Return `True` on success, `False` on failure.
-    """ 
-    ok = check_urllib_connectivity(url, expected_regex, label=label, timeout=timeout, case=case)
-
-    if _with_winhttp:
-        ok = ok and check_winhttp_connectivity(url, expected_regex, label=label, timeout=timeout, case=case)
-
-    return ok
+from __future__ import annotations
+import logging, os, re, urllib.request, socket, base64
+from types import FunctionType
+from urllib.parse import unquote, urlparse
+from contextlib import closing
+from unittest import TestCase
+from ..colors import Colors
+from .commons import ProxyConfig, _proxyconfig, report_failure
+
+try:
+    from .winhttp import check_winhttp_connectivity
+    _with_winhttp = True
+except ImportError:
+    _with_winhttp = False
+
+try:
+    from ..credentials import get_password
+    _with_credentials = True
+except ImportError:
+    _with_credentials = False
+
+
+logger = logging.getLogger(__name__)
+
+IP_ADDRESS_PATTERN = re.compile(r"^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$")
+
+
+def check_socket(host, port, timeout=1):
+    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:
+        sock.settimeout(timeout)
+        try:
+            returncode = sock.connect_ex((host, port))
+            if returncode == 0:
+                return True
+            else:
+                logger.debug("socket connnect_ex returned %d", returncode)
+                return False
+        except Exception as e:
+            logger.debug("socket connnect_ex: %s", e)
+            return False
+
+
+class SimpleProxyHandler(urllib.request.BaseHandler):
+    # Inspired from urllib.request.ProxyHandler   
+    handler_order = 100 # Proxies must be in front
+
+    def __init__(self, config: ProxyConfig):
+        self.config = config
+
+    def finalize(self) -> str:
+        # Get proxy url (for logs)
+        proxy_logurl = self.config.get_proxy_url(include_password="*")
+        logger.debug(f"use proxy for urllib: {proxy_logurl}")
+
+        # Check proxy existency
+        if not check_socket(self.config.host, self.config.port):
+            logger.warning(f"cannot connect to proxy for urllib: {proxy_logurl}")
+
+        # Prepare usefull variables for add_header and set_proxy
+        self.hostport = self.config.hostport
+        self.scheme = self.config.scheme
+
+        if self.config.username:
+            if not self.config.password:
+                if self.config.password_func:
+                    raise ValueError(f"password function did not return any value for username: {self.config.username}")
+                else:
+                    raise ValueError(f"missing password for urllib proxy: {proxy_logurl}")
+            userpass = '%s:%s' % (self.config.username, self.config.password)
+            self.authorization = "Basic " + base64.b64encode(userpass.encode()).decode("ascii")
+        else:
+            self.authorization = None
+
+    def http_open(self, req):
+        if not req.host:
+            return None
+        
+        if urllib.request.proxy_bypass(req.host):
+            # NOTE: because Proxy-Authorization header is not encrypted, we must add it ONLY when we're actually talking to the proxy
+            return None
+
+        if not hasattr(self, "authorization"):
+            self.finalize()
+
+        if self.authorization:
+            req.add_header("Proxy-Authorization", self.authorization)
+        req.set_proxy(self.hostport, self.scheme)
+        return None
+    
+    def https_open(self, req):
+        return self.http_open(req)
+
+
+def configure_proxy(url: str = None, exclusions: str = None, password_func: FunctionType = None):
+    """
+    Configure proxy for urllib requests (and winhttp requests created with `zut.network.winhttp.create_winhttp_request`).
+    """
+    # Detect proxy URL
+    if url is None:
+        if "HTTP_PROXY" in os.environ:
+            url = os.environ["HTTP_PROXY"]
+        elif "http_proxy" in os.environ:
+            url = os.environ["http_proxy"]
+        elif "HTTPS_PROXY" in os.environ:
+            url = os.environ["HTTPS_PROXY"]
+        elif "https_proxy" in os.environ:
+            url = os.environ["https_proxy"]
+
+    # Detect proxy exclusions
+    if exclusions is None:
+        if "NO_PROXY" in os.environ:
+            exclusions = os.environ["NO_PROXY"]
+        elif "no_proxy" in os.environ:
+            exclusions = os.environ["no_proxy"]
+
+    # Update proxy configuration
+    _proxyconfig.update(url, exclusions=exclusions)
+    
+    # Detect and register proxy func
+    if _proxyconfig.username and not _proxyconfig._password:
+        if password_func is None:
+            service = os.environ.get("PROXY_PASSWORD_SERVICE", None)
+            if service:
+                if not _with_credentials:
+                    logger.error(f"cannot register PROXY_PASSWORD_SERVICE \"{service}\": zut[credentials] optional dependencies are not installed")
+                else:
+                    password_func = lambda username: get_password(service, username)
+                
+        if password_func:
+            _proxyconfig.set_password_func(password_func)
+
+    def _del_if_exists(data: dict, attr: str):
+        if attr in data:
+            del data[attr]
+
+    # Remove environment variables (would take precedence in some cases, which might cause issues: e.g. should contain password or not?)
+    _del_if_exists(os.environ, "http_proxy")
+    _del_if_exists(os.environ, "https_proxy")
+    _del_if_exists(os.environ, "HTTP_PROXY")
+    _del_if_exists(os.environ, "HTTPS_PROXY")
+
+    # Ensure no_proxy is specified (must be passed as environment variable for urllib)
+    env_exclusions = _proxyconfig.env_exclusions_str
+    if env_exclusions:
+        os.environ["no_proxy"] = env_exclusions
+        os.environ["NO_PROXY"] = env_exclusions
+    else:
+        _del_if_exists(os.environ, "no_proxy")
+        _del_if_exists(os.environ, "NO_PROXY")
+
+    # Stop if no proxy specified/detected
+    if not _proxyconfig.host:
+        return
+    
+    # Register proxy for urllib
+    try:
+        handler = SimpleProxyHandler(_proxyconfig)
+        opener = urllib.request.build_opener(handler)
+        urllib.request.install_opener(opener)
+    except Exception as e:
+        logger.error("cannot register proxy for urllib: %s", e)
+
+
+def get_configured_proxy_url(for_url: str = None, include_password: bool = False) -> str:
+    """
+    Return currently configured proxy URL.
+    If `for_url` is given, return None if the proxy is excluded for this url.
+    """
+    if for_url:
+        o = urlparse(for_url)
+        if urllib.request.proxy_bypass(o.netloc if o.netloc else o.path):
+            return None
+    return _proxyconfig.get_proxy_url(include_password=include_password)
+
+
+def get_configured_proxy_hostport(for_url: str = None, include_password: bool = False) -> tuple[str,int]:
+    """
+    Return currently configured proxy host and port.
+    If `for_url` is given, return None if the proxy is excluded for this url.
+    """
+    if for_url:
+        o = urlparse(for_url)
+        if urllib.request.proxy_bypass(o.netloc if o.netloc else o.path):
+            return None
+    return _proxyconfig.host, _proxyconfig.port
+
+
+def get_configured_proxy_exclusions() -> list[str]:
+    """
+    Return currently configured proxy exclusions.
+    """
+    return _proxyconfig.exclusions
+
+
+def check_urllib_connectivity(url: str, expected_regex: re.Pattern|str, label=None, timeout: float = None, case: TestCase = None) -> bool:
+    """
+    Check network connectivity using urllib library.
+    - `timeout`: in seconds (defaults to 3 seconds).
+
+    Return `True` on success, `False` on failure.
+    """        
+    if not isinstance(expected_regex, re.Pattern):
+        expected_regex = re.compile(expected_regex)
+
+    if label is None:
+        label = url
+
+    if not timeout:
+        timeout = float(os.environ.get("CHECK_CONNECTIVITY_TIMEOUT", 10))
+
+    msg=f"{label} urllib"
+
+    try:
+        res = urllib.request.urlopen(url, timeout=timeout)
+        text = res.read().decode('utf-8')
+        text_startup = text[0:20].replace('\n', '\\n').replace('\r', '\\r') + ('' if len(text) > 20 else '')
+        
+        if not expected_regex.match(text):
+            report_failure(case, f"{msg}: response ({Colors.RED}{text_startup}{Colors.RESET}) does not match expected regex ({Colors.GRAY}{expected_regex.pattern}{Colors.RESET}")
+            return False
+
+        logger.info(f"{msg}: {Colors.GREEN}success{Colors.RESET} (response: {Colors.GRAY}{text_startup}{Colors.RESET})")
+        return True
+    except urllib.error.URLError as e:
+        report_failure(case, f"{msg}: {Colors.RED}{e.reason}{Colors.RESET}")
+        return False
+
+
+def check_connectivity(url: str, expected_regex: re.Pattern|str, label=None, timeout: float = None, case: TestCase = None) -> bool:
+    """
+    Check network connectivity with urllib library, and winhttp library on Windows (if [winhttp] extra dependencies are installed).
+    - `timeout`: in seconds (defaults to 3 seconds).
+
+    Return `True` on success, `False` on failure.
+    """ 
+    ok = check_urllib_connectivity(url, expected_regex, label=label, timeout=timeout, case=case)
+
+    if _with_winhttp:
+        ok = ok and check_winhttp_connectivity(url, expected_regex, label=label, timeout=timeout, case=case)
+
+    return ok
```

## zut/network/commons.py

 * *Ordering differences only*

```diff
@@ -1,121 +1,121 @@
-from __future__ import annotations
-import re, logging
-from types import FunctionType
-from urllib.parse import quote, unquote, urlparse
-from unittest import TestCase
-
-logger = logging.getLogger(__name__)
-
-
-def report_failure(case: TestCase, msg: str):
-    if case:
-        case.fail(msg)
-    else:
-        logger.error(msg)
-
-
-class ProxyConfig:
-    def __init__(self):
-        self.clear()
-
-    def clear(self):
-        self.host: str = None
-        self.port: int = None
-        self.username: str = None
-        self.password_func: FunctionType = None
-        self.scheme: str = "http"
-        self.exclusions: list[str] = None
-
-        # caches
-        self._non_cidr_exclusions_str = '__undefined__'
-        self._password: str = None
-
-    def update(self, url: str, exclusions: str|list[str] = None):
-        self.clear()
-        if url is None:
-            return
-
-        o = urlparse(url)
-        m = re.match(r"^(?:(?P<username>[^\:]+)(?:\:(?P<password>[^\:]+))?@)?(?P<host>[^@\:]+)\:(?P<port>\d+)$", o.netloc)
-        if not m:
-            raise ValueError("invalid url netloc \"%s\"" % o.netloc)
-
-        self.host = unquote(m.group("host")) if m.group("host") else None
-        self.port = int(m.group("port"))
-        self.username = unquote(m.group("username")) if m.group("username") else None
-        self._password = unquote(m.group("password")) if m.group("password") else None
-        self.scheme = o.scheme
-
-        if exclusions is not None:
-            if isinstance(exclusions, str):
-                self.exclusions = exclusions.split(',')
-            else:
-                self.exclusions = exclusions
-
-    def set_password_func(self, func: FunctionType):
-        self.password_func = func
-
-    @property
-    def is_rtm_host(self):
-        return self.host.endswith(('.rtm.fr','.rtm.lan'))
-
-    @property
-    def hostport(self):
-        if not self.host:
-            return None
-        return f"{self.host}:{self.port}"
-
-    @property
-    def env_exclusions_str(self) -> str:
-        """
-        Return a comma-separated string of exclusions, for use as an environment variable.
-        """
-        if not self.exclusions:
-            return None
-        return ",".join(self.exclusions)
-
-    @property
-    def non_cidr_exclusions_str(self) -> str:
-        """
-        Return a comma-separated string of exclusions, excluding CIDR blocks (not supported by WinHTTP).
-        """
-        if self._non_cidr_exclusions_str == '__undefined__':
-            if self.exclusions:
-                self._non_cidr_exclusions_str = ",".join([exclusion for exclusion in self.exclusions if not "/" in exclusion])
-            else:
-                logger.warning("no exclusions for proxy: missing NO_PROXY environment variable?")
-                self._non_cidr_exclusions_str = "localhost"
-
-        return self._non_cidr_exclusions_str
-
-    @property
-    def password(self):
-        if self._password is None:
-            if self.username and self.password_func:
-                self._password = self.password_func(self.username)
-
-        return self._password
-
-    def get_proxy_url(self, include_password=False):
-        if self.host is None:
-            return None
-        
-        proxy_url = self.scheme + "://"
-
-        if self.username:
-            proxy_url += quote(self.username)
-
-            if include_password:
-                if self.password:
-                    if include_password == "*":
-                        proxy_url += ":" + ("*" * len(self.password))
-                    else:
-                        proxy_url += ":" + quote(self.password)
-            
-            proxy_url += "@"
-
-        proxy_url += f"{quote(self.host)}:{self.port}"
-        return proxy_url
-
-
-_proxyconfig = ProxyConfig()
+from __future__ import annotations
+import re, logging
+from types import FunctionType
+from urllib.parse import quote, unquote, urlparse
+from unittest import TestCase
+
+logger = logging.getLogger(__name__)
+
+
+def report_failure(case: TestCase, msg: str):
+    if case:
+        case.fail(msg)
+    else:
+        logger.error(msg)
+
+
+class ProxyConfig:
+    def __init__(self):
+        self.clear()
+
+    def clear(self):
+        self.host: str = None
+        self.port: int = None
+        self.username: str = None
+        self.password_func: FunctionType = None
+        self.scheme: str = "http"
+        self.exclusions: list[str] = None
+
+        # caches
+        self._non_cidr_exclusions_str = '__undefined__'
+        self._password: str = None
+
+    def update(self, url: str, exclusions: str|list[str] = None):
+        self.clear()
+        if url is None:
+            return
+
+        o = urlparse(url)
+        m = re.match(r"^(?:(?P<username>[^\:]+)(?:\:(?P<password>[^\:]+))?@)?(?P<host>[^@\:]+)\:(?P<port>\d+)$", o.netloc)
+        if not m:
+            raise ValueError("invalid url netloc \"%s\"" % o.netloc)
+
+        self.host = unquote(m.group("host")) if m.group("host") else None
+        self.port = int(m.group("port"))
+        self.username = unquote(m.group("username")) if m.group("username") else None
+        self._password = unquote(m.group("password")) if m.group("password") else None
+        self.scheme = o.scheme
+
+        if exclusions is not None:
+            if isinstance(exclusions, str):
+                self.exclusions = exclusions.split(',')
+            else:
+                self.exclusions = exclusions
+
+    def set_password_func(self, func: FunctionType):
+        self.password_func = func
+
+    @property
+    def is_rtm_host(self):
+        return self.host.endswith(('.rtm.fr','.rtm.lan'))
+
+    @property
+    def hostport(self):
+        if not self.host:
+            return None
+        return f"{self.host}:{self.port}"
+
+    @property
+    def env_exclusions_str(self) -> str:
+        """
+        Return a comma-separated string of exclusions, for use as an environment variable.
+        """
+        if not self.exclusions:
+            return None
+        return ",".join(self.exclusions)
+
+    @property
+    def non_cidr_exclusions_str(self) -> str:
+        """
+        Return a comma-separated string of exclusions, excluding CIDR blocks (not supported by WinHTTP).
+        """
+        if self._non_cidr_exclusions_str == '__undefined__':
+            if self.exclusions:
+                self._non_cidr_exclusions_str = ",".join([exclusion for exclusion in self.exclusions if not "/" in exclusion])
+            else:
+                logger.warning("no exclusions for proxy: missing NO_PROXY environment variable?")
+                self._non_cidr_exclusions_str = "localhost"
+
+        return self._non_cidr_exclusions_str
+
+    @property
+    def password(self):
+        if self._password is None:
+            if self.username and self.password_func:
+                self._password = self.password_func(self.username)
+
+        return self._password
+
+    def get_proxy_url(self, include_password=False):
+        if self.host is None:
+            return None
+        
+        proxy_url = self.scheme + "://"
+
+        if self.username:
+            proxy_url += quote(self.username)
+
+            if include_password:
+                if self.password:
+                    if include_password == "*":
+                        proxy_url += ":" + ("*" * len(self.password))
+                    else:
+                        proxy_url += ":" + quote(self.password)
+            
+            proxy_url += "@"
+
+        proxy_url += f"{quote(self.host)}:{self.port}"
+        return proxy_url
+
+
+_proxyconfig = ProxyConfig()
```

## zut/network/winhttp.py

 * *Ordering differences only*

```diff
@@ -1,75 +1,75 @@
-from __future__ import annotations
-from unittest import TestCase
-import re, os, logging, win32com.client, win32inetcon, pywintypes
-from ..format import RED, GREEN, GRAY
-from .commons import _proxyconfig, report_failure
-
-logger = logging.getLogger(__name__)
-
-
-def create_winhttp_request(timeout: float = None):
-    """
-    Create a winhttp request.
-    - `timeout`: in seconds.
-    """
-    winhttp_req = win32com.client.Dispatch('WinHTTP.WinHTTPRequest.5.1')
-
-    if timeout:
-        winhttp_req.SetTimeouts(int(timeout*1000), int(timeout*1000), int(timeout*1000), int(timeout*1000))
-
-    if _proxyconfig.hostport:
-        # See: https://docs.microsoft.com/en-us/windows/win32/winhttp/iwinhttprequest-setproxy
-        # NOTE: no need to pass credentials, this is handled directly by Windows
-        HTTPREQUEST_PROXYSETTING_DEFAULT   = 0
-        HTTPREQUEST_PROXYSETTING_PRECONFIG = 0
-        HTTPREQUEST_PROXYSETTING_DIRECT    = 1
-        HTTPREQUEST_PROXYSETTING_PROXY     = 2
-        winhttp_req.SetProxy(HTTPREQUEST_PROXYSETTING_PROXY, _proxyconfig.hostport, _proxyconfig.non_cidr_exclusions_str)
-        winhttp_req.SetAutoLogonPolicy(HTTPREQUEST_PROXYSETTING_DEFAULT)
-
-    return winhttp_req
-
-
-def check_winhttp_connectivity(url: str, expected_regex: re.Pattern|str, label=None, timeout: float = None, case: TestCase = None) -> bool:
-    """
-    Check network connectivity using winhttp library.
-    - `timeout`: in seconds (defaults to 3 seconds).
-    
-    Return `True` on success, `False` on failure.
-    """        
-    if not isinstance(expected_regex, re.Pattern):
-        expected_regex = re.compile(expected_regex)
-
-    if label is None:
-        label = url
-
-    if not timeout:
-        timeout = float(os.environ.get("CHECK_CONNECTIVITY_TIMEOUT", 10))
-
-    msg=f"{label} winhttp"
-
-    winhttp_req = create_winhttp_request(timeout=timeout)
-    winhttp_req.Open('GET', url, False)
-    try:
-        winhttp_req.Send()
-        winhttp_req.WaitForResponse()
-        if winhttp_req.Status != 200:
-            report_failure(case, f"{msg}: {winhttp_req.Status} {winhttp_req.StatusText}")
-            return False
-
-        text = winhttp_req.ResponseText
-        text_startup = text[0:20].replace('\n', '\\n').replace('\r', '\\r') + ('' if len(text) > 20 else '')
-
-        if not expected_regex.match(text):
-            report_failure(case, f"{msg}: response ({RED % text_startup}) does not match expected regex ({GRAY % expected_regex.pattern})")
-            return False
-        
-        logger.info(f"{msg}: {GREEN % 'success'} (response: {GRAY % text_startup})")
-        return True
-    except pywintypes.com_error as e:
-        if e.excepinfo[5] + 2**32 & 0xffff == win32inetcon.ERROR_INTERNET_TIMEOUT:
-            details = "timed out"
-        else:
-            details = e.excepinfo[2].strip()
-        report_failure(case, f"{msg}: {RED % details}")
-        return False
+from __future__ import annotations
+from unittest import TestCase
+import re, os, logging, win32com.client, win32inetcon, pywintypes
+from ..format import RED, GREEN, GRAY
+from .commons import _proxyconfig, report_failure
+
+logger = logging.getLogger(__name__)
+
+
+def create_winhttp_request(timeout: float = None):
+    """
+    Create a winhttp request.
+    - `timeout`: in seconds.
+    """
+    winhttp_req = win32com.client.Dispatch('WinHTTP.WinHTTPRequest.5.1')
+
+    if timeout:
+        winhttp_req.SetTimeouts(int(timeout*1000), int(timeout*1000), int(timeout*1000), int(timeout*1000))
+
+    if _proxyconfig.hostport:
+        # See: https://docs.microsoft.com/en-us/windows/win32/winhttp/iwinhttprequest-setproxy
+        # NOTE: no need to pass credentials, this is handled directly by Windows
+        HTTPREQUEST_PROXYSETTING_DEFAULT   = 0
+        HTTPREQUEST_PROXYSETTING_PRECONFIG = 0
+        HTTPREQUEST_PROXYSETTING_DIRECT    = 1
+        HTTPREQUEST_PROXYSETTING_PROXY     = 2
+        winhttp_req.SetProxy(HTTPREQUEST_PROXYSETTING_PROXY, _proxyconfig.hostport, _proxyconfig.non_cidr_exclusions_str)
+        winhttp_req.SetAutoLogonPolicy(HTTPREQUEST_PROXYSETTING_DEFAULT)
+
+    return winhttp_req
+
+
+def check_winhttp_connectivity(url: str, expected_regex: re.Pattern|str, label=None, timeout: float = None, case: TestCase = None) -> bool:
+    """
+    Check network connectivity using winhttp library.
+    - `timeout`: in seconds (defaults to 3 seconds).
+    
+    Return `True` on success, `False` on failure.
+    """        
+    if not isinstance(expected_regex, re.Pattern):
+        expected_regex = re.compile(expected_regex)
+
+    if label is None:
+        label = url
+
+    if not timeout:
+        timeout = float(os.environ.get("CHECK_CONNECTIVITY_TIMEOUT", 10))
+
+    msg=f"{label} winhttp"
+
+    winhttp_req = create_winhttp_request(timeout=timeout)
+    winhttp_req.Open('GET', url, False)
+    try:
+        winhttp_req.Send()
+        winhttp_req.WaitForResponse()
+        if winhttp_req.Status != 200:
+            report_failure(case, f"{msg}: {winhttp_req.Status} {winhttp_req.StatusText}")
+            return False
+
+        text = winhttp_req.ResponseText
+        text_startup = text[0:20].replace('\n', '\\n').replace('\r', '\\r') + ('' if len(text) > 20 else '')
+
+        if not expected_regex.match(text):
+            report_failure(case, f"{msg}: response ({RED % text_startup}) does not match expected regex ({GRAY % expected_regex.pattern})")
+            return False
+        
+        logger.info(f"{msg}: {GREEN % 'success'} (response: {GRAY % text_startup})")
+        return True
+    except pywintypes.com_error as e:
+        if e.excepinfo[5] + 2**32 & 0xffff == win32inetcon.ERROR_INTERNET_TIMEOUT:
+            details = "timed out"
+        else:
+            details = e.excepinfo[2].strip()
+        report_failure(case, f"{msg}: {RED % details}")
+        return False
```

## Comparing `zut-0.6.0.dist-info/LICENSE.txt` & `zut-0.6.1.dist-info/LICENSE.txt`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-Copyright (C) 2022 Ipamo
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+Copyright (C) 2022 Ipamo
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 SOFTWARE.
```

## Comparing `zut-0.6.0.dist-info/METADATA` & `zut-0.6.1.dist-info/METADATA`

 * *Files 17% similar despite different names*

```diff
@@ -1,147 +1,147 @@
-Metadata-Version: 2.1
-Name: zut
-Version: 0.6.0
-Summary: Reusable Python, Django and PostgreSql utilities.
-Author-email: Ipamo <dev@ipamo.net>
-Project-URL: Homepage, https://github.com/ipamo/zut
-Project-URL: Bug Tracker, https://github.com/ipamo/zut/issues
-Keywords: reusable,util,utils,common,commons,flexout,csv,excel,tabulate,smb,samba,share
-Classifier: Development Status :: 4 - Beta
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Operating System :: OS Independent
-Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
-Classifier: Topic :: Software Development :: Libraries
-Requires-Python: >=3.7.3
-Description-Content-Type: text/markdown
-License-File: LICENSE.txt
-Requires-Dist: typing-extensions ; python_version < "3.11"
-Provides-Extra: all
-Requires-Dist: cmarkgfm ; extra == 'all'
-Requires-Dist: django ; extra == 'all'
-Requires-Dist: djangorestframework ; extra == 'all'
-Requires-Dist: openpyxl ; extra == 'all'
-Requires-Dist: defusedxml ; extra == 'all'
-Requires-Dist: pyodbc ; extra == 'all'
-Requires-Dist: sqlparams ; extra == 'all'
-Requires-Dist: sqlparse ; extra == 'all'
-Requires-Dist: psycopg2-binary ; extra == 'all'
-Requires-Dist: smbprotocol ; extra == 'all'
-Requires-Dist: tabulate ; extra == 'all'
-Requires-Dist: keyring ; (sys_platform == "win32") and extra == 'all'
-Requires-Dist: pywin32 ; (sys_platform == "win32") and extra == 'all'
-Provides-Extra: credentials
-Requires-Dist: keyring ; (sys_platform == "win32") and extra == 'credentials'
-Requires-Dist: pywin32 ; (sys_platform == "win32") and extra == 'credentials'
-Provides-Extra: django
-Requires-Dist: cmarkgfm ; extra == 'django'
-Requires-Dist: django ; extra == 'django'
-Requires-Dist: djangorestframework ; extra == 'django'
-Provides-Extra: excel
-Requires-Dist: openpyxl ; extra == 'excel'
-Requires-Dist: defusedxml ; extra == 'excel'
-Provides-Extra: mssql
-Requires-Dist: pyodbc ; extra == 'mssql'
-Requires-Dist: sqlparams ; extra == 'mssql'
-Requires-Dist: sqlparse ; extra == 'mssql'
-Provides-Extra: pg
-Requires-Dist: psycopg2-binary ; extra == 'pg'
-Requires-Dist: sqlparse ; extra == 'pg'
-Requires-Dist: sqlparams ; extra == 'pg'
-Provides-Extra: smb
-Requires-Dist: smbprotocol ; extra == 'smb'
-Provides-Extra: tabulate
-Requires-Dist: tabulate ; extra == 'tabulate'
-Provides-Extra: winhttp
-Requires-Dist: pywin32 ; (sys_platform == "win32") and extra == 'winhttp'
-
-Zut
-===
-
-Reusable Python, Django and PostgreSql utilities.
-
-## Installation
-
-From PyPI, with all optional dependencies:
-
-    pip install zut[all]
-
-From a Git branch or tag (using https or ssh):
-
-    pip install git+https://gitlab.com/ipamo/zut.git@main#egg=zut[all]
-    pip install git+ssh://git@gitlab.com/ipamo/zut.git@main#egg=zut[all]
-
-Note: optionaly dependencies may also be chosen individually. For example, use `zut[excel,pg]` instead of `zut[all]`.
-
-
-## Usage examples
-
-### Configure logging
-
-```py
-from zut import configure_logging
-configure_logging()
-```
-
-### Flexible in/out - TODO
-
-Write text or tabular data to a flexible, easily configurable output: CSV or Excel file, or tabulated stdout/stderr.
-
-The output file may be on the local file system or on a Windows/Samba share (including when the library is used on Linux).
-
-Export text to stdout or to a file:
-
-```py
-import sys
-from zut import out_file
-
-with out_file(filename or sys.stdout) as f:
-    f.write("Content")
-```
-    
-Export tabular data to stdout or to a file:
-
-```py
-import sys
-from zut import out_table
-
-with out_table(filename or sys.stdout, headers=["Id", "Word"]) as t:
-    t.append([1, "Hello"])
-    t.append([2, "World"])
-```
-
-Tabular data can also be exported using dictionnaries (in this case, headers will be detected automatically by the library):
-
-```py
-import sys
-from zut import out_table
-
-with out_table(filename or sys.stdout) as t:
-    t.append({'id': 1, 'name': "Hello"})
-    t.append({'id': 2, 'col3': True})
-```
-
-If `filename` has extension with `.xlsx`, output will be in Excel 2010 format.
-Otherwise it will be in CSV format.
-
-If `filename` starts with `\\`, output will be done on the corresponding Windows/Samba share.
-To indicate Samba credentials, call `configure_smb_credentials` before using function `out_table`.
-Example:
-
-```py
-from zut import out_table, configure_smb_credentials
-
-configure_smb_credentials(user=..., password=...)
-
-with out_table(r"\\server\share\path\to\file") as o:
-    ...
-```
-
-
-## Resources
-
-- [Maintenance of the library and repository](doc/maintain.md)
+Metadata-Version: 2.1
+Name: zut
+Version: 0.6.1
+Summary: Reusable Python, Django and PostgreSql utilities.
+Author-email: Ipamo <dev@ipamo.net>
+Project-URL: Homepage, https://github.com/ipamo/zut
+Project-URL: Bug Tracker, https://github.com/ipamo/zut/issues
+Keywords: reusable,util,utils,common,commons,flexout,csv,excel,tabulate,smb,samba,share
+Classifier: Development Status :: 4 - Beta
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Operating System :: OS Independent
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Classifier: Topic :: Software Development :: Libraries
+Requires-Python: >=3.7.3
+Description-Content-Type: text/markdown
+License-File: LICENSE.txt
+Requires-Dist: typing-extensions ; python_version < "3.11"
+Provides-Extra: all
+Requires-Dist: cmarkgfm ; extra == 'all'
+Requires-Dist: django ; extra == 'all'
+Requires-Dist: djangorestframework ; extra == 'all'
+Requires-Dist: openpyxl ; extra == 'all'
+Requires-Dist: defusedxml ; extra == 'all'
+Requires-Dist: pyodbc ; extra == 'all'
+Requires-Dist: sqlparams ; extra == 'all'
+Requires-Dist: sqlparse ; extra == 'all'
+Requires-Dist: psycopg2-binary ; extra == 'all'
+Requires-Dist: smbprotocol ; extra == 'all'
+Requires-Dist: tabulate ; extra == 'all'
+Requires-Dist: keyring ; (sys_platform == "win32") and extra == 'all'
+Requires-Dist: pywin32 ; (sys_platform == "win32") and extra == 'all'
+Provides-Extra: credentials
+Requires-Dist: keyring ; (sys_platform == "win32") and extra == 'credentials'
+Requires-Dist: pywin32 ; (sys_platform == "win32") and extra == 'credentials'
+Provides-Extra: django
+Requires-Dist: cmarkgfm ; extra == 'django'
+Requires-Dist: django ; extra == 'django'
+Requires-Dist: djangorestframework ; extra == 'django'
+Provides-Extra: excel
+Requires-Dist: openpyxl ; extra == 'excel'
+Requires-Dist: defusedxml ; extra == 'excel'
+Provides-Extra: mssql
+Requires-Dist: pyodbc ; extra == 'mssql'
+Requires-Dist: sqlparams ; extra == 'mssql'
+Requires-Dist: sqlparse ; extra == 'mssql'
+Provides-Extra: pg
+Requires-Dist: psycopg2-binary ; extra == 'pg'
+Requires-Dist: sqlparse ; extra == 'pg'
+Requires-Dist: sqlparams ; extra == 'pg'
+Provides-Extra: smb
+Requires-Dist: smbprotocol ; extra == 'smb'
+Provides-Extra: tabulate
+Requires-Dist: tabulate ; extra == 'tabulate'
+Provides-Extra: winhttp
+Requires-Dist: pywin32 ; (sys_platform == "win32") and extra == 'winhttp'
+
+Zut
+===
+
+Reusable Python, Django and PostgreSql utilities.
+
+## Installation
+
+From PyPI, with all optional dependencies:
+
+    pip install zut[all]
+
+From a Git branch or tag (using https or ssh):
+
+    pip install git+https://gitlab.com/ipamo/zut.git@main#egg=zut[all]
+    pip install git+ssh://git@gitlab.com/ipamo/zut.git@main#egg=zut[all]
+
+Note: optionaly dependencies may also be chosen individually. For example, use `zut[excel,pg]` instead of `zut[all]`.
+
+
+## Usage examples
+
+### Configure logging
+
+```py
+from zut import configure_logging
+configure_logging()
+```
+
+### Flexible in/out - TODO
+
+Write text or tabular data to a flexible, easily configurable output: CSV or Excel file, or tabulated stdout/stderr.
+
+The output file may be on the local file system or on a Windows/Samba share (including when the library is used on Linux).
+
+Export text to stdout or to a file:
+
+```py
+import sys
+from zut import out_file
+
+with out_file(filename or sys.stdout) as f:
+    f.write("Content")
+```
+    
+Export tabular data to stdout or to a file:
+
+```py
+import sys
+from zut import out_table
+
+with out_table(filename or sys.stdout, headers=["Id", "Word"]) as t:
+    t.append([1, "Hello"])
+    t.append([2, "World"])
+```
+
+Tabular data can also be exported using dictionnaries (in this case, headers will be detected automatically by the library):
+
+```py
+import sys
+from zut import out_table
+
+with out_table(filename or sys.stdout) as t:
+    t.append({'id': 1, 'name': "Hello"})
+    t.append({'id': 2, 'col3': True})
+```
+
+If `filename` has extension with `.xlsx`, output will be in Excel 2010 format.
+Otherwise it will be in CSV format.
+
+If `filename` starts with `\\`, output will be done on the corresponding Windows/Samba share.
+To indicate Samba credentials, call `configure_smb_credentials` before using function `out_table`.
+Example:
+
+```py
+from zut import out_table, configure_smb_credentials
+
+configure_smb_credentials(user=..., password=...)
+
+with out_table(r"\\server\share\path\to\file") as o:
+    ...
+```
+
+
+## Resources
+
+- [Maintenance of the library and repository](doc/maintain.md)
```

## Comparing `zut-0.6.0.dist-info/RECORD` & `zut-0.6.1.dist-info/RECORD`

 * *Files 15% similar despite different names*

```diff
@@ -1,53 +1,57 @@
-zut/__init__.py,sha256=UdZZvUfhMui1q8zJwmPN6f6CYnekmGud3nihH43lnDc,1879
-zut/_version.py,sha256=tir60FttD0Iy4c7DDLL7XL5dIY5zf6B5u6HcQg7ngsw,160
-zut/apps.py,sha256=XEZI-wmtvcp12znG4fLK1wxEEcWnE5m5ByeQnCZn09I,263
-zut/colors.py,sha256=EdBHFTMoYO7b-oGJo9ViJTYJfWt3iI1EkLS4SALKNO8,915
-zut/commands.py,sha256=dFh7zq-SLHHVjR2lW2NaHXD0xbUDLwPBt0ZAkosH91k,5337
-zut/credentials.py,sha256=Sguuj9FD2l24xccVig3FS7vM4Jzca_jHzie7CtysQaY,9123
-zut/csv.py,sha256=ez-qHZ4RK5nfuImRDYDxaqjE0ZA-R3UoHftZSSk_FK0,1175
-zut/datetime.py,sha256=ueh7LjoMTba9_G_1ERg4Qz3zuG0PxLeCQZ9AIOS_Q4M,1436
-zut/excel.py,sha256=W1D9eDQK7B2AUeQvJGmljq0z3RNLiX5ZhlOC2iwPRBM,16858
+zut/__init__.py,sha256=Dm3HjoyxG9FOOTVIRHFlVngrb22PTIZxDuL6rKmoIpw,2054
+zut/_version.py,sha256=ebX_Zevgtz2Fe5hIE-TwhUGLetd_YdZa20foaBGOTBE,164
+zut/apps.py,sha256=jQ5XEeKzi7EBmtPjeZb8sAph8XWTT98g8zqGwVWIK2U,276
+zut/colors.py,sha256=T_NrD_MBlHoL5-muusPNmMlDcP3fGZnsVnd8jICGZIQ,949
+zut/commands.py,sha256=FGIRQuD4ZAU8r9s6gzxEWvm_u_P780G1VMSenO_HeDI,5710
+zut/config.py,sha256=Tqep4qzz-TKxxzrdaENtPy9pFBA7WUS3taalfjVu7VE,1524
+zut/credentials.py,sha256=pAap-oHTWEsBzMFRpXee0oOKN2s4bZJWyhQ8prfnBog,9366
+zut/csv.py,sha256=I8VuJaEbRuHsr_EkxOalsY0-vOmMKx3O5QoCSAHcv2s,1218
+zut/datetime.py,sha256=Xu69utORaIC7QT92vGzhGZb0x-u7k1_GAByPLn3hO10,1488
+zut/excel.py,sha256=uhJDjKd7ya92DbspxBzzEt4EItJVRVnyOpmFuaDWkBw,17303
 zut/filesh.py,sha256=qMzdOM3Y0bm7tkdGcpYPkft4rTsbVmO0rhSf4Kew5ZU,8086
-zut/git.py,sha256=U5CGH_ridbSh607ijx4DXwGA6llIbo5LE2Vunktr9gI,2508
-zut/gpg.py,sha256=sXQstE7HGJLPNjrU-M7tbSlHX97YLfJOXOsnrmLbTZ4,1494
-zut/json.py,sha256=ayq4byVpHtxaWMbGwYxtnMiH9wF8pJwoIUZc9WSmb5o,4770
-zut/logging.py,sha256=F2EySN2Ff9XrChLVp70e5qOF_ymwdEy93PJD2-o0Bqo,4401
-zut/misc.py,sha256=HBVVmW4nS8E6EgZtTWZD6n0CyKFIFhl_1zXf2Uq6FxM,3441
-zut/numeric.py,sha256=YdTkIZaQ5n43aeKA3cbMwN7Y-PwGR3PuHMc-6ty8oPA,3030
-zut/tabular.py,sha256=4BhVXNuGwlBh8wpRFffnEPrdsTx0fBc8lebMSHjtx1g,1987
-zut/text.py,sha256=MaXP-0cdanQx8vXqbw8dvWxb1K9kcm2vyEyIDB4AQkA,1949
-zut/db/__init__.py,sha256=slCjgv03FgNAKLY9F-ER-_FzE2Ev8uqGhYgerkvAX7U,1581
-zut/db/commons.py,sha256=j0pOmfayZrNKRcZKsBlRzowC-ZABDmcrXD8T3eRlKqM,13475
-zut/db/mssql.py,sha256=UEskN65Ubu531b3h4BeITEHCF-VuwFpZGrDR8pVCUDo,2288
-zut/db/pg.py,sha256=vnLRO0M6tnDmiXN-5-MTskq1ZcfXHNk39Nb8XQZnvbw,8293
-zut/db/sql_pg/010_extensions.sql,sha256=O_cAxgoDfBa3HbwklHapV8kpwvCAvEUDEBDZ3LCPALM,41
-zut/db/sql_pg/020_slugify.sql,sha256=U3n-sPePz8htN4Zdqfc4O-SEsyAlAVbfiZw9to8wBHc,464
+zut/git.py,sha256=A0oJiGkMOTleHyD6guaz2q507iGW6FBdpUyoAmPUXyU,2590
+zut/gpg.py,sha256=W12GOjojVuj6ZcOquVCwda8jJNzYXhQD7IUxbhiDP4s,1534
+zut/json.py,sha256=9UrErYUQyEYz11fQUtVovYv26XKPlcdDuKynupNzD38,4943
+zut/logging.py,sha256=2fwK6UWXBFqP-TrWm4D4Y2W4JFDCvMOeH4dWU9TLf7I,4530
+zut/misc.py,sha256=REdffcv5MeQ1PBPNXLNJAfqf5TBcIo3APp-qxvg7xJg,3765
+zut/numeric.py,sha256=-toFizvPFv-X7Nh5ZOvrlbdeCPt5cNN6eWUa_fjBsUo,3116
+zut/tabular.py,sha256=XQUEFiYW6uUbqLxWrzhLs_4iPuUuQI7rIPDZMPhhZEw,2054
+zut/text.py,sha256=ePkvz5CkB2J9enwZwV1H-EMfgVsmLjRsp-0m-sw9aZw,2019
+zut/db/__init__.py,sha256=kIIvke3nvHIRCmwrkwu72mLOHLnBMBpFMBQ5Hh1zhNA,1665
+zut/db/commons.py,sha256=eu1Sexz16VVtCCFhgJgt4eEnGoDq6YHOwBzkD1ti2tU,15036
+zut/db/mssql.py,sha256=wTYz_w13PxOkptesuoqlC8bI2MGZwm1RmyEuD764NQ4,2650
+zut/db/pg.py,sha256=9sXyh8qvRxnCP43MvQ1kYJ3VMSJLK_1PRbh9Fh9U-1Y,8672
+zut/db/sql_pg/010_extensions.sql,sha256=_uPdGwRw1eMwgO57xs-WSftqSbG62dmGJbdCl9mh7Z0,42
+zut/db/sql_pg/020_slugify.sql,sha256=bQy0KSOD3pYQnEHonP2zAENxINWrxIWtPEvdBfnl4Po,479
+zut/django/migration_tools.py,sha256=fzpZsDhGPRJj7lsWKPzuMiwqGrMGWpFiY7UHsS0Li3w,1157
 zut/django/management/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 zut/django/management/commands/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-zut/django/management/commands/reinit.py,sha256=Jd_y95WeKglVCaMyk8jTi18zfc8l-_Uf_w2OQ4yJYRU,6745
-zut/django/management/commands/seed_zut.py,sha256=DcWSFHr5MSQQQxYW2iV2_zHPIKCysvydCF_BacLK63s,454
-zut/django/middleware/__init__.py,sha256=Lo_B1k2cvoGRBibO-MByhPPjH6zZcofcTH_dyzQPSPU,3191
-zut/django/templatetags/__init__.py,sha256=UJDTUjhOgjwbo5q3oEkVhUXVPdXFbYz-uLCqmAZjsYU,70
-zut/django/templatetags/static_lib.py,sha256=dDwQsLIJNTxcxXwpyXeTr8iu6D1c1kuUrYQv54Mu-A4,2718
-zut/django/views/MarkdownTemplateView.py,sha256=9trPoi2k8Pqpzy-VctXryexLDmN3XpuWvMsLqcBWf6I,1609
-zut/django/views/__init__.py,sha256=6ngjBAEvTDPau-D74pKOar5sgyavB4UgrXkjV6iAc3E,55
-zut/inout/InCsv.py,sha256=UUiIFnz-AxIVPYQTl4IHQQnz_047wd31uzjhP3a7Opg,1722
-zut/inout/InDb.py,sha256=aBGY-R3XsO0HMKLYK5q4O3umT23WJbNr-NENaFHvoQU,1697
-zut/inout/InExcel.py,sha256=wxuq-DabJud1u-Exq-UXLyKKdflyaCyABVnijHA8g10,1401
-zut/inout/InTable.py,sha256=GPinSq2sgG8F9ve5-nyHooMQZytoUjnaERxRVq3IQ20,4023
-zut/inout/OutCsv.py,sha256=7HIOCGxTO67U-uDFZp5TtdVXL-AXfoPy7xCgQ-OmN9E,4547
-zut/inout/OutDb.py,sha256=C1veZOndK0JOKdbK-oKDeImPyNG34eokIXPenmNq9Es,2513
-zut/inout/OutExcel.py,sha256=-AE0cQEJ8zD418jCXIk6BV4Ee_8fJhNQLQeEjuvdnA8,3151
-zut/inout/OutFile.py,sha256=pHqT-6LT1hJXANdBOBT12SKCJP6X3M6djAQzTbJNhvo,3138
-zut/inout/OutTable.py,sha256=4DJiej8aHQwaNl1GdCiLhGQ4cHLo-4T3jFxk69RavQk,9354
-zut/inout/OutTabulate.py,sha256=8U_8uInbJPBX2dfApIi8GUy6wLty0I8pv9i4PXvHGVo,1121
-zut/inout/__init__.py,sha256=H7gLWvx4hUsd-u6ze0jpt-OwVl554GXSVTxj_mx_Dps,6198
-zut/inout/utils.py,sha256=zlTPLLjHPq84k6juEhkZ-lBlMVU6bo-uVIpHdI5Y0nw,2734
-zut/network/__init__.py,sha256=Or_-LjSzJEcUWrpw56pyVRgB1Ba9aGgQgL-hqZWIioE,8906
-zut/network/commons.py,sha256=rLDIIOhyT3FnidK1bva53QEXA7WJB8PiHJj9_Eol2FA,3729
-zut/network/winhttp.py,sha256=1ZcK2jFl9uJ5mDoo5ZDOx_27hvIHQ6Ytwda-GakhcJg,2887
-zut-0.6.0.dist-info/LICENSE.txt,sha256=YW_vj_kSud2P_JX-0rynxMZ1LXvL7YNpNHiwD-m_znQ,1048
-zut-0.6.0.dist-info/METADATA,sha256=VbI8W5BaBKZMhasYY4rTIA1OyangPJ50Dx_zPZfLruk,4693
-zut-0.6.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-zut-0.6.0.dist-info/top_level.txt,sha256=0b5rfbMHLPzpcwBifEaAzzfyv1gZPGT7RXa2myDnXXs,4
-zut-0.6.0.dist-info/RECORD,,
+zut/django/management/commands/reinit.py,sha256=UD4GPc1zE1XO6uQwL9qnB9-oV6Rxnbiz-h9jm7y8ZUY,8265
+zut/django/middleware/__init__.py,sha256=feBoOfNooaUB3e4PgsGr3w_2-2fWZBBhD__-YmZ6rDo,3271
+zut/django/templatetags/__init__.py,sha256=ilO4n2YF-8Nmcg4gB1sH9hB5Sa_lDzFu-gjoDuBMJAk,71
+zut/django/templatetags/static_lib.py,sha256=p3jbZGCNlqOWpDZFnGXonKlOOHZKQuNwR6ziF0wY0io,2808
+zut/django/tracking/__init__.py,sha256=KtHO-ri7ThbrYeu3Wvjpo73Izk7Jp7Dqouu9lwooDIc,9036
+zut/django/tracking/models.py,sha256=gB1wQG-JyYq3cXPKBGXMJB2AG2AlAcBzT7eXNyKsKBA,4666
+zut/django/tracking/sql_pg/tracking_history_view.sql,sha256=o79jhgTnQ8nvx_nlILKQDnFjccC1M3WgAEYUAjMd5Gw,621
+zut/django/views/MarkdownTemplateView.py,sha256=fNNDD5Z71fZucbnUKhcyCCBmDFFLzY6DVEgdn3pcYkA,1652
+zut/django/views/__init__.py,sha256=1ueMo8cvT4-sr9csWSxwWvlGot7__HWqAwT38gIVVZU,56
+zut/inout/InCsv.py,sha256=eBNJGK6gfsscwPFdgZftgSwnmuXFfObTz2GumukXAlw,1779
+zut/inout/InDb.py,sha256=o0TSp00JZZU8z_8q89KPoicD2coP5YRUj2VMzxCvrsY,2169
+zut/inout/InExcel.py,sha256=y_Wj9a6End60hgMFV-y17YJ80Ee88opc1deq3XbKs_4,1449
+zut/inout/InTable.py,sha256=pxmJXeQKQ89laYvkHHPwnFMenQOm6aCtfNil1zzVqAY,4133
+zut/inout/OutCsv.py,sha256=okDi8uhG1UajXXf6l0aS4eqUEPvOEJDMTdoB73P3PJE,4669
+zut/inout/OutDb.py,sha256=a0YZolcIy2FGeRyWIDSg5crtoGGkjTxdZtGHgB9L-3M,2901
+zut/inout/OutExcel.py,sha256=gxx72SZaA8J1WvoprG800KKUibbw5WHFVT1Q4RbFKJc,3251
+zut/inout/OutFile.py,sha256=ecsBLG4Y5wDER4nUSeFcYucCKyjl6PRczdx6OhhWOTI,3235
+zut/inout/OutTable.py,sha256=N5H1n3EaKCyK9rlAxBH9p_XC5deVSh6kzsS8LiwLPxs,9635
+zut/inout/OutTabulate.py,sha256=4ShET0leKM2BcowqzjYk4x2kp39N_uBoZULxiCwhqGc,1165
+zut/inout/__init__.py,sha256=Do1H4fQ7_dP0MmBpJV3YCeqm-RcwCwzLeMh3B3iv498,7369
+zut/inout/utils.py,sha256=c75CvXC9MUD4vb5I4Ha3EkaUx6EuvclafKEX3piqcs4,2812
+zut/network/__init__.py,sha256=DaidKvr4BcJ3xL7fT1jUixRsXM1E8WQKgdbk0xCGvH8,9147
+zut/network/commons.py,sha256=ZeVqQ5I1neDMx3-GEUJFgCTOIlBAVSXmgPvhc7ETh4A,3850
+zut/network/winhttp.py,sha256=sJQQc0a7-BLI5NEI82F4x4qEYBNtx31kAgNZkOGvoyM,2962
+zut-0.6.1.dist-info/LICENSE.txt,sha256=g3B0wycmR6XLn-gWLyJ8IwvBHSAs3pzqmiwME45tzJk,1066
+zut-0.6.1.dist-info/METADATA,sha256=N6ptAUEtB_Y5aiar4PtLVXttmILhrnDfEvB-53x8xzU,4840
+zut-0.6.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+zut-0.6.1.dist-info/top_level.txt,sha256=0b5rfbMHLPzpcwBifEaAzzfyv1gZPGT7RXa2myDnXXs,4
+zut-0.6.1.dist-info/RECORD,,
```

