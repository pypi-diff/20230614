# Comparing `tmp/pytorch_ood-0.1.0-py3-none-any.whl.zip` & `tmp/pytorch_ood-0.1.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,19 +1,19 @@
-Zip file size: 101939 bytes, number of entries: 70
--rw-r--r--  2.0 unx      209 b- defN 23-Mar-24 09:31 pytorch_ood/__init__.py
--rw-r--r--  2.0 unx     1152 b- defN 23-Mar-10 12:41 pytorch_ood/api.py
+Zip file size: 100845 bytes, number of entries: 69
+-rw-r--r--  2.0 unx      209 b- defN 23-May-16 08:23 pytorch_ood/__init__.py
+-rw-r--r--  2.0 unx     2341 b- defN 23-May-07 00:37 pytorch_ood/api.py
 -rw-r--r--  2.0 unx       11 b- defN 23-Mar-08 12:25 pytorch_ood/dataset/__init__.py
 -rw-r--r--  2.0 unx      165 b- defN 23-Mar-08 12:25 pytorch_ood/dataset/audio/__init__.py
 -rw-r--r--  2.0 unx     3059 b- defN 23-Mar-10 12:25 pytorch_ood/dataset/audio/fsdd.py
 -rw-r--r--  2.0 unx     3043 b- defN 23-Mar-15 16:07 pytorch_ood/dataset/img/__init__.py
 -rw-r--r--  2.0 unx     2363 b- defN 23-Mar-15 16:07 pytorch_ood/dataset/img/base.py
--rw-r--r--  2.0 unx     4330 b- defN 23-Mar-10 12:41 pytorch_ood/dataset/img/chars74k.py
+-rw-r--r--  2.0 unx     4329 b- defN 23-May-07 00:37 pytorch_ood/dataset/img/chars74k.py
 -rw-r--r--  2.0 unx     3076 b- defN 23-Mar-15 16:07 pytorch_ood/dataset/img/cifar.py
 -rw-r--r--  2.0 unx     1186 b- defN 23-Mar-15 16:07 pytorch_ood/dataset/img/fooling.py
--rw-r--r--  2.0 unx     5794 b- defN 23-Mar-10 12:25 pytorch_ood/dataset/img/imagenet.py
+-rw-r--r--  2.0 unx     5793 b- defN 23-May-07 00:37 pytorch_ood/dataset/img/imagenet.py
 -rw-r--r--  2.0 unx     4270 b- defN 23-Mar-10 12:25 pytorch_ood/dataset/img/mnistc.py
 -rw-r--r--  2.0 unx     5382 b- defN 23-Mar-10 12:25 pytorch_ood/dataset/img/mvtech.py
 -rw-r--r--  2.0 unx     3671 b- defN 23-Mar-10 12:25 pytorch_ood/dataset/img/noise.py
 -rw-r--r--  2.0 unx     4875 b- defN 23-Mar-10 12:25 pytorch_ood/dataset/img/odin.py
 -rw-r--r--  2.0 unx     8477 b- defN 23-Mar-15 16:07 pytorch_ood/dataset/img/pixmix.py
 -rw-r--r--  2.0 unx     4242 b- defN 23-Mar-23 13:43 pytorch_ood/dataset/img/streethazards.py
 -rw-r--r--  2.0 unx     3001 b- defN 23-Mar-10 12:25 pytorch_ood/dataset/img/textures.py
@@ -24,49 +24,48 @@
 -rw-r--r--  2.0 unx     1049 b- defN 23-Mar-10 12:25 pytorch_ood/dataset/txt/__init__.py
 -rw-r--r--  2.0 unx     2665 b- defN 23-Mar-10 12:25 pytorch_ood/dataset/txt/multi30k.py
 -rw-r--r--  2.0 unx     4002 b- defN 23-Mar-10 12:25 pytorch_ood/dataset/txt/newsgroups.py
 -rw-r--r--  2.0 unx     5954 b- defN 23-Mar-10 12:25 pytorch_ood/dataset/txt/reuters.py
 -rw-r--r--  2.0 unx     1983 b- defN 23-Mar-08 12:25 pytorch_ood/dataset/txt/stop_words.py
 -rw-r--r--  2.0 unx     3331 b- defN 23-Mar-10 12:25 pytorch_ood/dataset/txt/wiki.py
 -rw-r--r--  2.0 unx     2034 b- defN 23-Mar-10 12:25 pytorch_ood/dataset/txt/wmt16.py
--rw-r--r--  2.0 unx     1881 b- defN 23-Mar-23 14:06 pytorch_ood/detector/__init__.py
--rw-r--r--  2.0 unx     1934 b- defN 23-Mar-15 16:07 pytorch_ood/detector/energy.py
--rw-r--r--  2.0 unx     1639 b- defN 23-Mar-23 13:43 pytorch_ood/detector/entropy.py
--rw-r--r--  2.0 unx     3478 b- defN 23-Mar-23 13:43 pytorch_ood/detector/klmatching.py
--rw-r--r--  2.0 unx     6666 b- defN 23-Mar-23 13:43 pytorch_ood/detector/mahalanobis.py
--rw-r--r--  2.0 unx     1412 b- defN 23-Mar-15 16:07 pytorch_ood/detector/maxlogit.py
--rw-r--r--  2.0 unx     3127 b- defN 23-Mar-15 16:07 pytorch_ood/detector/mcd.py
--rw-r--r--  2.0 unx     5365 b- defN 23-Mar-15 16:07 pytorch_ood/detector/odin.py
--rw-r--r--  2.0 unx     1891 b- defN 23-Mar-15 16:07 pytorch_ood/detector/softmax.py
--rw-r--r--  2.0 unx     4320 b- defN 23-Mar-23 13:43 pytorch_ood/detector/vim.py
+-rw-r--r--  2.0 unx     2395 b- defN 23-May-07 00:37 pytorch_ood/detector/__init__.py
+-rw-r--r--  2.0 unx     2352 b- defN 23-May-07 00:37 pytorch_ood/detector/energy.py
+-rw-r--r--  2.0 unx     2046 b- defN 23-May-07 00:37 pytorch_ood/detector/entropy.py
+-rw-r--r--  2.0 unx     4402 b- defN 23-May-07 00:37 pytorch_ood/detector/klmatching.py
+-rw-r--r--  2.0 unx     7700 b- defN 23-May-07 00:37 pytorch_ood/detector/mahalanobis.py
+-rw-r--r--  2.0 unx     1810 b- defN 23-May-07 00:37 pytorch_ood/detector/maxlogit.py
+-rw-r--r--  2.0 unx     3619 b- defN 23-May-07 00:37 pytorch_ood/detector/mcd.py
+-rw-r--r--  2.0 unx     5816 b- defN 23-May-07 00:37 pytorch_ood/detector/odin.py
+-rw-r--r--  2.0 unx     2316 b- defN 23-May-07 00:37 pytorch_ood/detector/softmax.py
+-rw-r--r--  2.0 unx     5267 b- defN 23-May-07 00:37 pytorch_ood/detector/vim.py
 -rw-r--r--  2.0 unx      323 b- defN 23-Mar-15 16:07 pytorch_ood/detector/openmax/__init__.py
--rw-r--r--  2.0 unx     7058 b- defN 23-Mar-10 12:25 pytorch_ood/detector/openmax/numpy.py
--rw-r--r--  2.0 unx     3158 b- defN 23-Mar-10 12:41 pytorch_ood/detector/openmax/torch.py
+-rw-r--r--  2.0 unx     7041 b- defN 23-Apr-12 11:02 pytorch_ood/detector/openmax/numpy.py
+-rw-r--r--  2.0 unx     3318 b- defN 23-May-07 00:37 pytorch_ood/detector/openmax/torch.py
 -rw-r--r--  2.0 unx     5408 b- defN 23-Mar-23 13:43 pytorch_ood/loss/__init__.py
 -rw-r--r--  2.0 unx     1589 b- defN 23-Mar-15 16:07 pytorch_ood/loss/background.py
 -rw-r--r--  2.0 unx     3762 b- defN 23-Mar-10 12:41 pytorch_ood/loss/cac.py
 -rw-r--r--  2.0 unx     4014 b- defN 23-Mar-10 12:41 pytorch_ood/loss/center.py
 -rw-r--r--  2.0 unx     2378 b- defN 23-Mar-15 16:07 pytorch_ood/loss/conf.py
 -rw-r--r--  2.0 unx     1188 b- defN 23-Mar-10 12:41 pytorch_ood/loss/crossentropy.py
 -rw-r--r--  2.0 unx     2565 b- defN 23-Mar-23 13:43 pytorch_ood/loss/energy.py
 -rw-r--r--  2.0 unx     2741 b- defN 23-Mar-23 13:43 pytorch_ood/loss/entropy.py
 -rw-r--r--  2.0 unx     4599 b- defN 23-Mar-10 12:41 pytorch_ood/loss/ii.py
 -rw-r--r--  2.0 unx     4875 b- defN 23-Mar-23 13:43 pytorch_ood/loss/mchad.py
 -rw-r--r--  2.0 unx     2667 b- defN 23-Mar-23 13:43 pytorch_ood/loss/objectosphere.py
 -rw-r--r--  2.0 unx     2199 b- defN 23-Mar-15 16:07 pytorch_ood/loss/oe.py
--rw-r--r--  2.0 unx     4556 b- defN 23-Mar-10 12:41 pytorch_ood/loss/svdd.py
--rw-r--r--  2.0 unx      867 b- defN 23-Mar-15 16:07 pytorch_ood/model/__init__.py
+-rw-r--r--  2.0 unx     4581 b- defN 23-May-07 00:37 pytorch_ood/loss/svdd.py
+-rw-r--r--  2.0 unx      723 b- defN 23-May-07 00:59 pytorch_ood/model/__init__.py
 -rw-r--r--  2.0 unx     4676 b- defN 23-Mar-10 12:41 pytorch_ood/model/centers.py
 -rw-r--r--  2.0 unx     1576 b- defN 23-Mar-23 19:24 pytorch_ood/model/gru.py
--rw-r--r--  2.0 unx     9219 b- defN 23-Mar-10 12:25 pytorch_ood/model/vit.py
 -rw-r--r--  2.0 unx    10550 b- defN 23-Mar-10 12:41 pytorch_ood/model/wrn.py
 -rw-r--r--  2.0 unx       70 b- defN 23-Mar-08 12:25 pytorch_ood/utils/__init__.py
 -rw-r--r--  2.0 unx    10960 b- defN 23-Mar-15 16:07 pytorch_ood/utils/gdown.py
--rw-r--r--  2.0 unx     7037 b- defN 23-Mar-15 16:07 pytorch_ood/utils/metrics.py
+-rw-r--r--  2.0 unx     7093 b- defN 23-May-16 07:50 pytorch_ood/utils/metrics.py
 -rw-r--r--  2.0 unx     2111 b- defN 23-Mar-08 12:25 pytorch_ood/utils/transforms.py
--rw-r--r--  2.0 unx     8689 b- defN 23-Mar-23 13:43 pytorch_ood/utils/utils.py
--rw-rw-rw-  2.0 unx    11351 b- defN 23-Mar-27 11:15 pytorch_ood-0.1.0.dist-info/LICENSE
--rw-r--r--  2.0 unx    23527 b- defN 23-Mar-27 11:15 pytorch_ood-0.1.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Mar-27 11:15 pytorch_ood-0.1.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       12 b- defN 23-Mar-27 11:15 pytorch_ood-0.1.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     6107 b- defN 23-Mar-27 11:15 pytorch_ood-0.1.0.dist-info/RECORD
-70 files, 276995 bytes uncompressed, 92237 bytes compressed:  66.7%
+-rw-r--r--  2.0 unx     8846 b- defN 23-May-07 00:37 pytorch_ood/utils/utils.py
+-rw-rw-rw-  2.0 unx    11351 b- defN 23-May-16 08:23 pytorch_ood-0.1.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx    23654 b- defN 23-May-16 08:23 pytorch_ood-0.1.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-16 08:23 pytorch_ood-0.1.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       12 b- defN 23-May-16 08:23 pytorch_ood-0.1.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6026 b- defN 23-May-16 08:23 pytorch_ood-0.1.1.dist-info/RECORD
+69 files, 275256 bytes uncompressed, 91267 bytes compressed:  66.8%
```

## zipnote {}

```diff
@@ -168,17 +168,14 @@
 
 Filename: pytorch_ood/model/centers.py
 Comment: 
 
 Filename: pytorch_ood/model/gru.py
 Comment: 
 
-Filename: pytorch_ood/model/vit.py
-Comment: 
-
 Filename: pytorch_ood/model/wrn.py
 Comment: 
 
 Filename: pytorch_ood/utils/__init__.py
 Comment: 
 
 Filename: pytorch_ood/utils/gdown.py
@@ -189,23 +186,23 @@
 
 Filename: pytorch_ood/utils/transforms.py
 Comment: 
 
 Filename: pytorch_ood/utils/utils.py
 Comment: 
 
-Filename: pytorch_ood-0.1.0.dist-info/LICENSE
+Filename: pytorch_ood-0.1.1.dist-info/LICENSE
 Comment: 
 
-Filename: pytorch_ood-0.1.0.dist-info/METADATA
+Filename: pytorch_ood-0.1.1.dist-info/METADATA
 Comment: 
 
-Filename: pytorch_ood-0.1.0.dist-info/WHEEL
+Filename: pytorch_ood-0.1.1.dist-info/WHEEL
 Comment: 
 
-Filename: pytorch_ood-0.1.0.dist-info/top_level.txt
+Filename: pytorch_ood-0.1.1.dist-info/top_level.txt
 Comment: 
 
-Filename: pytorch_ood-0.1.0.dist-info/RECORD
+Filename: pytorch_ood-0.1.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pytorch_ood/__init__.py

```diff
@@ -1,8 +1,8 @@
 """
 PyTorch Out-of-Distribution Detection
 """
-__version__ = "0.1.0"
+__version__ = "0.1.1"
 
 from . import api, dataset, detector, loss, model, utils
 
 __all__ = ["dataset", "detector", "loss", "model", "utils", "api", "__version__"]
```

## pytorch_ood/api.py

```diff
@@ -1,48 +1,83 @@
 from abc import ABC, abstractmethod
 from typing import TypeVar
 
-import torch
+from torch import Tensor
 from torch.utils.data import DataLoader
 
 Self = TypeVar("Self")
 
 
 class RequiresFittingException(Exception):
     """
     Raised when predict is called on a detector that has not been fitted.
     """
 
-    pass
+    def __init__(self, msg="You have to call fit() before predict()"):
+        super(RequiresFittingException, self).__init__(msg)
+
+
+class ModelNotSetException(ValueError):
+    """
+    Raised when predict() is called but no model was given.
+    """
+
+    def __init__(self, msg="When using predict(), model must not be None"):
+        super(ModelNotSetException, self).__init__(msg)
 
 
 class Detector(ABC):
     """
     Abstract Base Class for an Out-of-Distribution Detector
     """
 
-    def __call__(self, *args, **kwargs) -> torch.Tensor:
+    def __call__(self, *args, **kwargs) -> Tensor:
         """
         Forwards to predict
         """
         return self.predict(*args, **kwargs)
 
     @abstractmethod
     def fit(self: Self, data_loader: DataLoader) -> Self:
         """
-        Fit the model to a dataset. Some methods require this.
+        Fit the detector to a dataset. Some methods require this.
 
         :param data_loader: dataset to fit on. This is usually the training dataset.
+
+        :raise ModelNotSetException: if model was not set
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def fit_features(self: Self, x: Tensor, y: Tensor) -> Self:
+        """
+        Fit the detector directly on features. Some methods require this.
+
+        :param x: training features to use for fitting.
+        :param y: corresponding class labels.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def predict(self, x: Tensor) -> Tensor:
+        """
+        Calculates outlier scores. Inputs will be passed through the model.
+
+        :param x: batch of data
+        :return: outlier scores for points
+
+        :raise RequiresFitException: if detector has to be fitted to some data
+        :raise ModelNotSetException: if model was not set
         """
         raise NotImplementedError
 
     @abstractmethod
-    def predict(self, x: torch.Tensor) -> torch.Tensor:
+    def predict_features(self, x: Tensor) -> Tensor:
         """
-        Calculates outlier scores.
+        Calculates outlier scores based on features.
 
         :param x: batch of data
         :return: outlier scores for points
 
         :raise RequiresFitException: if detector has to be fitted to some data
         """
         raise NotImplementedError
```

## pytorch_ood/dataset/img/chars74k.py

```diff
@@ -84,15 +84,14 @@
         self._load()
         self._create_class_remapping()
 
     def _check_integrity(self) -> bool:
         return check_integrity(join(self.root, self.filename_dataset), self.tgz_dataset_md5)
 
     def _download(self):
-
         if self._check_integrity():
             return
 
         download_url(self.url_dataset, self.root, self.filename_dataset, self.tgz_dataset_md5)
         download_url(self.url_list, self.root, self.filename_list, self.tgz_list_md5)
         with tarfile.open(os.path.join(self.root, self.filename_dataset), "r:gz") as tar:
             tar.extractall(path=self.root)
```

## pytorch_ood/dataset/img/imagenet.py

```diff
@@ -29,15 +29,14 @@
     def __init__(
         self,
         root: str,
         transform: Optional[Callable] = None,
         target_transform: Optional[Callable] = None,
         download: bool = False,
     ):
-
         self.root = root
 
         if download:
             self.download()
 
         if not self._check_integrity():
             raise RuntimeError(
```

## pytorch_ood/detector/__init__.py

```diff
@@ -1,33 +1,47 @@
 """
 Detectors
 ******************
 
-This package contains a collection of different Out-of-Distribution Detectors.
+This module provides a collection of different Out-of-Distribution Detectors.
 
 API
 ------
 Each detector implements a common API which contains a ``predict`` and a ``fit`` method, where ``fit`` is optional.
-The objects ``__call__`` methods become an alias for the ``predict`` function, so you can use
+The objects ``__call__`` methods is delegated to the the ``predict`` function, so you can use
 
 .. code:: python
 
     detector = Detector(model)
     detector.fit(data_loader)
     scores = detector(x)
 
 
+Feature-based Interface
+^^^^^^^^^^^^^^^^^^^^^^^^
+
+Alternatively, you can also use the ``fit_features`` and ``predict_features`` methods.
+In that case, inputs will not be passed through the model. This can help to avoid passing
+data through the model multiple times when fitting several detectors. Detectors who do not
+support this will raise an exception.
+
+.. code:: python
+
+    detector = Detector(model=None)
+    detector.fit_features(train_features, train_labels)
+    scores = detector.predict_features(test_features)
+
 Some of the detectors support grid-like input, so that they can be used for anomaly segmentation
-without the need for further adjustment.
+without further adjustment.
 
 
 ..  autoclass:: pytorch_ood.api.Detector
     :members:
 
-Maximum Softmax
+Maximum Softmax (MSP)
 -------------------------------
 .. automodule:: pytorch_ood.detector.softmax
 
 Maximum Logit
 -------------------------------
 .. automodule:: pytorch_ood.detector.maxlogit
```

## pytorch_ood/detector/energy.py

```diff
@@ -7,17 +7,18 @@
 
 ..  autoclass:: pytorch_ood.detector.EnergyBased
     :members:
 
 """
 from typing import Optional, TypeVar
 
-import torch
+from torch import Tensor, logsumexp
+from torch.nn import Module
 
-from ..api import Detector
+from ..api import Detector, ModelNotSetException
 
 Self = TypeVar("Self")
 
 
 class EnergyBased(Detector):
     """
     Implements the Energy Score of  *Energy-based Out-of-distribution Detection*.
@@ -40,32 +41,47 @@
 
     def fit(self: Self, *args, **kwargs) -> Self:
         """
         Not required.
         """
         return self
 
-    def __init__(self, model: torch.nn.Module, t: Optional[float] = 1):
+    def fit_features(self: Self, *args, **kwargs) -> Self:
+        """
+        Not required.
+        """
+        return self
+
+    def __init__(self, model: Module, t: Optional[float] = 1.0):
         """
         :param t: Temperature value :math:`T`. Default is 1.
         """
         super(EnergyBased, self).__init__()
         self.t: float = t  #: Temperature
         self.model = model
 
-    def predict(self, x: torch.Tensor) -> torch.Tensor:
+    def predict(self, x: Tensor) -> Tensor:
         """
         Calculate negative energy for inputs
 
         :param x: input tensor, will be passed through model
 
         :return: Energy score
         """
+        if self.model is None:
+            raise ModelNotSetException
+
         return self.score(self.model(x), t=self.t)
 
+    def predict_features(self, logits: Tensor) -> Tensor:
+        """
+        :param logits: logits given by the model
+        """
+        return EnergyBased.score(logits, t=self.t)
+
     @staticmethod
-    def score(logits: torch.Tensor, t: Optional[float] = 1) -> torch.Tensor:
+    def score(logits: Tensor, t: Optional[float] = 1.0) -> Tensor:
         """
         :param logits: logits of input
         :param t: temperature value
         """
-        return -t * torch.logsumexp(logits / t, dim=1)
+        return -t * logsumexp(logits / t, dim=1)
```

## pytorch_ood/detector/entropy.py

```diff
@@ -7,17 +7,18 @@
 
 ..  autoclass:: pytorch_ood.detector.Entropy
     :members:
 
 """
 from typing import Optional, TypeVar
 
-from torch import Tensor, nn
+from torch import Tensor
+from torch.nn import Module
 
-from ..api import Detector
+from ..api import Detector, ModelNotSetException
 
 Self = TypeVar("Self")
 
 
 class Entropy(Detector):
     """
     Implements Entropy-based OOD detection.
@@ -35,31 +36,46 @@
 
     def fit(self: Self, *args, **kwargs) -> Self:
         """
         Not required.
         """
         return self
 
-    def __init__(self, model: nn.Module):
+    def fit_features(self: Self, *args, **kwargs) -> Self:
+        """
+        Not required.
+        """
+        return self
+
+    def __init__(self, model: Module):
         """
         :param model: the model :math:`f`
         """
         super(Entropy, self).__init__()
         self.model = model
 
     def predict(self, x: Tensor) -> Tensor:
         """
         Calculate entropy for inputs
 
         :param x: input tensor, will be passed through model
 
         :return: Entropy score
         """
+        if self.model is None:
+            raise ModelNotSetException
+
         return self.score(self.model(x))
 
+    def predict_features(self, logits: Tensor) -> Tensor:
+        """
+        :param logits: logits given by your model
+        """
+        return self.score(logits)
+
     @staticmethod
     def score(logits: Tensor) -> Tensor:
         """
         :param logits: logits of input
         """
         p = logits.softmax(dim=1)
         return -(p.log() * p).sum(dim=1)
```

## pytorch_ood/detector/klmatching.py

```diff
@@ -9,20 +9,20 @@
     :members:
 
 """
 import logging
 from typing import TypeVar
 
 import torch
-from torch.nn import Parameter, ParameterDict
+from torch import Tensor
+from torch.nn import Module, Parameter, ParameterDict
 from torch.utils.data import DataLoader
 
-from pytorch_ood.utils import TensorBuffer, extract_features, is_known
-
-from ..api import Detector, RequiresFittingException
+from ..api import Detector, ModelNotSetException, RequiresFittingException
+from ..utils import extract_features
 
 log = logging.getLogger()
 
 Self = TypeVar("Self")
 
 
 class KLMatching(Detector):
@@ -37,15 +37,15 @@
     :math:`D_{KL}[p(y \\vert x) \\Vert d_y]` is used as outlier score.
 
     This method can also be applied to multi-class settings.
 
     :see Paper: `ArXiv <https://arxiv.org/abs/1911.11132>`__
     """
 
-    def __init__(self, model: torch.nn.Module):
+    def __init__(self, model: Module):
         """
         :param model: neural network, is assumed to output logits.
         """
         super(KLMatching, self).__init__()
         self.model = model
         self.dists: ParameterDict = ParameterDict()  #: Typical posteriors per class
 
@@ -53,47 +53,73 @@
         """
         Estimates typical distributions for each class.
         Ignores OOD samples.
 
         :param data_loader: validation data loader
         :param device: device which should be used for calculations
         """
+        if self.model is None:
+            raise ModelNotSetException
+
         logits, labels = extract_features(data_loader, self.model, device)
+        return self.fit_features(logits, labels, device)
+
+    def fit_features(self: Self, logits: Tensor, labels: Tensor, device="cpu") -> Self:
+        """
+        Estimates typical distributions for each class.
+        Ignores OOD samples.
+
+        :param logits: logits
+        :param labels: class labels
+        :param device: device which should be used for calculations
+        """
+        logits, labels = logits.to(device), labels.to(device)
         y_hat = logits.max(dim=1).indices
         probabilities = logits.softmax(dim=1)
 
         for label in labels.unique():
             log.debug(f"Fitting class {label}")
             d_k = probabilities[labels == label].mean(dim=0)
             self.dists[str(label.item())] = Parameter(d_k)
 
         return self
 
-    def predict(self, x: torch.Tensor) -> torch.Tensor:
+    def predict_features(self, p: Tensor) -> Tensor:
         """
-        Calculates KL-Divergence between predicted posteriors and typical posteriors.
-
-        :param x: input tensor, will be passed through model
-        :return: Outlier scores
+        :param p: probabilities predicted by the model
         """
-        if len(self.dists) == 0:
-            raise RequiresFittingException("KL-Matching has to be fitted on validation data.")
-
-        #
-        device = x.device
-        self.dists.to(device)
-
-        probs = self.model(x).softmax(dim=1)
-        predictions = probs.argmax(dim=1)
-        scores = torch.empty(size=(probs.shape[0],), device=device)
+        device = p.device
+        predictions = p.argmax(dim=1)
+        scores = torch.empty(size=(p.shape[0],), device=device)
 
         for label in predictions.unique():
             if str(label.item()) not in self.dists:
                 raise ValueError(f"Label {label.item()} not fitted.")
 
             dist = self.dists[str(label.item())]
-            class_p = probs[predictions == label]
+            class_p = p[predictions == label]
             class_d = dist.unsqueeze(0).repeat(class_p.shape[0], 1)
             d_kl = (class_p * (class_p / class_d).log()).sum(dim=1)
             scores[predictions == label] = d_kl
 
         return scores
+
+    def predict(self, x: Tensor) -> Tensor:
+        """
+        Calculates KL-Divergence between predicted posteriors and typical posteriors.
+
+        :param x: input tensor, will be passed through model
+        :return: Outlier scores
+        """
+        if len(self.dists) == 0:
+            raise RequiresFittingException("KL-Matching has to be fitted on validation data.")
+
+        if self.model is None:
+            raise ModelNotSetException
+
+        # we move the dict with the typical posteriors to the same device as the input
+        # this might be not desirable in some cases, but avoids errors
+        device = x.device
+        self.dists.to(device)
+
+        p = self.model(x).softmax(dim=1)
+        return self.predict_features(p)
```

## pytorch_ood/detector/mahalanobis.py

```diff
@@ -9,18 +9,19 @@
     :members:
 """
 import logging
 import warnings
 from typing import Callable, List, Optional, TypeVar
 
 import torch
+from torch import Tensor
 from torch.autograd import Variable
 from torch.utils.data import DataLoader
 
-from ..api import Detector, RequiresFittingException
+from ..api import Detector, ModelNotSetException, RequiresFittingException
 from ..utils import TensorBuffer, contains_unknown, extract_features, is_known, is_unknown
 
 log = logging.getLogger(__name__)
 
 Self = TypeVar("Self")
 
 
@@ -36,28 +37,28 @@
 
     :see Implementation: `GitHub <https://github.com/pokaxpoka/deep_Mahalanobis_detector>`__
     :see Paper: `ArXiv <https://arxiv.org/abs/1807.03888>`__
     """
 
     def __init__(
         self,
-        model: Callable[[torch.Tensor], torch.Tensor],
+        model: Callable[[Tensor], Tensor],
         eps: float = 0.002,
         norm_std: Optional[List] = None,
     ):
         """
         :param model: the Neural Network, should output features
         :param eps: magnitude for gradient based input preprocessing
         :param norm_std: Standard deviations for input normalization
         """
         super(Mahalanobis, self).__init__()
         self.model = model
-        self.mu: torch.Tensor = None  #: Centers
-        self.cov: torch.Tensor = None  #: Covariance Matrix
-        self.precision: torch.Tensor = None  #: Precision Matrix
+        self.mu: Tensor = None  #: Centers
+        self.cov: Tensor = None  #: Covariance Matrix
+        self.precision: Tensor = None  #: Precision Matrix
         self.eps: float = eps  #: epsilon
         self.norm_std = norm_std
 
     def fit(self: Self, data_loader: DataLoader, device: str = None) -> Self:
         """
         Fit parameters of the multi variate gaussian.
 
@@ -66,14 +67,31 @@
         :return:
         """
         if device is None:
             device = list(self.model.parameters())[0].device
             log.warning(f"No device given. Will use '{device}'.")
 
         z, y = extract_features(data_loader, self.model, device)
+        return self.fit_features(z, y, device)
+
+    def fit_features(self: Self, z: Tensor, y: Tensor, device: str = None) -> Self:
+        """
+        Fit parameters of the multi variate gaussian.
+
+        :param z: features
+        :param y: class labels
+        :param device: device to use
+        :return:
+        """
+
+        if device is None:
+            device = list(self.model.parameters())[0].device
+            log.warning(f"No device given. Will use '{device}'.")
+
+        z, y = z.to(device), y.to(device)
 
         log.debug("Calculating mahalanobis parameters.")
         classes = y.unique()
 
         # we assume here that all class 0 >= labels <= classes.max() exist
         assert len(classes) == classes.max().item() + 1
         assert not contains_unknown(classes)
@@ -88,26 +106,22 @@
             zs = z[idxs].to(device)
             self.mu[clazz] = zs.mean(dim=0)
             self.cov += (zs - self.mu[clazz]).T.mm(zs - self.mu[clazz])
 
         self.precision = torch.linalg.inv(self.cov)
         return self
 
-    def predict(self, x: torch.Tensor) -> torch.Tensor:
+    def predict_features(self, x: Tensor) -> Tensor:
         """
-        :param x: input tensor
-        """
-        if self.mu is None:
-            raise RequiresFittingException
-
-        if self.eps > 0:
-            x = self._odin_preprocess(x, x.device)
+        Calculates mahalanobis distance directly on features.
+        ODIN preprocessing will not be applied.
 
-        features = self.model(x)
-        features = features.view(features.size(0), features.size(1), -1)
+        :param x: features, as given by the model.
+        """
+        features = x.view(x.size(0), x.size(1), -1)
         features = torch.mean(features, 2)
         noise_gaussian_scores = []
 
         for clazz in range(self.n_classes):
             centered_features = features.data - self.mu[clazz]
             term_gau = (
                 -0.5
@@ -119,15 +133,31 @@
             noise_gaussian_scores.append(term_gau.view(-1, 1))
 
         noise_gaussian_score = torch.cat(noise_gaussian_scores, 1)
 
         noise_gaussian_score = torch.max(noise_gaussian_score, dim=1).values
         return -noise_gaussian_score
 
-    def _odin_preprocess(self, x, dev):
+    def predict(self, x: Tensor) -> Tensor:
+        """
+        :param x: input tensor
+        """
+        if self.mu is None:
+            raise RequiresFittingException
+
+        if self.model is None:
+            raise ModelNotSetException
+
+        if self.eps > 0:
+            x = self._odin_preprocess(x, x.device)
+
+        features = self.model(x)
+        return self.predict_features(features)
+
+    def _odin_preprocess(self, x: Tensor, dev: str):
         """
         NOTE: the original implementation uses mean over feature maps. here, we just flatten
         """
         # does not work in inference mode, this sometimes collides with pytorch-lightning
         if torch.is_inference_mode_enabled():
             warnings.warn("ODIN not compatible with inference mode. Will be deactivated.")
 
@@ -142,25 +172,26 @@
                 score = None
 
                 for clazz in range(self.n_classes):
                     centered_features = features.data - self.mu[clazz]
                     term_gau = (
                         -0.5
                         * torch.mm(
-                            torch.mm(centered_features, self.precision), centered_features.t()
+                            torch.mm(centered_features, self.precision),
+                            centered_features.t(),
                         ).diag()
                     )
 
                     if clazz == 0:
                         score = term_gau.view(-1, 1)
                     else:
                         score = torch.cat((score, term_gau.view(-1, 1)), dim=1)
 
-                # Input_processing
-                # calculate gradient of inputs with respect to score of predicted class, according to mahalanobis distance
+                # calculate gradient of inputs with respect to score of predicted class,
+                # according to mahalanobis distance
                 sample_pred = score.max(dim=1).indices
                 batch_sample_mean = self.mu.index_select(0, sample_pred)
                 centered_features = features - Variable(batch_sample_mean)
                 pure_gau = (
                     -0.5
                     * torch.mm(
                         torch.mm(centered_features, Variable(self.precision)),
@@ -181,8 +212,14 @@
                 )
         perturbed_x = x.data - self.eps * gradient
 
         return perturbed_x
 
     @property
     def n_classes(self):
+        """
+        Number of classes the model is fitted for
+        """
+        if self.mu is None:
+            raise RequiresFittingException
+
         return self.mu.shape[0]
```

## pytorch_ood/detector/maxlogit.py

```diff
@@ -6,17 +6,18 @@
 
 ..  autoclass:: pytorch_ood.detector.MaxLogit
     :members:
 
 """
 from typing import TypeVar
 
-import torch
+from torch import Tensor
+from torch.nn import Module
 
-from ..api import Detector
+from ..api import Detector, ModelNotSetException
 
 Self = TypeVar("Self")
 
 
 class MaxLogit(Detector):
     """
     Implements the Max Logit Method for OOD Detection as proposed in
@@ -26,32 +27,47 @@
 
     where :math:`f_y(x)` indicates the :math:`y^{th}` logits value predicted by :math:`f`.
 
     :see Paper:
        `ArXiv <https://.org/abs/1911.11132>`__
     """
 
-    def __init__(self, model: torch.nn.Module):
+    def __init__(self, model: Module):
         """
         :param t: temperature value T. Default is 1.
         """
         super(MaxLogit, self).__init__()
         self.model = model
 
-    def predict(self, x: torch.Tensor) -> torch.Tensor:
+    def predict(self, x: Tensor) -> Tensor:
         """
         :param x:  model inputs
         """
+        if self.model is None:
+            raise ModelNotSetException
+
         return self.score(self.model(x))
 
+    def predict_features(self, logits: Tensor) -> Tensor:
+        """
+        :param logits: logits as given by the model
+        """
+        return MaxLogit.score(logits)
+
     def fit(self: Self, *args, **kwargs) -> Self:
         """
         Not required
         """
         return self
 
+    def fit_features(self: Self, *args, **kwargs) -> Self:
+        """
+        Not required
+        """
+        return self
+
     @staticmethod
-    def score(logits: torch.Tensor) -> torch.Tensor:
+    def score(logits: Tensor) -> Tensor:
         """
         :param logits: logits for samples
         """
         return -logits.max(dim=1).values
```

## pytorch_ood/detector/mcd.py

```diff
@@ -9,17 +9,18 @@
     :members:
 
 """
 import logging
 from typing import Optional, TypeVar
 
 import torch
-from torch import nn
+from torch import Tensor, nn
+from torch.nn import Module
 
-from ..api import Detector
+from ..api import Detector, ModelNotSetException
 
 log = logging.getLogger(__name__)
 Self = TypeVar("Self")
 
 
 class MCD(Detector):
     """
@@ -35,41 +36,57 @@
     :see Paper: `ICML <http://proceedings.mlr.press/v48/gal16.pdf>`__
 
     .. warning:: This implementations puts the model into evaluation mode (except for variants of the BatchNorm Layers).
         This could also affect other modules and is currently a workaround.
 
     """
 
-    def __init__(self, model: nn.Module, samples: int = 30):
+    def __init__(self, model: Module, samples: int = 30):
         """
 
         :param model: the module to use for the forward pass. Should output logits.
         :param samples: number of iterations
         """
         self.model = model
         self.n_samples = samples  #: number :math:`N` of samples
 
     def fit(self: Self, data_loader) -> Self:
         """
         Not required
         """
         return self
 
+    def fit_features(self: Self, x: Tensor, y: Tensor) -> Self:
+        """
+        Not required
+        """
+        raise NotImplementedError
+
+    def predict_features(self, x: Tensor) -> Tensor:
+        """
+        This method can not be used, as the input has to be passed several times through the model.
+
+        :raise Exception:
+        """
+        raise Exception("You must use a model for MCD")
+
     @staticmethod
-    def run(model: torch.nn.Module, x: torch.Tensor, samples: int) -> torch.Tensor:
+    def run(model: Module, x: Tensor, samples: int) -> Tensor:
         """
         Assumes that the model outputs logits.
 
         .. note :: Input tensor should be on the same device as the model.
 
         :param model: neural network
         :param x: input
         :param samples: number of rounds
         :return: averaged output of the model
         """
+        if model is None:
+            raise ModelNotSetException
 
         mode_switch = False
 
         dev = x.device
 
         if not model.training:
             log.debug("Putting model into training mode ... ")
@@ -94,13 +111,13 @@
 
         if mode_switch:
             log.debug("Putting model into eval mode ... ")
             model.eval()
 
         return results
 
-    def predict(self, x: torch.Tensor) -> torch.Tensor:
+    def predict(self, x: Tensor) -> Tensor:
         """
         :param x: input
         :return: negative maximum average class score of the model
         """
         return -MCD.run(self.model, x, self.n_samples).max(dim=1).values
```

## pytorch_ood/detector/odin.py

```diff
@@ -12,34 +12,36 @@
 
 """
 import logging
 import warnings
 from typing import Callable, List, Optional, TypeVar
 
 import torch
+from torch import Tensor
 from torch.autograd import Variable
+from torch.nn import Module
 from torch.nn import functional as F
 
-from ..api import Detector
+from ..api import Detector, ModelNotSetException
 
 log = logging.getLogger(__name__)
 
 Self = TypeVar("Self")
 
 
 def zero_grad(x):
-    if type(x) is torch.Tensor():
+    if type(x) is Tensor():
         torch.fill_(x, 0)
 
 
 def odin_preprocessing(
     model: torch.nn.Module,
-    x: torch.Tensor,
-    y: Optional[torch.Tensor] = None,
-    criterion: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,
+    x: Tensor,
+    y: Optional[Tensor] = None,
+    criterion: Optional[Callable[[Tensor], Tensor]] = None,
     eps: float = 0.05,
     temperature: float = 1000,
     norm_std: Optional[List[float]] = None,
 ):
     """
     Functional version of ODIN.
 
@@ -49,14 +51,17 @@
         prediction will be used
     :param criterion: loss function :math:`\\mathcal{L}` to use. If none is given, we will use negative log
             likelihood
     :param eps: step size :math:`\\epsilon` of the gradient ascend step
     :param temperature: temperature :math:`T` to use for scaling
     :param norm_std: standard deviations used during preprocessing
     """
+    if model is None:
+        raise ModelNotSetException
+
     # does not work in inference mode, this sometimes collides with pytorch-lightning
     if torch.is_inference_mode_enabled():
         warnings.warn("ODIN not compatible with inference mode. Will be deactivated.")
 
     # we make this assignment here, because adding the default to the constructor messes with sphinx
     if criterion is None:
         criterion = F.nll_loss
@@ -106,18 +111,18 @@
     :see Paper: `ArXiv <https://arxiv.org/abs/1706.02690>`__
     :see Implementation: `GitHub <https://github.com/facebookresearch/odin/>`__
 
     """
 
     def __init__(
         self,
-        model: torch.nn.Module,
-        criterion: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,
+        model: Module,
+        criterion: Optional[Callable[[Tensor], Tensor]] = None,
         eps: float = 0.05,
-        temperature: float = 1000,
+        temperature: float = 1000.0,
         norm_std: Optional[List[float]] = None,
     ):
         """
         :param model: module to backpropagate through
         :param criterion: loss function :math:`\\mathcal{L}` to use. If None is given, we will use negative log
             likelihood
         :param eps: step size :math:`\\epsilon` of the gradient descent step
@@ -132,15 +137,15 @@
             criterion = F.nll_loss
 
         self.criterion = criterion  #: criterion :math:`\mathcal{L}`
         self.eps = eps  #: size :math:`\epsilon` of the gradient step in the input space
         self.temperature = temperature  #: temperature value :math:`T`
         self.norm_std = norm_std
 
-    def predict(self, x: torch.Tensor) -> torch.Tensor:
+    def predict(self, x: Tensor) -> Tensor:
         """
         Calculates softmax outlier scores on ODIN pre-processed inputs.
 
         :param x: input tensor
         :return: outlier scores for each sample
         """
         x_hat = odin_preprocessing(
@@ -155,7 +160,21 @@
         return -self.model(x_hat).softmax(dim=1).max(dim=1).values
 
     def fit(self: Self, *args, **kwargs) -> Self:
         """
         Not required
         """
         return self
+
+    def fit_features(self: Self, *args, **kwargs) -> Self:
+        """
+        Not required
+        """
+        return self
+
+    def predict_features(self, x: Tensor) -> Tensor:
+        """
+        Since ODIN requires backpropagating through the model, this method can not be used.
+
+        :raise Exception:
+        """
+        raise Exception("You must use a model for ODIN")
```

## pytorch_ood/detector/softmax.py

```diff
@@ -7,24 +7,25 @@
 
 ..  autoclass:: pytorch_ood.detector.MaxSoftmax
     :members:
 
 """
 from typing import Optional, TypeVar
 
-import torch
+from torch import Tensor
+from torch.nn import Module
 
-from ..api import Detector
+from ..api import Detector, ModelNotSetException
 
 Self = TypeVar("Self")
 
 
 class MaxSoftmax(Detector):
     """
-    Implements the Maximum Softmax Thresholding Baseline for OOD detection.
+    Implements the Maximum Softmax Probability (MSP) Thresholding baseline for OOD detection.
 
     Optionally, implements temperature scaling, which divides the logits by a constant temperature :math:`T`
     before calculating the softmax.
 
     .. math:: - \\max_y \\sigma_y(f(x) / T)
 
     where :math:`\\sigma` is the softmax function and :math:`\\sigma_y`  indicates the :math:`y^{th}` value of the
@@ -33,35 +34,50 @@
     :see Paper:
         `ArXiv <https://arxiv.org/abs/1610.02136>`_
     :see Implementation:
         `GitHub <https://github.com/hendrycks/error-detection>`_
 
     """
 
-    def __init__(self, model: torch.nn.Module, t: Optional[float] = 1):
+    def __init__(self, model: Module, t: Optional[float] = 1.0):
         """
         :param model: neural network to use
-        :param t: temperature value T. Default is 1.
+        :param t: temperature value :math:`T`. Default is 1.
         """
         super(MaxSoftmax, self).__init__()
         self.t = t
         self.model = model
 
-    def predict(self, x: torch.Tensor) -> torch.Tensor:
+    def predict(self, x: Tensor) -> Tensor:
         """
-        :param x: model input, will be passed through neural network
+        :param x: input, will be passed through model
         """
-        return self.score(self.model(x), t=1)
+        if self.model is None:
+            raise ModelNotSetException
+
+        return self.score(self.model(x), t=self.t)
 
     def fit(self: Self, *args, **kwargs) -> Self:
         """
         Not required
         """
         return self
 
+    def fit_features(self: Self, *args, **kwargs) -> Self:
+        """
+        Not required
+        """
+        return self
+
+    def predict_features(self, logits: Tensor) -> Tensor:
+        """
+        :param logits: logits given by the model
+        """
+        return MaxSoftmax.score(logits, self.t)
+
     @staticmethod
-    def score(logits: torch.Tensor, t: Optional[float] = 1) -> torch.Tensor:
+    def score(logits: Tensor, t: Optional[float] = 1.0) -> Tensor:
         """
         :param logits: logits for samples
         :param t: temperature value
         """
         return -logits.div(t).softmax(dim=1).max(dim=1).values
```

## pytorch_ood/detector/vim.py

```diff
@@ -12,18 +12,18 @@
 import logging
 from typing import Callable, TypeVar
 
 import numpy as np
 import torch
 from numpy.linalg import norm, pinv
 from scipy.special import logsumexp
+from torch import Tensor
 
-from pytorch_ood.utils import TensorBuffer, extract_features, is_known
-
-from ..api import Detector, RequiresFittingException
+from ..api import Detector, ModelNotSetException, RequiresFittingException
+from ..utils import extract_features
 
 log = logging.getLogger(__name__)
 Self = TypeVar("Self")
 
 
 class ViM(Detector):
     """
@@ -62,49 +62,80 @@
         """
         Calculates logits from features
 
         TODO: this could be done in pytorch
         """
         return np.matmul(features, self.w.T) + self.b
 
-    def predict(self, x: torch.Tensor) -> torch.Tensor:
+    def predict(self, x: Tensor) -> Tensor:
         """
         :param x: model input, will be passed through neural network
         """
+        if self.model is None:
+            raise ModelNotSetException
+
         if self.principal_subspace is None or self.alpha is None:
             raise RequiresFittingException()
 
         with torch.no_grad():
-            features = self.model(x).cpu()
+            features = self.model(x)
+
+        return self.predict_features(features)
+
+    def __repr__(self):
+        return f"ViM(d={self.n_dim})"
+
+    def fit(self: Self, data_loader, device="cpu") -> Self:
+        """
+        Extracts features and logits, computes principle subspace and alpha. Ignores OOD samples.
+
+        :param data_loader: dataset to fit on
+        :param device: device to use
+        :return:
+        """
+        try:
+            from sklearn.covariance import EmpiricalCovariance
+        except ImportError:
+            raise Exception("You need to install sklearn to use ViM.")
 
-        logits = self._get_logits(features)
+        if self.model is None:
+            raise ModelNotSetException
+
+        features, labels = extract_features(data_loader, self.model, device)
+        return self.fit_features(features, labels)
+
+    def predict_features(self, x: Tensor) -> Tensor:
+        """
+        :param x: features as given by the model
+        """
+        x = x.detach().cpu().numpy()
+        logits = self._get_logits(x)
 
         # calculate residual
-        x_p_t = norm(np.matmul(features - self.u, self.principal_subspace), axis=-1)
+        x_p_t = norm(np.matmul(x - self.u, self.principal_subspace), axis=-1)
         vlogit = x_p_t * self.alpha
         # clip for numerical stability, float32 easily overflows in logsumexp
         energy = logsumexp(np.clip(logits, -100, 100), axis=-1)
         score = -vlogit + energy
+        return -Tensor(score)
 
-        return -torch.Tensor(score)
-
-    def __repr__(self):
-        return f"ViM(d={self.n_dim})"
-
-    def fit(self: Self, data_loader, device="cpu") -> Self:
+    def fit_features(self: Self, features: Tensor, labels: Tensor) -> Self:
         """
         Extracts features and logits, computes principle subspace and alpha. Ignores OOD samples.
+
+        :param features: features
+        :param labels: class labels
+        :return:
         """
         try:
             from sklearn.covariance import EmpiricalCovariance
         except ImportError:
             raise Exception("You need to install sklearn to use ViM.")
 
-        features, labels = extract_features(data_loader, self.model, device)
-        features = features.numpy()
+        features = features.cpu().numpy()
 
         if features.shape[1] < self.n_dim:
             n = features.shape[1] // 2
             log.warning(
                 f"{features.shape[1]=} is smaller than {self.n_dim=}. Will be adjusted to {n}"
             )
             self.n_dim = n
```

## pytorch_ood/detector/openmax/numpy.py

```diff
@@ -45,28 +45,30 @@
 
     def _reset(self):
         self.centers = dict()
         self.n_dims = None
         self.distributions = dict()
         self.is_fitted = False
 
-    def fit(self, x, y):
+    def fit(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:
         """
         Fit Openmax layer
 
         :param x: class activations/logits for samples
         :param y: labels for samples
         """
         self._reset()
         classes = np.unique(y)
         self.n_dims = x.shape[1]
         assert x is not None
         assert y is not None
         assert self.alpha is not None
+
         log.debug(f"Input shape: {x.shape}")
+
         self.alpha = min(x.shape[1], self.alpha)
         for n, clazz in enumerate(classes):
             idxs = y == clazz
             if self.centers.get(clazz) is None:
                 self.centers[clazz] = self._get_center(x[idxs])
             else:
                 pass
@@ -77,15 +79,15 @@
             model.fit_high(tailtofit, len(tailtofit))
             if not model.is_valid:
                 log.error(f"Fitting was invalid ({len(tailtofit)} instances for class {clazz})")
             self.distributions[clazz] = model
         self.is_fitted = True
         return self
 
-    def predict(self, x) -> np.ndarray:
+    def predict(self, x: np.ndarray) -> np.ndarray:
         """
         Calculate revised activation vector.
 
         :param x: class activations/logits for samples
 
         :returns: revised activation vector
         """
@@ -126,15 +128,14 @@
                 # get probability of instance being an outlier.
                 # if high -> outlier
                 # low -> no outlier
                 try:
                     w = self.distributions[pred_class].w_score(dist)
                     ws[pred_class] = w
                 except KeyError:
-                    # print(f"Error: class not found: {pred_class}")
                     pass
             wscores = 1 - ws * ranked_alpha  # wscores will be 1 except for the top predictions
             # now that we have calculated the weights, calc the revised activation vector
             revised_activation[j, 1:] = activation * wscores
             # calculate proxy activation for outlier class
             # will be high if wscores is low
             v_0 = np.sum(activation * (1 - wscores))
```

## pytorch_ood/detector/openmax/torch.py

```diff
@@ -1,15 +1,20 @@
+"""
+Torch wrapper for a numpy implementation of openmax.
+"""
 import logging
 from typing import Optional, TypeVar
 
 import torch
+from torch import Tensor
+from torch.nn import Module
 from torch.utils.data import DataLoader
 
-from ...api import Detector
-from ...utils import TensorBuffer, is_known
+from ...api import Detector, ModelNotSetException
+from ...utils import extract_features
 
 log = logging.getLogger(__name__)
 Self = TypeVar("Self")
 
 
 class OpenMax(Detector):
     """
@@ -27,67 +32,68 @@
 
     :see Paper: `ArXiv <https://arxiv.org/abs/1511.06233>`__
     :see Implementation: `GitHub <https://github.com/abhijitbendale/OSDN>`__
     """
 
     def __init__(
         self,
-        model: torch.nn.Module,
+        model: Module,
         tailsize: int = 25,
         alpha: int = 10,
         euclid_weight: float = 1.0,
     ):
         """
         :param model: neural network, assumed to output logits
         :param tailsize: length of the tail to fit the distribution to
         :param alpha: number of class activations to revise
         :param euclid_weight: weight for the euclidean distance.
         """
         self.model = model
 
         # we import it here because of its dependency to the broken libmr
-        from .numpy import OpenMax as NPOpenMax
+        from .numpy import OpenMax as NumpyOpenMax
 
-        self.openmax = NPOpenMax(tailsize=tailsize, alpha=alpha, euclid_weight=euclid_weight)
+        self._openmax = NumpyOpenMax(tailsize=tailsize, alpha=alpha, euclid_weight=euclid_weight)
 
     def fit(self: Self, data_loader: DataLoader, device: Optional[str] = "cpu") -> Self:
         """
         Determines parameters of the weibull functions for each class.
 
         :param data_loader: Data to use for fitting
         :param device: Device used for calculations
         """
-        z, y = OpenMax._extract(data_loader, self.model, device=device)
-        self.openmax.fit(z.numpy(), y.numpy())
+        if self.model is None:
+            raise ModelNotSetException
+
+        z, y = extract_features(data_loader, self.model, device)
+        return self.fit_features(z, y)
+
+    def fit_features(self: Self, logits: Tensor, y: Tensor) -> Self:
+        """
+        Determines parameters of the weibull functions for each class.
+
+        :param logits: logits given by the model
+        :param y: class labels
+        :return:
+        """
+        logits, y = logits.cpu().numpy(), y.cpu().numpy()
+        self._openmax.fit(logits, y)
         return self
 
-    def predict(self, x: torch.Tensor) -> torch.Tensor:
+    def predict(self, x: Tensor) -> Tensor:
         """
         :param x: input, will be passed through the model to obtain logits
         """
+        if self.model is None:
+            raise ModelNotSetException
+
         with torch.no_grad():
-            z = self.model(x).cpu().numpy()
+            logits = self.model(x)
 
-        return torch.tensor(self.openmax.predict(z)[:, 0])
+        return self.predict_features(logits)
 
-    @staticmethod
-    def _extract(data_loader, model: torch.nn.Module, device):
+    def predict_features(self, logits: Tensor) -> Tensor:
         """
-        Extract embeddings from model. Ignores OOD data.
+        :param logits: logits given by model
         """
-        buffer = TensorBuffer()
-        log.debug("Extracting features")
-        for batch in data_loader:
-            x, y = batch
-            x = x.to(device)
-            known = is_known(y)
-            z = model(x[known])
-            # flatten
-            x = z.view(x.shape[0], -1)
-            buffer.append("embedding", z)
-            buffer.append("label", y[known])
-
-        z = buffer.get("embedding")
-        y = buffer.get("label")
-
-        buffer.clear()
-        return z, y
+        logits = logits.cpu().numpy()
+        return torch.tensor(self._openmax.predict(logits)[:, 0])
```

## pytorch_ood/loss/svdd.py

```diff
@@ -23,15 +23,18 @@
 
     :see Paper: `ICML <http://proceedings.mlr.press/v80/ruff18a/ruff18a.pdf>`__
 
     :note: The center is a parameter, so this model has to be moved to the correct device
     """
 
     def __init__(
-        self, n_dim: int, reduction: Optional[str] = "mean", center: Optional[torch.Tensor] = None
+        self,
+        n_dim: int,
+        reduction: Optional[str] = "mean",
+        center: Optional[torch.Tensor] = None,
     ):
         """
         :param n_dim: dimensionality of the output space
         :param reduction: reduction method to apply, one of ``mean``, ``sum`` or ``none``
         :param center: position of the center :math:`\\mu \\in \\mathbb{R}^n` where :math:`n` is the dimensionality of
             the output space
         """
```

## pytorch_ood/model/__init__.py

```diff
@@ -10,20 +10,14 @@
 
 Wide ResNet
 -------------
 
 ..  autoclass:: pytorch_ood.model.WideResNet
     :members:
 
-Vision Transformer
----------------------
-..  autoclass:: pytorch_ood.model.VisionTransformer
-    :members:
-
-
 Natural Language Processing
 ==============================
 
 GRU Classifier
 ---------------------
 ..  autoclass:: pytorch_ood.model.GRUClassifier
     :members:
@@ -43,9 +37,8 @@
 ..  autoclass:: pytorch_ood.model.RunningCenters
     :members:
 
 
 """
 from .centers import ClassCenters, RunningCenters
 from .gru import GRUClassifier
-from .vit import VisionTransformer
 from .wrn import WideResNet
```

## pytorch_ood/utils/metrics.py

```diff
@@ -90,14 +90,17 @@
     :param target: target label
     :param k: cutoff value
     :return:
     """
     # results will be sorted in reverse order
     fpr, tpr, _ = binary_roc(pred, target)
     idx = torch.searchsorted(tpr, k)
+    if idx == fpr.shape[0]:
+        return fpr[idx - 1]
+
     return fpr[idx]
 
 
 class OODMetrics(object):
     """
     Calculates various metrics used in OOD detection experiments.
 
@@ -198,15 +201,14 @@
         if self.buffer.is_empty():
             raise ValueError("Must be given data to calculate metrics.")
 
         if self.mode == "segmentation":
             metrics = {key: self.buffer[key].mean() for key in self.buffer.keys()}
 
         elif self.mode == "classification":
-
             labels = self.buffer.get("labels").view(-1)
             scores = self.buffer.get("scores").view(-1)
 
             if len(torch.unique(labels)) != 2:
                 raise ValueError("Data must contain IN and OOD samples.")
 
             metrics = self._compute(labels, scores)
```

## pytorch_ood/utils/utils.py

```diff
@@ -1,30 +1,31 @@
 """
 
 """
 import logging
 import math
 import random
 from collections import defaultdict
-from typing import Any, Dict, KeysView, List, TypeVar, Union
+from typing import Any, Callable, Dict, KeysView, Optional, Tuple, TypeVar, Union
 
 import numpy as np
 import torch
 import torch.nn.functional as F
 from torch import Tensor
+from torch.utils.data import DataLoader
 
 log = logging.getLogger(__name__)
 
 
 Self = TypeVar("Self")
 
 
 def temperature_calibration(
-    logits: torch.Tensor,
-    labels: torch.Tensor,
+    logits: Tensor,
+    labels: Tensor,
     lower: float = 0.2,
     upper: float = 5.0,
     eps: float = 0.0001,
 ) -> float:
     """
     Implements confidence calibration from the paper
     *On Calibration of Modern Neural Networks*.
@@ -79,55 +80,53 @@
     frac = 2 * n_train / (n_test + n_target)
     return 1 - math.sqrt(frac)
 
 
 #######################################
 # Helpers for labels
 #######################################
-def is_known(labels) -> Union[bool, torch.Tensor]:
+def is_known(labels) -> Union[bool, Tensor]:
     """
     :returns: True, if label :math:`>= 0`
     """
     return labels >= 0
 
 
-def is_unknown(labels) -> Union[bool, torch.Tensor]:
+def is_unknown(labels) -> Union[bool, Tensor]:
     """
     :returns: True, if label :math:`< 0`
     """
     return labels < 0
 
 
-def contains_known_and_unknown(labels) -> Union[bool, torch.Tensor]:
+def contains_known_and_unknown(labels) -> Union[bool, Tensor]:
     """
     :return: true if the labels contain *IN* and *OOD* classes
     """
     return contains_known(labels) and contains_unknown(labels)
 
 
-def contains_known(labels) -> Union[bool, torch.Tensor]:
+def contains_known(labels) -> Union[bool, Tensor]:
     """
     :return: true if the labels contains any *IN* labels
     """
     return is_known(labels).any()
 
 
-def contains_unknown(labels) -> Union[bool, torch.Tensor]:
+def contains_unknown(labels) -> Union[bool, Tensor]:
     """
     :return: true if the labels contains any *OOD* labels
     """
     return is_unknown(labels).any()
 
 
 #######################################
 # Distance functions etc.
 #######################################
-def estimate_class_centers(
-    embedding: torch.Tensor, target: torch.Tensor, num_centers: int = None
-) -> torch.Tensor:
+def estimate_class_centers(embedding: Tensor, target: Tensor, num_centers: int = None) -> Tensor:
     """
     Estimates class centers from the given embeddings and labels, using mean as estimator.
 
     TODO: the loop can prob. be replaced
     """
     batch_classes = torch.unique(target).long().to(embedding.device)
     if num_centers is None:
@@ -146,15 +145,15 @@
     n_centers = centers.shape[0]
     distances = torch.empty((n_instances, n_centers)).to(embeddings.device)
     for clazz in torch.arange(n_centers):
         distances[:, clazz] = torch.norm(embeddings - centers[clazz], dim=1, p=2)
     return distances
 
 
-def pairwise_distances(x, y=None) -> torch.Tensor:
+def pairwise_distances(x, y=None) -> Tensor:
     """
     Calculate pairwise distance by quadratic expansion.
 
     :param x: is a Nxd matrix
     :param y:  Mxd matrix
 
     Output: dist is a NxM matrix where dist[i,j] is the square norm between x[i,:] and y[j,:]
@@ -189,48 +188,48 @@
 
     def is_empty(self) -> bool:
         """
         Returns true if this buffer does not hold any tensors.
         """
         return len(self._buffer) == 0
 
-    def append(self: Self, key, value: torch.Tensor) -> Self:
+    def append(self: Self, key, value: Tensor) -> Self:
         """
         Appends a tensor to the buffer.
 
         :param key: tensor identifier
         :param value: tensor
         """
-        if not isinstance(value, torch.Tensor):
+        if not isinstance(value, Tensor):
             raise ValueError(f"Can not handle value type {type(value)}")
 
         value = value.detach().to(self.device)
         self._buffer[key].append(value)
         return self
 
     def __contains__(self, elem) -> bool:
         return elem in self._buffer
 
-    def __getitem__(self, item) -> torch.Tensor:
+    def __getitem__(self, item) -> Tensor:
         return self.get(item)
 
-    def sample(self, key) -> torch.Tensor:
+    def sample(self, key) -> Tensor:
         """
         Samples a random tensor from the buffer
 
         :param key: tensor identifier
         :return: random tensor
         """
         index = torch.randint(0, len(self._buffer[key]), size=(1,))
         return self._buffer[key][index]
 
     def keys(self) -> KeysView:
         return self._buffer.keys()
 
-    def get(self, key) -> torch.Tensor:
+    def get(self, key) -> Tensor:
         """
         Retrieves tensor from the buffer
 
         :param key: tensor identifier
         :return: concatenated tensor
         """
         if key not in self._buffer:
@@ -254,15 +253,15 @@
         :return: self
         """
         d = {k: self.get(k).cpu() for k in self._buffer.keys()}
         torch.save(d, path)
         return self
 
 
-def apply_reduction(tensor: torch.Tensor, reduction: str) -> torch.Tensor:
+def apply_reduction(tensor: Tensor, reduction: str) -> Tensor:
     """
     Apply specific reduction to a tensor
     """
     if reduction == "mean":
         return tensor.mean()
     elif reduction == "sum":
         return tensor.sum()
@@ -280,22 +279,24 @@
     """
     torch.manual_seed(seed)
     torch.cuda.manual_seed_all(seed)
     random.seed(seed)
     np.random.seed(seed)
 
 
-def extract_features(data_loader, model, device):
-    """
-    Helper to extract from model. Ignores OOD inputs.
-
-    :param data_loader:
-    :param model:
-    :param device:
-    :return:
+def extract_features(
+    data_loader: DataLoader, model: Callable[[Tensor], Tensor], device: Optional[str]
+) -> Tuple[Tensor, Tensor]:
+    """
+    Helper to extract outputs from model. Ignores OOD inputs.
+
+    :param data_loader: dataset to extract from
+    :param model: neural network to pass inputs to
+    :param device: device used for calculations
+    :return: Tuple with outputs and labels
     """
     # TODO: add option to buffer to GPU
     buffer = TensorBuffer()
 
     with torch.no_grad():
         for batch in data_loader:
             x, y = batch
```

## Comparing `pytorch_ood-0.1.0.dist-info/LICENSE` & `pytorch_ood-0.1.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pytorch_ood-0.1.0.dist-info/METADATA` & `pytorch_ood-0.1.1.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pytorch-ood
-Version: 0.1.0
+Version: 0.1.1
 Summary: A Library for Out-of-Distribution Detection with PyTorch
 Home-page: https://gitlab.com/kkirchheim/pytorch-ood
 Author: Konstantin Kirchheim
 License: Apache 2.0
 Project-URL: Bug Tracker, https://gitlab.com/kkirchheim/pytorch-ood/-/issues
 Project-URL: repository, https://gitlab.com/kkirchheim/pytorch-ood
 Keywords: OOD,PyTorch,Out-of-Distribution Detection
@@ -20,14 +20,18 @@
 PyTorch Out-of-Distribution Detection
 ****************************************
 
 .. image:: https://img.shields.io/badge/docs-online-blue
    :target: https://pytorch-ood.readthedocs.io/en/latest/
    :alt: Documentation
 
+.. image:: https://img.shields.io/pypi/v/pytorch-ood?color=light
+   :target: https://pypi.org/project/pytorch-ood/
+   :alt: License
+
 .. image:: https://img.shields.io/pypi/l/pytorch-ood
    :target: https://gitlab.com/kkirchheim/pytorch-ood/-/blob/master/LICENSE
    :alt: License
 
 .. image:: https://img.shields.io/badge/-Python 3.8+-blue?logo=python&logoColor=white
    :target: https://www.python.org/
    :alt: Python
@@ -56,15 +60,15 @@
 
 - Out-of-Distribution Detection Methods
 - Loss Functions
 - Datasets
 - Neural Network Architectures as well as pretrained weights
 - Useful Utilities
 
-and is designed such that it should be compatible with frameworks like,
+and is designed such that it should be compatible with frameworks
 like `pytorch-lightning <https://www.pytorchlightning.ai>`_ and
 `pytorch-segmentation-models <https://github.com/qubvel/segmentation_models.pytorch>`_.
 The library also covers some methods from closely related fields such as Open-Set Recognition, Novelty Detection,
 Confidence Estimation and Anomaly Detection.
```

## Comparing `pytorch_ood-0.1.0.dist-info/RECORD` & `pytorch_ood-0.1.1.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-pytorch_ood/__init__.py,sha256=l9R5SRJfPe2_FqHSiimJhWozQ2H_X2Iwzw371QytsFs,209
-pytorch_ood/api.py,sha256=Qv-uz9QNfu4-Z-UaN57hTDZNHfknK4tD_m3OLD1Pv_4,1152
+pytorch_ood/__init__.py,sha256=4LZTVhllkxdkv5CIt54ZuqhIMNdXiF0A9Hvs6eXrVN8,209
+pytorch_ood/api.py,sha256=eOp1WgYknpzDq8v1hY8V7oWXsI6SQHyCAC1C1dSL6MI,2341
 pytorch_ood/dataset/__init__.py,sha256=4mwJ-YED4MXBlZXeqi04Qg5zxqDMxaszHOupjVy9LAQ,11
 pytorch_ood/dataset/audio/__init__.py,sha256=zYLehvyFdN4DjcGRPpoLVMgSp9HNiAiXAfjxT8CH10k,165
 pytorch_ood/dataset/audio/fsdd.py,sha256=cYKcHtbV7BhGKav_d5qFvNiLRwhvwzWTC93txgu2ezg,3059
 pytorch_ood/dataset/img/__init__.py,sha256=nMyYTHfzWNtP742oxKCCB3EP2tOFN7VY3hO4Pz0sX88,3043
 pytorch_ood/dataset/img/base.py,sha256=LLFSqBB2tkVjCRHcqAUwZNW3KCPON0kJClOFvrq3xy0,2363
-pytorch_ood/dataset/img/chars74k.py,sha256=Lbqs8F8gpPWSWNIvOY4bmL5BNwqCMkVcTDfkikw_ghk,4330
+pytorch_ood/dataset/img/chars74k.py,sha256=YA4Hx92fS29jxrpG0zEc09RaRSpmGRdMQMBJ4lyAm2E,4329
 pytorch_ood/dataset/img/cifar.py,sha256=seM3RpADphYfIt9ON5Wpslv5_QNTs5fhi3JfEkVH-vE,3076
 pytorch_ood/dataset/img/fooling.py,sha256=iOQMf2keqOzd8sfhyZ2ONiBcAux4FSAU9Bz2sGts58o,1186
-pytorch_ood/dataset/img/imagenet.py,sha256=cURuUrFBTEifIi10HNSRjJPFoAc0ToFQL_Ox8CYDS8E,5794
+pytorch_ood/dataset/img/imagenet.py,sha256=MpA-6DD6R1JlIWmS1Ytuik-kfImCnPc-H_Urzj13eqU,5793
 pytorch_ood/dataset/img/mnistc.py,sha256=Wp06GFhRi9_iM6SSe_rUG3GuolMUtuZ_qlR7ctpY4l0,4270
 pytorch_ood/dataset/img/mvtech.py,sha256=Ki5BjbohoWQ-W3tGNAh_1yxidutiujQfBI8tvgEL5R4,5382
 pytorch_ood/dataset/img/noise.py,sha256=eJSs2sPPcfZJN09kIcST3RKXPa2YP42Ciey8R3ThD0A,3671
 pytorch_ood/dataset/img/odin.py,sha256=xoA0MP2zFHd3ME5CIVXUB-8_EKRnEzAmXaospk-F6BY,4875
 pytorch_ood/dataset/img/pixmix.py,sha256=um3PfP9Gb03TwjoEyOCxatI_ci824FHY35ygX-E7kBU,8477
 pytorch_ood/dataset/img/streethazards.py,sha256=R4-EMG9voEOshJEBmFtf17KijNcuveuufOVwQVMhqGE,4242
 pytorch_ood/dataset/img/textures.py,sha256=VLEv5Md6pNqivOhAp4vUwxuEWfAwfvkPMvnd7zSmDrM,3001
@@ -23,48 +23,47 @@
 pytorch_ood/dataset/txt/__init__.py,sha256=wpmf8nqnJkDI-CAgAI-pH07PoV0UKu_miSreNaEX9dw,1049
 pytorch_ood/dataset/txt/multi30k.py,sha256=35ssfLwz8zrj50UwxA0s9rw5VKXMQADuwRXt3Bmcspg,2665
 pytorch_ood/dataset/txt/newsgroups.py,sha256=4tTVikOfXEEUw0CX0vm2pzPKhund5VYNaUYlyGWCNIQ,4002
 pytorch_ood/dataset/txt/reuters.py,sha256=ZWyQ_8VfmPu-9CdDf1PByMqy6HMoxjXGqscmICcHQTk,5954
 pytorch_ood/dataset/txt/stop_words.py,sha256=qSZlcCeyoKKpHfvJJjU6PHcXeG1pia324VdT6jE1HgI,1983
 pytorch_ood/dataset/txt/wiki.py,sha256=Mqvke4I6gh7nvTTw66lcJcamn7TXJ-CnDKSPHYPBHOU,3331
 pytorch_ood/dataset/txt/wmt16.py,sha256=5DVHikenTFOtwn0LaqEEh6xEYeZ9rhQYdrXEzWyDt14,2034
-pytorch_ood/detector/__init__.py,sha256=G6EbPPLu0kej563LXk8rPYgAK0syfmv4Nd_POpaw-zk,1881
-pytorch_ood/detector/energy.py,sha256=sD3RUMFv1OK8CzEbcF7mBL7aAPblHogsW1U2snNBPrI,1934
-pytorch_ood/detector/entropy.py,sha256=GEZ8OBjmQDEuwnx1OiV--LllSc58dNXPozUNs1g0kXA,1639
-pytorch_ood/detector/klmatching.py,sha256=N_SdhE3BNc3NfoaxwWVlpH9sfu5JWcAD4UZNwx3MNB0,3478
-pytorch_ood/detector/mahalanobis.py,sha256=4JdgbYBAuzPH7q9H_YmjHERP7UZuhGWpQJ2873Thi30,6666
-pytorch_ood/detector/maxlogit.py,sha256=_gmxQFC1QC5GJXFDWSlW-f-nb0-4IKebSSGPfgGPQq4,1412
-pytorch_ood/detector/mcd.py,sha256=RY48t0wSHFWP_aNDVPdoEKfWuTBx_S6_ECiiF461_EM,3127
-pytorch_ood/detector/odin.py,sha256=3CogUR9JXn_SuvVl_fFXEov6UKmOFD8xwzivtyNBzvo,5365
-pytorch_ood/detector/softmax.py,sha256=RtIJqLTew__DLbiCqW8MGV3mxWmI-asUPGOYGYk7mTM,1891
-pytorch_ood/detector/vim.py,sha256=HvZSGfFg1KI8COnvIMp49_Z0or65NPI0hwhwN7u9l-g,4320
+pytorch_ood/detector/__init__.py,sha256=Z8XLIhK5XVRENoaFLZa8iPT6TqGIuTLcGENYoJ9Jk38,2395
+pytorch_ood/detector/energy.py,sha256=HtLHQAZ1Oh_xcuyRf3CwJxc0Bov2WeQe6k8inhFjukM,2352
+pytorch_ood/detector/entropy.py,sha256=n2V7rUcORUf9xTnN7wDtMb3waUy9c47QSlcsi3-6_gI,2046
+pytorch_ood/detector/klmatching.py,sha256=NKv0Boyo8GlpTWPv8O4ESA5cWOL1_OeZILEZYVwJ6sY,4402
+pytorch_ood/detector/mahalanobis.py,sha256=zJ66KNPMrEfA9Ms5VYEY4SqyDD2stL6lr8nbFlAQU80,7700
+pytorch_ood/detector/maxlogit.py,sha256=vkOO3fLWVLLQjKAupGe4H8CFi_A2rWwGLEFhz26M45Q,1810
+pytorch_ood/detector/mcd.py,sha256=GSRw_p86ss5PN1NZSiykGhh-xRYpQfJ6vaU4-4btSRs,3619
+pytorch_ood/detector/odin.py,sha256=rFjyD_sVgRzqTnQq43tXg38pUzyAUrQtWODfC8JgplE,5816
+pytorch_ood/detector/softmax.py,sha256=9LbSwPVPb_GCU3ABIRuG4m2wY2-MqyLK-rgo4qR1bFg,2316
+pytorch_ood/detector/vim.py,sha256=v5Og7bL3oGEXSPfv9WQv1iF0wRofNfKrVttO4EDSE6I,5267
 pytorch_ood/detector/openmax/__init__.py,sha256=aLSjSeuO9EgxWo5SBbLyOqMLcIMIbgpju9WUL8wtgyE,323
-pytorch_ood/detector/openmax/numpy.py,sha256=dKgRQhNYwVnOSMztJw1iXyVCFXW0KvB-pxURfv3Dtic,7058
-pytorch_ood/detector/openmax/torch.py,sha256=W3GEeALqSXPOM1sAmkU1eoUsn8R-2sWN8RWOqhK5zGs,3158
+pytorch_ood/detector/openmax/numpy.py,sha256=zh5SHjrHhfeWX9Tol5aaDbaKtxT70j8JjSC7EtHi1s0,7041
+pytorch_ood/detector/openmax/torch.py,sha256=Ztzt7Y8IRgRocVZ-TzBbBBmGFIFlH5RZyITiMtAC-EQ,3318
 pytorch_ood/loss/__init__.py,sha256=p3b1JB96f4YjaDkk1PvaaMFs7tDxUyPW6_lZqR5gFGI,5408
 pytorch_ood/loss/background.py,sha256=d9TNUzeWqsZ7c-EjAGFrWPdOagnQIcJ4hTmTtJD50sc,1589
 pytorch_ood/loss/cac.py,sha256=gMKyfxHSiCJOAfTvveHFfcCMxCbAUvo3pQ5d71-cmew,3762
 pytorch_ood/loss/center.py,sha256=xcG1GQWcq312GTg1LrlADvc_8g1yVSUkV5nMc12V_94,4014
 pytorch_ood/loss/conf.py,sha256=KZdIlKVGqr_rRMR8M9CvDsWtBxTRlG4zgWUVZnBjBkM,2378
 pytorch_ood/loss/crossentropy.py,sha256=JAsScyzOA3GTn2IlHIKU6ya7KyLuu-8r_zhNuLqrNjY,1188
 pytorch_ood/loss/energy.py,sha256=Hns8hACIwuMewFzAu58_uozSRDdkFG-uT8qi-iukPtw,2565
 pytorch_ood/loss/entropy.py,sha256=K02U4FMyIKZAmOLRnifD_NUPjDZx3zo5LzNdLMT0jpY,2741
 pytorch_ood/loss/ii.py,sha256=ISc-GCRgYnACNMS9GpkfpjJEqPXwChbULp81ykq2M6U,4599
 pytorch_ood/loss/mchad.py,sha256=u-a4tOnDqHeKgwtUb8XPtJqdCsGPtOIUcOnm1p9LgaE,4875
 pytorch_ood/loss/objectosphere.py,sha256=eubnoMhEkmdIhuFAB485CBX-eT0NObxt7FTCkp8HAJE,2667
 pytorch_ood/loss/oe.py,sha256=k9VRWIGyodwk5crz6snu8KGb2VSzGRmP8lCYBxm10WE,2199
-pytorch_ood/loss/svdd.py,sha256=Y2_1guPpIT-tuigkVa_thA4PE1s_fot9CzJuRsKwviM,4556
-pytorch_ood/model/__init__.py,sha256=EfGCiflKm2k0LkCk4BXTGknaBPtTbWWLUeJ8HqXLjG0,867
+pytorch_ood/loss/svdd.py,sha256=F3Z2Bs9oMY5cQnuaCls3Lw4ap5d-95FPt86-ZQpleSs,4581
+pytorch_ood/model/__init__.py,sha256=U2iKbCxQMfzkk1q9eeo2Wg2Bm81-UUpN9nA2o5dDwZ0,723
 pytorch_ood/model/centers.py,sha256=A40xpyEMScwy4d_LlSUzv2TLCTK4EsvEGm_D1fQNDY0,4676
 pytorch_ood/model/gru.py,sha256=zvM9nYgBDo7fTbQ0SZjhUNNXDLNNZy8FRm8z1xwegIc,1576
-pytorch_ood/model/vit.py,sha256=s7jE19e7UuoQ58MpLlP-7YJCJx4hkcfSOjweEemA_Lk,9219
 pytorch_ood/model/wrn.py,sha256=9KRw047lZxozDJcNGmBI8g5h_CwJ0r5cYJNGdS28PEc,10550
 pytorch_ood/utils/__init__.py,sha256=7HadmLfYRDYnSzWduQZdcIIOzfH-8-_BO8lbq1g-wNk,70
 pytorch_ood/utils/gdown.py,sha256=EC-bvd3Qp82zO0wrpLpwh2QVzDQSCKM5SwwO759B5_0,10960
-pytorch_ood/utils/metrics.py,sha256=IyCHsZ4t2l1uuTispMrPiep8G7IoUXY-BCskk7hCeJ4,7037
+pytorch_ood/utils/metrics.py,sha256=0a9-c9fUPMK3MjLGo6amWSn18wVGJZv3VotNv9oaILg,7093
 pytorch_ood/utils/transforms.py,sha256=uf2MAtsq0BezMv2Vv4e3_EgJNWPeTM7yeEC7eGtVPO0,2111
-pytorch_ood/utils/utils.py,sha256=zxM0VrbGnVw8rgQI9ir8qCsq_GbnDqUDnB-mid_nt54,8689
-pytorch_ood-0.1.0.dist-info/LICENSE,sha256=sCskNJvx8BCnWvaR8F3J3W0pm3bJhObe3XZKfuLTvnA,11351
-pytorch_ood-0.1.0.dist-info/METADATA,sha256=9zd5TyeFLO-gnDEK0SdjQbjGDIF9r304nal3bYYS6hw,23527
-pytorch_ood-0.1.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-pytorch_ood-0.1.0.dist-info/top_level.txt,sha256=DQdb7oLs5bEbGwRkFDMkBe3xeGccj8U-P4eHsi8VOS0,12
-pytorch_ood-0.1.0.dist-info/RECORD,,
+pytorch_ood/utils/utils.py,sha256=B0JwybjKRG1nTUMCIcAcYr130bKKcFBqg2phc6jfamo,8846
+pytorch_ood-0.1.1.dist-info/LICENSE,sha256=sCskNJvx8BCnWvaR8F3J3W0pm3bJhObe3XZKfuLTvnA,11351
+pytorch_ood-0.1.1.dist-info/METADATA,sha256=jCI5kOZX1ZqDgKAWYmcP3b7WUvX5nctitrl84LCLl4w,23654
+pytorch_ood-0.1.1.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+pytorch_ood-0.1.1.dist-info/top_level.txt,sha256=DQdb7oLs5bEbGwRkFDMkBe3xeGccj8U-P4eHsi8VOS0,12
+pytorch_ood-0.1.1.dist-info/RECORD,,
```

